{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/My-LinkedIn-red?&style=flat)](https://www.linkedin.com/in/brandon-laroche-3b73691b7/) [![GitHub](https://img.shields.io/badge/GitHub-red?logo=github)](https://github.com/brandon57l/) [![huggingface](https://img.shields.io/badge/HuggingFace-red)\n",
        "](https://huggingface.co/brandon57)\n",
        "<!-- [![GitHub](https://img.shields.io/badge/huggingface-red?logo=huggingface&logoColor=white&style=flat)\n",
        "](https://www.linkedin.com/in/brandon-laroche-3b73691b7/) -->\n"
      ],
      "metadata": {
        "id": "hWRr6WWjPE0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "X8jkshBMrxsq",
        "outputId": "e6955934-f484-4732-eb33-f3aa77aae926",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1 style=\"color: green;\">‚úÖ Done !</h1>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title # 1Ô∏è‚É£ **Installation**\n",
        "from IPython.display import clear_output, HTML\n",
        "\n",
        "clear_output()\n",
        "display(HTML('<h2 style=\"color: blue;\">Installation des d√©pendances... üöÄ</h2>'))\n",
        "\n",
        "\n",
        "!wget -P ~ https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i ~/cloudflared-linux-amd64.deb\n",
        "clear_output()\n",
        "display(HTML('<h3 style=\"color: black;\">Cloudflared - OK</h3>'))\n",
        "\n",
        "!pip install flask flask-cors\n",
        "clear_output()\n",
        "display(HTML('<h3 style=\"color: black;\">Flask/Cors - OK</h3>'))\n",
        "\n",
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "clear_output()\n",
        "display(HTML('<h3 style=\"color: black;\">Torch/Cuda - OK</h3>'))\n",
        "\n",
        "!pip uninstall faster-whisper ctranslate2 -y\n",
        "!pip install --no-cache-dir ctranslate2 faster-whisper\n",
        "clear_output()\n",
        "display(HTML('<h3 style=\"color: black;\">Whisper - OK</h3>'))\n",
        "\n",
        "\n",
        "# !pip install -q openai-whisper\n",
        "# clear_output()\n",
        "# display(HTML('<h3 style=\"color: black;\">Whisper - OK</h3>'))\n",
        "\n",
        "!pip install yt-dlp pydub==0.25.1\n",
        "clear_output()\n",
        "display(HTML('<h3 style=\"color: black;\">Youtube dep - OK</h3>'))\n",
        "\n",
        "clear_output()\n",
        "display(HTML('<h1 style=\"color: green;\">‚úÖ Done !</h1>'))\n",
        "\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version PyTorch built with: {torch.version.cuda}\")\n",
        "    print(f\"cuDNN version PyTorch built with: {torch.backends.cudnn.version()}\")\n",
        "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # **Transcription & Traduction (Upload Incr√©mental)**\n",
        "\n",
        "# --- Initial Cleanup ---\n",
        "print(\"--- Initial Cleanup ---\")\n",
        "!rm -f /content/audio_output/*\n",
        "!rm -f /content/audio_output_optimized/*\n",
        "!rm -f /content/audio_output_optimized_v2/*\n",
        "!rm -rf /content/audio_chunks/*\n",
        "print(\"Cleanup done.\")\n",
        "\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "import math\n",
        "import shutil\n",
        "import requests\n",
        "import sys\n",
        "import random\n",
        "import concurrent.futures # <-- Ajout pour l'ex√©cution concurrente\n",
        "\n",
        "# --- Library Imports & Checks ---\n",
        "print(\"\\n--- Library Imports & Checks ---\")\n",
        "try:\n",
        "    import torch\n",
        "    print(\"‚úÖ PyTorch install√©.\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå PyTorch non install√©. Veuillez l'installer.\")\n",
        "    exit()\n",
        "try:\n",
        "    from faster_whisper import WhisperModel\n",
        "    print(\"‚úÖ faster-whisper install√©.\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå faster-whisper non install√©. Veuillez l'installer.\")\n",
        "    exit()\n",
        "try:\n",
        "    from pydub import AudioSegment\n",
        "    print(\"‚úÖ pydub install√©.\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå pydub non install√©. Veuillez l'installer.\")\n",
        "    exit()\n",
        "\n",
        "# --- GPU Check ---\n",
        "print(\"\\n--- GPU Check ---\")\n",
        "IS_GPU_AVAILABLE = torch.cuda.is_available()\n",
        "print(f\"‚úÖ GPU d√©tect√©: {IS_GPU_AVAILABLE}\")\n",
        "if IS_GPU_AVAILABLE:\n",
        "    try:\n",
        "        print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "        major, minor = torch.cuda.get_device_capability(0)\n",
        "        print(f\"   Compute Capability: {major}.{minor}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Impossible de r√©cup√©rer les d√©tails du GPU: {e}\")\n",
        "\n",
        "\n",
        "# --- Directories ---\n",
        "print(\"\\n--- Directories ---\")\n",
        "CHUNK_DIR = \"/content/audio_chunks\"\n",
        "OUTPUT_DIR_V2 = \"/content/audio_output_optimized_v2\"\n",
        "os.makedirs(CHUNK_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ Dossier chunks pr√™t: {CHUNK_DIR}\")\n",
        "os.makedirs(OUTPUT_DIR_V2, exist_ok=True)\n",
        "print(f\"‚úÖ Dossier sortie pr√™t: {OUTPUT_DIR_V2}\")\n",
        "\n",
        "# ==============================================================\n",
        "# --- FONCTIONS TRANSCRIPTION (INCHANG√âES) ---\n",
        "# ==============================================================\n",
        "print(\"\\n--- D√©finition Fonctions Transcription ---\")\n",
        "# --- download_youtube_audio_improved (INCHANG√âE) ---\n",
        "def download_youtube_audio_improved(youtube_url, output_path):\n",
        "    \"\"\"Downloads audio from a YouTube URL using yt-dlp and retrieves metadata.\"\"\"\n",
        "    # ... (code inchang√©) ...\n",
        "    print(f\"\\n--- T√©l√©chargement Audio depuis YouTube ---\")\n",
        "    print(f\"URL: {youtube_url}\")\n",
        "    print(f\"Destination: {output_path}\")\n",
        "\n",
        "    video_info = None\n",
        "    audio_file_path = None\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        try:\n",
        "            os.makedirs(output_dir)\n",
        "            print(f\"üìÅ Cr√©ation r√©pertoire: {output_dir}\")\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Erreur cr√©ation r√©pertoire {output_dir}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    print(\"‚ÑπÔ∏è R√©cup√©ration m√©tadonn√©es...\")\n",
        "    try:\n",
        "        cmd = [\"yt-dlp\", \"--dump-json\", \"--encoding\", \"utf-8\", \"--socket-timeout\", \"30\", youtube_url]\n",
        "        metadata_result = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', timeout=60)\n",
        "        m = json.loads(metadata_result.stdout)\n",
        "        # Extract video ID robustly\n",
        "        video_id_match = re.search(r\"v=([a-zA-Z0-9_-]+)\", youtube_url)\n",
        "        v_id = m.get(\"id\") or (video_id_match.group(1) if video_id_match else None) or \"UNKNOWN_ID\"\n",
        "\n",
        "        video_info = {\n",
        "            \"video_id\": v_id,\n",
        "            \"channel_name\": m.get(\"uploader\", \"N/A\"),\n",
        "            \"channel_url\": m.get(\"uploader_url\", \"N/A\"),\n",
        "            \"title\": m.get(\"title\", \"N/A\"),\n",
        "            \"description\": m.get(\"description\", \"N/A\"),\n",
        "            \"original_url\": youtube_url,\n",
        "            \"duration\": m.get(\"duration\"), # Keep duration if available\n",
        "            \"upload_date\": m.get(\"upload_date\"), # Keep upload date if available\n",
        "        }\n",
        "        print(\"‚úÖ M√©tadonn√©es r√©cup√©r√©es.\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "         print(f\"‚ùå Timeout r√©cup√©ration m√©tadonn√©es.\")\n",
        "         return None, None\n",
        "    except subprocess.CalledProcessError as e:\n",
        "         print(f\"‚ùå Erreur yt-dlp (metadata): {e}\\n{e.stderr}\")\n",
        "         return None, None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Erreur d√©codage JSON m√©tadonn√©es: {e}\")\n",
        "        return None, {\"original_url\": youtube_url, \"video_id\": \"UNKNOWN_ERROR_JSON\"}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue (m√©tadonn√©es): {e}\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"üîÑ V√©rif/T√©l√©chargement audio -> {os.path.basename(output_path)}...\")\n",
        "    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "        print(f\"‚úÖ Fichier audio existant trouv√© et non vide. Utilisation de '{os.path.basename(output_path)}'.\")\n",
        "        audio_file_path = output_path\n",
        "    else:\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"‚ÑπÔ∏è Fichier existant '{os.path.basename(output_path)}' est vide. Re-t√©l√©chargement...\")\n",
        "        else:\n",
        "            print(f\"‚ÑπÔ∏è Fichier '{os.path.basename(output_path)}' absent. T√©l√©chargement...\")\n",
        "        try:\n",
        "            cmd = [\n",
        "                \"yt-dlp\", \"-x\",\n",
        "                \"--audio-format\", \"mp3\",\n",
        "                \"--audio-quality\", \"0\", # 0 is best quality\n",
        "                \"--force-overwrites\",\n",
        "                \"-o\", output_path,\n",
        "                \"--encoding\", \"utf-8\",\n",
        "                \"--socket-timeout\", \"30\",\n",
        "                 youtube_url\n",
        "            ]\n",
        "            dl_res = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', timeout=1800) # 30 min timeout\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f\"‚úÖ Audio t√©l√©charg√© avec succ√®s.\")\n",
        "                audio_file_path = output_path\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è yt-dlp a termin√© sans erreur, mais le fichier est absent ou vide.\")\n",
        "                print(f\"   Sortie yt-dlp:\\n{dl_res.stdout}\\n{dl_res.stderr}\")\n",
        "                return None, video_info\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"‚ùå Timeout durant le t√©l√©chargement audio.\")\n",
        "            return None, video_info\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Erreur yt-dlp (t√©l√©chargement): {e}\\n{e.stderr}\")\n",
        "            return None, video_info\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur inattendue (t√©l√©chargement): {e}\")\n",
        "            return None, video_info\n",
        "\n",
        "    if audio_file_path and (not os.path.exists(audio_file_path) or os.path.getsize(audio_file_path) == 0):\n",
        "        print(f\"‚ùå ERREUR FINALE: Le chemin du fichier audio est d√©fini mais le fichier est absent ou vide.\")\n",
        "        return None, video_info\n",
        "\n",
        "    return audio_file_path, video_info\n",
        "\n",
        "# --- split_audio_by_fixed_duration (INCHANG√âE) ---\n",
        "def split_audio_by_fixed_duration(input_audio_path, output_chunk_dir, chunk_duration_s):\n",
        "    \"\"\"Splits an audio file into fixed duration chunks using pydub.\"\"\"\n",
        "    # ... (code inchang√©) ...\n",
        "    print(f\"\\nüîä D√©coupage audio '{os.path.basename(input_audio_path)}' en chunks de {chunk_duration_s}s...\")\n",
        "    if not os.path.exists(input_audio_path) or os.path.getsize(input_audio_path) == 0:\n",
        "        print(f\"‚ùå Fichier audio d'entr√©e absent ou vide: {input_audio_path}\")\n",
        "        return [], []\n",
        "    if chunk_duration_s <= 0:\n",
        "        print(f\"‚ùå Dur√©e de chunk invalide ({chunk_duration_s}s). Doit √™tre positive.\")\n",
        "        return [], []\n",
        "\n",
        "    paths, offsets = [], []\n",
        "    chunk_dur_ms = int(chunk_duration_s * 1000)\n",
        "\n",
        "    try:\n",
        "        print(f\"   Chargement de l'audio avec pydub...\")\n",
        "        audio = AudioSegment.from_file(input_audio_path)\n",
        "        total_dur_ms = len(audio)\n",
        "        total_dur_s = total_dur_ms / 1000.0\n",
        "        print(f\"   Audio charg√© ({total_dur_s:.2f}s). D√©coupage en cours...\")\n",
        "\n",
        "        if total_dur_ms <= 0:\n",
        "            print(f\"‚ùå La dur√©e de l'audio est nulle ou n√©gative.\")\n",
        "            return [], []\n",
        "\n",
        "        start_ms = 0\n",
        "        idx = 0\n",
        "        while start_ms < total_dur_ms:\n",
        "            end_ms = start_ms + chunk_dur_ms\n",
        "            chunk = audio[start_ms:end_ms]\n",
        "            cur_dur_ms = len(chunk)\n",
        "            cur_dur_s = cur_dur_ms / 1000.0\n",
        "            cur_start_s = start_ms / 1000.0\n",
        "\n",
        "            if cur_dur_s <= 0.1:\n",
        "                start_ms = end_ms\n",
        "                continue\n",
        "\n",
        "            start_fmt = f\"{cur_start_s:.3f}\".replace('.', '_')\n",
        "            dur_fmt = f\"{cur_dur_s:.3f}\".replace('.', '_')\n",
        "            fname = f\"chunk_{idx:04d}_start{start_fmt}s_dur{dur_fmt}s.mp3\"\n",
        "            fpath = os.path.join(output_chunk_dir, fname)\n",
        "\n",
        "            try:\n",
        "                chunk.export(fpath, format=\"mp3\")\n",
        "                if os.path.exists(fpath) and os.path.getsize(fpath) > 0:\n",
        "                    paths.append(fpath)\n",
        "                    offsets.append(cur_start_s)\n",
        "                else:\n",
        "                    print(f\"     ‚ö†Ô∏è √âchec export ou fichier vide cr√©√©: {fpath}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Erreur lors de l'export du chunk {fpath}: {e}\")\n",
        "\n",
        "            start_ms = end_ms\n",
        "            idx += 1\n",
        "\n",
        "        if not paths:\n",
        "            print(\"‚ùå Aucun chunk n'a √©t√© cr√©√© ou sauvegard√©.\")\n",
        "        else:\n",
        "            print(f\"‚úÖ D√©coupage termin√©: {len(paths)} chunks cr√©√©s dans {output_chunk_dir}\")\n",
        "\n",
        "        return paths, offsets\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Erreur pydub: Fichier d'entr√©e non trouv√©: {input_audio_path}\")\n",
        "        return [], []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue durant le d√©coupage: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return [], []\n",
        "\n",
        "# --- transcribe_audio_faster (INCHANG√âE) ---\n",
        "def transcribe_audio_faster(file_path, model, chunk_offset_s, beam_size, vad_filter, vad_min_silence_ms):\n",
        "    \"\"\"Transcribes a single audio file chunk using the preloaded faster-whisper model.\"\"\"\n",
        "    # ... (code inchang√©) ...\n",
        "    start_time_transcribe = time.time()\n",
        "    print(f\"\\nüéôÔ∏è Transcription: {os.path.basename(file_path)} (Offset Global: {chunk_offset_s:.3f}s)\")\n",
        "\n",
        "    segments_data = []\n",
        "    total_duration = 0\n",
        "    last_prog = -1\n",
        "\n",
        "    if model is None:\n",
        "        print(\"‚ùå Mod√®le Whisper non charg√©! Impossible de transcrire.\")\n",
        "        return {\"segments\": []}\n",
        "\n",
        "    try:\n",
        "        vad_params = {\"min_silence_duration_ms\": vad_min_silence_ms} if vad_filter else None\n",
        "\n",
        "        segments_generator, info = model.transcribe(\n",
        "            file_path,\n",
        "            beam_size=beam_size,\n",
        "            vad_filter=vad_filter,\n",
        "            vad_parameters=vad_params\n",
        "        )\n",
        "\n",
        "        lang, prob, total_duration = info.language, info.language_probability, info.duration\n",
        "        print(f\"   Infos chunk d√©tect√©es: Lang='{lang}' (Conf: {prob:.2f}), Dur√©e: {total_duration:.2f}s\")\n",
        "\n",
        "        if total_duration <= 0:\n",
        "            print(\"   ‚ö†Ô∏è Dur√©e du chunk audio nulle ou n√©gative selon Whisper.\")\n",
        "            return {\"segments\": []}\n",
        "\n",
        "        print(\"   Traitement des segments...\")\n",
        "        seg_count = 0\n",
        "        for segment in segments_generator:\n",
        "            seg_count += 1\n",
        "            start_local, end_local = segment.start, segment.end\n",
        "            duration_local = max(0, end_local - start_local)\n",
        "            text = segment.text.strip() if segment.text else \"\"\n",
        "            start_global = round(start_local + chunk_offset_s, 3)\n",
        "            duration_rounded = round(duration_local, 3)\n",
        "\n",
        "            prog = min(100.0, (end_local / total_duration) * 100) if total_duration > 0 else 0\n",
        "            rounded_prog = math.floor(prog)\n",
        "            if rounded_prog > last_prog and (rounded_prog % 10 == 0 or rounded_prog >= 99):\n",
        "                bar_len = 20\n",
        "                filled_len = int(bar_len * prog / 100)\n",
        "                bar = '‚ñà' * filled_len + '-' * (bar_len - filled_len)\n",
        "                sys.stdout.write(f\"\\r   Progression: [{bar}] {prog:.0f}% \")\n",
        "                sys.stdout.flush()\n",
        "                last_prog = rounded_prog\n",
        "\n",
        "            segments_data.append({\n",
        "                \"text\": text,\n",
        "                \"start\": start_global,\n",
        "                \"duration\": duration_rounded\n",
        "            })\n",
        "        sys.stdout.write(\"\\n\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        transcription_time = time.time() - start_time_transcribe\n",
        "        print(f\"   üïí Transcription du chunk termin√©e en {transcription_time:.2f}s. {seg_count} segments trouv√©s.\")\n",
        "        if seg_count == 0:\n",
        "            print(\"   ‚ö†Ô∏è Aucun segment de parole trouv√© dans ce chunk.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erreur durant la transcription du chunk: {e}\")\n",
        "        traceback.print_exc()\n",
        "        sys.stdout.write(\"\\n\")\n",
        "        sys.stdout.flush()\n",
        "        return {\"segments\": []}\n",
        "\n",
        "    return {\"segments\": segments_data}\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# --- FONCTIONS TRADUCTION (INCHANG√âES EN ELLES-M√äMES) ---\n",
        "# ==============================================================\n",
        "print(\"\\n--- D√©finition Fonctions Traduction ---\")\n",
        "\n",
        "# --- chunk_list (INCHANG√âE) ---\n",
        "def chunk_list(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    if not isinstance(lst, list):\n",
        "        raise TypeError(\"Input must be a list.\")\n",
        "    if n <= 0:\n",
        "        raise ValueError(\"Chunk size must be positive.\")\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "# --- extract_json_from_response (INCHANG√âE) ---\n",
        "def extract_json_from_response(text_response):\n",
        "    # ... (code inchang√©) ...\n",
        "    match_fence = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', text_response, re.DOTALL)\n",
        "    if match_fence:\n",
        "        return match_fence.group(1).strip()\n",
        "    match_list = re.search(r'(\\[[^\\]]*\\])', text_response, re.DOTALL)\n",
        "    match_obj = re.search(r'(\\{[\\s\\S]*\\})', text_response, re.DOTALL)\n",
        "    json_string = None\n",
        "    first_match_pos = float('inf')\n",
        "    if match_list and match_list.start() < first_match_pos:\n",
        "        json_string = match_list.group(1)\n",
        "        first_match_pos = match_list.start()\n",
        "    if match_obj and match_obj.start() < first_match_pos:\n",
        "        json_string = match_obj.group(1)\n",
        "    if json_string:\n",
        "        return json_string.strip()\n",
        "    return text_response.strip()\n",
        "\n",
        "\n",
        "# --- translate_audio_chunk_segments (INCHANG√âE EN ELLE-M√äME) ---\n",
        "# Cette fonction est appel√©e par chaque thread, elle n'a pas besoin de conna√Ætre la concurrence.\n",
        "def translate_audio_chunk_segments(transcript_segments, api_key, audio_chunk_index, total_audio_chunks, segment_chunk_index=None, total_segment_chunks=None):\n",
        "    \"\"\"\n",
        "    Translates a list of transcript segments using Google Gemini API.\n",
        "    Logs added to indicate segment chunk index if provided.\n",
        "    \"\"\"\n",
        "    # ... (code inchang√©) ...\n",
        "    if not transcript_segments:\n",
        "        log_prefix = f\"   >> [Audio Chunk {audio_chunk_index + 1}/{total_audio_chunks}]\"\n",
        "        print(f\"{log_prefix} Aucun segment √† traduire (peut-√™tre un sous-chunk vide?).\", flush=True)\n",
        "        return []\n",
        "\n",
        "    start_time = time.time()\n",
        "    num_segments = len(transcript_segments)\n",
        "\n",
        "    log_prefix = f\"   >> [Audio Chunk {audio_chunk_index + 1}/{total_audio_chunks}\"\n",
        "    if segment_chunk_index is not None and total_segment_chunks is not None:\n",
        "        log_prefix += f\" | Segment Chunk {segment_chunk_index + 1}/{total_segment_chunks}\"\n",
        "    log_prefix += \"]\"\n",
        "\n",
        "    # Rendre le log initial plus discret car il y en aura potentiellement beaucoup en parall√®le\n",
        "    # print(f\"{log_prefix} üîÑ Traduction de {num_segments} segments...\", flush=True)\n",
        "\n",
        "    if not api_key or not api_key.startswith(\"AIzaSy\"):\n",
        "         print(f\"{log_prefix} ‚ùå Cl√© API Gemini invalide ou manquante.\", flush=True)\n",
        "         return None # Retourne None explicitement en cas d'erreur de cl√©\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}\"\n",
        "\n",
        "    try:\n",
        "        transcript_str = json.dumps(transcript_segments, ensure_ascii=False, separators=(',', ':'))\n",
        "    except TypeError as e:\n",
        "        print(f\"{log_prefix} ‚ùå Erreur pr√©paration JSON pour API: {e}\", flush=True)\n",
        "        return None # Retourne None en cas d'erreur\n",
        "\n",
        "    prompt = (\n",
        "        \"You are an expert multilingual transcriber and translator.\\n\"\n",
        "        \"INPUT: A JSON array. Each object in the array represents a transcript segment and has keys 'text', 'start', and 'duration'.\\n\"\n",
        "        \"TASK: Process EACH segment object in the input JSON array:\\n\"\n",
        "        \"1. Identify the original language of the 'text' field.\\n\"\n",
        "        \"2. Add a new key 'text_english' containing the English translation of the original 'text'.\\n\"\n",
        "        \"3. Add a new key 'text_french' containing the French translation of the original 'text'.\\n\"\n",
        "        \"4. IMPORTANT: Preserve ALL original keys ('text', 'start', 'duration') and their values.\\n\"\n",
        "        \"OUTPUT: Return ONLY the modified JSON array containing all processed segments. Ensure the output is a single, valid JSON array. Do NOT include any extra text, explanations, or markdown formatting (like ```json ... ```) outside the JSON array itself.\\n\\n\"\n",
        "        \"INPUT JSON:\\n\"\n",
        "        f\"{transcript_str}\"\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": 0.2,\n",
        "            \"maxOutputTokens\": 8192,\n",
        "            \"response_mime_type\": \"application/json\"\n",
        "        },\n",
        "        \"safetySettings\": [\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    max_retries = 3\n",
        "    base_delay = 3\n",
        "    raw_text_response = \"\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        if attempt > 0:\n",
        "            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
        "            # print(f\"{log_prefix} ‚è≥ Tentative Traduction {attempt + 1}/{max_retries} apr√®s {delay:.1f}s d'attente...\", flush=True)\n",
        "            time.sleep(delay)\n",
        "\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=payload, timeout=240)\n",
        "\n",
        "            if response.status_code == 429:\n",
        "                print(f\"{log_prefix} ‚ö†Ô∏è Erreur 429 (Rate Limit API). Tentative {attempt + 1}/{max_retries}. Re-essai...\", flush=True)\n",
        "                time.sleep(20 + random.uniform(0, 5)) # D√©lai plus long pour rate limit\n",
        "                if attempt == max_retries - 1:\n",
        "                     print(f\"{log_prefix} ‚ùå Rate Limit persiste apr√®s {max_retries} tentatives.\", flush=True)\n",
        "                     return None # √âchec final apr√®s retries pour rate limit\n",
        "                continue # Passe √† la tentative suivante\n",
        "\n",
        "            response.raise_for_status() # L√®ve une exception pour les autres erreurs HTTP (4xx, 5xx)\n",
        "            response_data = response.json()\n",
        "\n",
        "            # --- Validation R√©ponse Gemini (inchang√©e) ---\n",
        "            if not response_data.get(\"candidates\"):\n",
        "                prompt_feedback = response_data.get(\"promptFeedback\", {})\n",
        "                block_reason = prompt_feedback.get(\"blockReason\")\n",
        "                safety_ratings = prompt_feedback.get(\"safetyRatings\", [])\n",
        "                error_message = f\"Aucun 'candidates' dans la r√©ponse API.\"\n",
        "                if block_reason: error_message += f\" Raison blocage: {block_reason}.\"\n",
        "                if safety_ratings: error_message += f\" Safety Ratings: {safety_ratings}\"\n",
        "                print(f\"{log_prefix} ‚ùå Erreur API: {error_message}\", flush=True)\n",
        "                if block_reason == \"SAFETY\": return None # Ne pas retenter pour SAFETY block\n",
        "                if attempt == max_retries - 1: return None # √âchec final apr√®s retries\n",
        "                continue # Retenter pour autres erreurs\n",
        "\n",
        "            candidate = response_data[\"candidates\"][0]\n",
        "            finish_reason = candidate.get(\"finishReason\")\n",
        "\n",
        "            if finish_reason not in [\"STOP\", \"MAX_TOKENS\"]:\n",
        "                safety_ratings = candidate.get(\"safetyRatings\", [])\n",
        "                print(f\"{log_prefix} ‚ùå Fin anormale API: {finish_reason}.\", flush=True)\n",
        "                if safety_ratings: print(f\"      -> Safety Ratings: {safety_ratings}\", flush=True)\n",
        "                if finish_reason == \"SAFETY\": return None # Ne pas retenter pour SAFETY finish reason\n",
        "                if attempt == max_retries - 1: return None # √âchec final\n",
        "                continue # Retenter pour autres fins anormales\n",
        "\n",
        "            if not (\"content\" in candidate and \"parts\" in candidate[\"content\"] and\n",
        "                    candidate[\"content\"][\"parts\"] and \"text\" in candidate[\"content\"][\"parts\"][0]):\n",
        "                print(f\"{log_prefix} ‚ùå Structure de contenu de r√©ponse API invalide.\", flush=True)\n",
        "                if attempt == max_retries - 1: return None # √âchec final\n",
        "                continue # Retenter\n",
        "\n",
        "            # --- Extraction et Parsing JSON (inchang√©) ---\n",
        "            raw_text_response = candidate[\"content\"][\"parts\"][0][\"text\"]\n",
        "            json_string = raw_text_response\n",
        "            try:\n",
        "                result_json = json.loads(json_string)\n",
        "            except json.JSONDecodeError as e:\n",
        "                 print(f\"{log_prefix} ‚ùå Erreur d√©codage JSON de la r√©ponse API: {e}\", flush=True)\n",
        "                 print(f\"      R√©ponse brute re√ßue:\\n{raw_text_response[:500]}...\")\n",
        "                 if attempt == max_retries - 1: return None # √âchec final\n",
        "                 continue # Retenter\n",
        "\n",
        "            # --- Validation R√©sultat Pars√© (inchang√©e) ---\n",
        "            if not isinstance(result_json, list):\n",
        "                print(f\"{log_prefix} ‚ùå Le JSON d√©cod√© n'est pas une liste.\", flush=True)\n",
        "                if attempt == max_retries - 1: return None # √âchec final\n",
        "                continue # Retenter\n",
        "\n",
        "            if len(result_json) != num_segments:\n",
        "                print(f\"{log_prefix} ‚ö†Ô∏è Nombre de segments retourn√©s ({len(result_json)}) != entr√©e ({num_segments}). Possiblement tronqu√©?\", flush=True)\n",
        "\n",
        "            if result_json and isinstance(result_json[0], dict):\n",
        "                 if 'text_english' not in result_json[0] or 'text_french' not in result_json[0]:\n",
        "                     print(f\"{log_prefix} ‚ö†Ô∏è Cl√©s traduites manquantes dans le premier segment retourn√©.\", flush=True)\n",
        "\n",
        "            # --- Succ√®s ---\n",
        "            elapsed_time = time.time() - start_time\n",
        "            # Log de succ√®s plus discret\n",
        "            # print(f\"{log_prefix} ‚úÖ Traduction r√©ussie en {elapsed_time:.2f}s.\", flush=True)\n",
        "            if finish_reason == \"MAX_TOKENS\":\n",
        "                 print(f\"{log_prefix} ‚ö†Ô∏è Attention: MAX_TOKENS atteint. Traduction pourrait √™tre incompl√®te.\", flush=True)\n",
        "            return result_json # Retourne la liste des segments traduits\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"{log_prefix} ‚ö†Ô∏è Timeout API (Tentative {attempt + 1}/{max_retries}).\", flush=True)\n",
        "            if attempt == max_retries - 1: return None # √âchec final apr√®s retries\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            # G√©rer les erreurs HTTP qui ne sont pas des rate limits ici\n",
        "            print(f\"{log_prefix} ‚ùå Erreur R√©seau/HTTP API (Tentative {attempt + 1}/{max_retries}): {e}\", flush=True)\n",
        "            if attempt == max_retries - 1: return None # √âchec final apr√®s retries\n",
        "            # Attente avant re-essai pour erreurs g√©n√©riques\n",
        "            time.sleep(base_delay * (2 ** attempt) + random.uniform(0, 1))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{log_prefix} ‚ùå Erreur inattendue durant traduction (Tentative {attempt + 1}/{max_retries}): {e}\", flush=True)\n",
        "            traceback.print_exc() # Imprime la trace pour le d√©bogage\n",
        "            if attempt == max_retries - 1: return None # √âchec final apr√®s retries\n",
        "            # Attente avant re-essai pour erreurs inattendues\n",
        "            time.sleep(base_delay * (2 ** attempt) + random.uniform(0, 1))\n",
        "\n",
        "\n",
        "    # Si la boucle se termine sans succ√®s apr√®s toutes les tentatives\n",
        "    print(f\"{log_prefix} ‚ùå Traduction √âCHOU√âE apr√®s {max_retries} tentatives.\", flush=True)\n",
        "    return None # Retourne None pour indiquer l'√©chec final\n",
        "\n",
        "\n",
        "# =======================================================================\n",
        "# --- Configuration et Ex√©cution Principale (AVEC TRADUCTION CONCURRENTE) ---\n",
        "# =======================================================================\n",
        "print(\"\\n\\n\" + \"=\"*40 + \"\\n--- Configuration Principale ---\\n\" + \"=\"*40)\n",
        "\n",
        "# --- Param√®tres de G√©n√©ral ---\n",
        "youtube_url = \"https://www.youtube.com/watch?v=EDf6I36S-aE&pp=0gcJCYQJAYcqIYzv\" #@param {\"type\":\"string\"}\n",
        "model_size = \"large-v3\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v3\"]\n",
        "gemini_api_key = \"AIzaSyAI2CLDtikFeKi5P6UxgXi9D9bMwYA6l8w\" #@param {\"type\":\"string\"}\n",
        "\n",
        "#@markdown _____\n",
        "\n",
        "#@markdown *Les param√®tres suivent sont configur√©s pour un fonctionnement optimal. Si vous ne savez pas ce qu'ils font, il est g√©n√©ralement judicieux de ne pas les modifier.*\n",
        "\n",
        "# --- Param√®tres de Transcription ---\n",
        "output_filename_base = \"youtube_audio.mp3\"\n",
        "output_path_original = os.path.join(OUTPUT_DIR_V2, output_filename_base)\n",
        "output_chunk_dir = CHUNK_DIR\n",
        "\n",
        "enable_pre_splitting = True #@param {type:\"boolean\"}\n",
        "split_fixed_duration_minutes = 10 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "use_vad_during_transcription = False #@param {type:\"boolean\"}\n",
        "vad_silence_duration_ms = 500 #@param {type:\"slider\", min:100, max:2000, step:50}\n",
        "beam_search_size = 5\n",
        "\n",
        "# --- Param√®tres de Traduction ---\n",
        "enable_segment_chunking = True #@param {type:\"boolean\"}\n",
        "max_segments_per_translation_chunk = 30 #@param {type:\"integer\"}\n",
        "# NOUVEAU PARAM√àTRE pour la concurrence\n",
        "max_concurrent_translation_tasks = 14 #@param {type:\"integer\"} # Nombre max d'appels API Gemini simultan√©s\n",
        "\n",
        "# --- Param√®tres d'Upload ---\n",
        "upload_chunk_url = \"default\"  #@param {\"type\":\"string\"}\n",
        "if upload_chunk_url == \"default\":\n",
        "    upload_chunk_url = \"https://qingplay.pythonanywhere.com/update_transcript_chunk\"\n",
        "\n",
        "enable_incremental_upload = True #@param {type:\"boolean\"}\n",
        "\n",
        "# --- Variables Globales ---\n",
        "original_audio_file_path = None\n",
        "video_info = None\n",
        "final_output_data_local = None\n",
        "json_output_filename_final = None\n",
        "loaded_whisper_model = None\n",
        "\n",
        "# --- Validation Configuration ---\n",
        "print(\"--- Validation Configuration ---\")\n",
        "valid_config = True\n",
        "if not youtube_url or not youtube_url.startswith(\"http\"):\n",
        "    print(\"‚ùå URL YouTube invalide.\")\n",
        "    valid_config = False\n",
        "if not gemini_api_key or gemini_api_key == \"YOUR_GEMINI_API_KEY\":\n",
        "     if enable_incremental_upload or enable_segment_chunking:\n",
        "         print(\"‚ùå Cl√© API Gemini manquante ou non remplac√©e. La traduction et/ou l'upload √©choueront.\")\n",
        "         valid_config = False\n",
        "     else:\n",
        "         print(\"‚ö†Ô∏è Cl√© API Gemini manquante ou non remplac√©e. Traduction et upload seront ignor√©s.\")\n",
        "     gemini_api_key = None\n",
        "if enable_incremental_upload and (not upload_chunk_url or not upload_chunk_url.startswith(\"http\")):\n",
        "    print(\"‚ùå URL d'upload invalide.\")\n",
        "    valid_config = False\n",
        "if enable_segment_chunking and max_segments_per_translation_chunk <= 0:\n",
        "    print(\"‚ùå max_segments_per_translation_chunk doit √™tre positif.\")\n",
        "    valid_config = False\n",
        "if enable_segment_chunking and max_concurrent_translation_tasks <= 0:\n",
        "    print(\"‚ùå max_concurrent_translation_tasks doit √™tre positif.\")\n",
        "    valid_config = False\n",
        "\n",
        "if not valid_config:\n",
        "    print(\"\\nüö´ Erreurs de configuration d√©tect√©es. Arr√™t du script.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"‚úÖ Configuration valid√©e.\")\n",
        "    print(f\"   Chunking Audio: {'Activ√©' if enable_pre_splitting else 'D√©sactiv√©'} (Dur√©e: {split_fixed_duration_minutes} min)\")\n",
        "    print(f\"   Mod√®le Whisper: {model_size}, VAD: {'Activ√©' if use_vad_during_transcription else 'D√©sactiv√©'}\")\n",
        "    print(f\"   Traduction : {'Activ√©e' if gemini_api_key else 'D√©sactiv√©e'}\")\n",
        "    if gemini_api_key:\n",
        "        print(f\"     Chunking Segments: {'Activ√©' if enable_segment_chunking else 'D√©sactiv√©'} (Max Segments/Chunk: {max_segments_per_translation_chunk if enable_segment_chunking else 'N/A'})\")\n",
        "        if enable_segment_chunking:\n",
        "             print(f\"     T√¢ches traduction concurrentes max: {max_concurrent_translation_tasks}\")\n",
        "    print(f\"   Upload Incr√©mental: {'Activ√©' if enable_incremental_upload else 'D√©sactiv√©'}\")\n",
        "\n",
        "\n",
        "# --- Ex√©cution ---\n",
        "try:\n",
        "    # ========================================\n",
        "    # √âTAPE 1: Pr√©paration & Chargement Mod√®le\n",
        "    # ========================================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 1: PR√âPARATION & CHARGEMENT MOD√àLE\\n\" + \"=\"*40)\n",
        "    start_step1 = time.time()\n",
        "\n",
        "    original_audio_file_path, video_info = download_youtube_audio_improved(youtube_url, output_path_original)\n",
        "\n",
        "    if not original_audio_file_path or not video_info:\n",
        "        raise ValueError(\"√âchec du t√©l√©chargement audio ou de la r√©cup√©ration des m√©tadonn√©es.\")\n",
        "\n",
        "    safe_video_id = video_info.get(\"video_id\", \"UNKNOWN_ID\").replace(\"-\", \"_\")\n",
        "\n",
        "    print(\"\\n--- Infos Vid√©o R√©cup√©r√©es ---\")\n",
        "    print(f\"   ID: {video_info.get('video_id', 'N/A')}\")\n",
        "    print(f\"   Titre: {video_info.get('title', 'N/A')}\")\n",
        "    print(f\"   Cha√Æne: {video_info.get('channel_name', 'N/A')}\")\n",
        "    print(f\"   Dur√©e: {video_info.get('duration', 'N/A')}s\")\n",
        "    print(\"----------------------------\\n\")\n",
        "\n",
        "    audio_files_to_process = []\n",
        "    chunk_offsets = []\n",
        "\n",
        "    if enable_pre_splitting:\n",
        "        split_secs = split_fixed_duration_minutes * 60\n",
        "        paths, offs = split_audio_by_fixed_duration(original_audio_file_path, output_chunk_dir, split_secs)\n",
        "        if paths:\n",
        "            audio_files_to_process = paths\n",
        "            chunk_offsets = offs\n",
        "            print(f\"‚úÖ Pr√©-d√©coupage activ√©. {len(paths)} chunks audio √† traiter.\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Le d√©coupage a √©chou√©. Traitement du fichier audio entier.\")\n",
        "            audio_files_to_process = [original_audio_file_path]\n",
        "            chunk_offsets = [0.0]\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Pr√©-d√©coupage d√©sactiv√©. Traitement du fichier audio entier.\")\n",
        "        audio_files_to_process = [original_audio_file_path]\n",
        "        chunk_offsets = [0.0]\n",
        "\n",
        "    total_audio_chunks = len(audio_files_to_process)\n",
        "    print(f\"Nombre total de fichiers audio √† traiter: {total_audio_chunks}\")\n",
        "\n",
        "    # --- Chargement Mod√®le Whisper ---\n",
        "    print(\"\\n--- Chargement Mod√®le Whisper ---\")\n",
        "    print(f\"Mod√®le demand√©: {model_size}\")\n",
        "    start_load = time.time()\n",
        "    device = \"cuda\" if IS_GPU_AVAILABLE else \"cpu\"\n",
        "    compute_type = \"default\"\n",
        "\n",
        "    if IS_GPU_AVAILABLE:\n",
        "        major, _ = torch.cuda.get_device_capability(0)\n",
        "        if major >= 8: compute_type = \"bfloat16\"; print(\"   Utilisation compute_type: bfloat16\")\n",
        "        else: compute_type = \"float16\"; print(\"   Utilisation compute_type: float16\")\n",
        "    else: compute_type = \"int8\"; print(\"   Utilisation compute_type: int8 (CPU)\")\n",
        "\n",
        "    try:\n",
        "        loaded_whisper_model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "        load_time = time.time() - start_load\n",
        "        print(f\"‚úÖ Mod√®le '{model_size}' charg√© sur {device} ({compute_type}) en {load_time:.2f}s.\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"√âchec chargement mod√®le Whisper: {e}\")\n",
        "\n",
        "    step1_time = time.time() - start_step1\n",
        "    print(f\"‚è±Ô∏è Temps √âtape 1 (Pr√©paration): {step1_time:.2f}s\")\n",
        "\n",
        "    # ================================================\n",
        "    # √âTAPE 2: Boucle de Traitement & Upload Intercal√© (MODIFI√âE POUR CONCURRENCE)\n",
        "    # ================================================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 2: TRAITEMENT & UPLOAD INTERCAL√âS\\n\" + \"=\"*40)\n",
        "    all_final_segments_local = []\n",
        "    total_transcribed_segments = 0\n",
        "    total_translated_segments_ok = 0\n",
        "    successful_uploads = 0\n",
        "    failed_uploads = 0\n",
        "    failed_translation_audio_chunks = []\n",
        "    global_start_proc = time.time()\n",
        "\n",
        "    if not audio_files_to_process:\n",
        "        print(\"üö´ Aucun fichier audio √† traiter.\")\n",
        "    else:\n",
        "        for i, file_path in enumerate(audio_files_to_process):\n",
        "            chunk_start_time = time.time()\n",
        "            current_offset = chunk_offsets[i]\n",
        "            print(f\"\\n--- Traitement Chunk Audio {i + 1}/{total_audio_chunks}: {os.path.basename(file_path)} ---\")\n",
        "\n",
        "            # --- 2.1 Transcription (Inchang√©) ---\n",
        "            transcript_result = transcribe_audio_faster(\n",
        "                file_path, loaded_whisper_model, current_offset,\n",
        "                beam_search_size, use_vad_during_transcription, vad_silence_duration_ms\n",
        "            )\n",
        "            current_chunk_segments_transcribed = transcript_result.get(\"segments\", [])\n",
        "\n",
        "            if not current_chunk_segments_transcribed:\n",
        "                print(f\"   -> Aucun segment transcrit pour ce chunk audio.\")\n",
        "                chunk_end_time = time.time()\n",
        "                print(f\"   ‚è±Ô∏è Temps traitement Chunk Audio {i+1}: {chunk_end_time - chunk_start_time:.2f}s\")\n",
        "                continue\n",
        "\n",
        "            num_transcribed = len(current_chunk_segments_transcribed)\n",
        "            total_transcribed_segments += num_transcribed\n",
        "            print(f\"   -> Transcrit {num_transcribed} segments.\")\n",
        "\n",
        "            # --- 2.2 Traduction (avec gestion concurrence si activ√©e) ---\n",
        "            aggregated_translated_segments_for_audio_chunk = []\n",
        "            translation_failed_for_this_audio_chunk = False\n",
        "            translation_api_calls = 0\n",
        "            translation_start_time = time.time()\n",
        "\n",
        "\n",
        "            if not gemini_api_key:\n",
        "                print(\"   >> ‚ö†Ô∏è Cl√© API Gemini non fournie. Traduction ignor√©e.\")\n",
        "                all_final_segments_local.extend(current_chunk_segments_transcribed)\n",
        "            else:\n",
        "                # --- 2.2.1 Segment Chunking & Concurrent Execution ---\n",
        "                if enable_segment_chunking:\n",
        "                    segment_sub_chunks = list(chunk_list(current_chunk_segments_transcribed, max_segments_per_translation_chunk))\n",
        "                    total_segment_sub_chunks = len(segment_sub_chunks)\n",
        "                    print(f\"   >> Pr√©paration traduction pour {num_transcribed} segments en {total_segment_sub_chunks} sous-chunk(s).\")\n",
        "                    print(f\"      (Utilisation de max {max_concurrent_translation_tasks} workers concurrents)\")\n",
        "\n",
        "                    # Dictionnaire pour stocker les r√©sultats dans l'ordre\n",
        "                    # Cl√©: index du sous-chunk (j), Valeur: r√©sultat de la traduction (liste ou None)\n",
        "                    futures_results = {}\n",
        "                    futures_map = {} # Pour mapper Future -> index j\n",
        "\n",
        "                    # Utilisation de ThreadPoolExecutor pour les appels API concurrents\n",
        "                    with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_translation_tasks) as executor:\n",
        "                        # Soumettre toutes les t√¢ches de traduction\n",
        "                        for j, segment_sub_chunk in enumerate(segment_sub_chunks):\n",
        "                             # V√©rifier si le sous-chunk n'est pas vide avant de soumettre\n",
        "                             if segment_sub_chunk:\n",
        "                                 future = executor.submit(\n",
        "                                     translate_audio_chunk_segments,\n",
        "                                     segment_sub_chunk,\n",
        "                                     gemini_api_key,\n",
        "                                     i, # Audio chunk index (0-based)\n",
        "                                     total_audio_chunks,\n",
        "                                     j, # Segment sub-chunk index (0-based)\n",
        "                                     total_segment_sub_chunks\n",
        "                                 )\n",
        "                                 futures_map[future] = j # Stocker l'index associ√© au future\n",
        "                             else:\n",
        "                                 # G√©rer le cas d'un sous-chunk vide (ne devrait pas arriver avec chunk_list sauf si liste initiale vide)\n",
        "                                 futures_results[j] = [] # Un sous-chunk vide donne un r√©sultat vide\n",
        "\n",
        "                        translation_api_calls = len(futures_map) # Nombre d'appels API r√©els\n",
        "                        print(f\"      >> {translation_api_calls} t√¢ches de traduction soumises...\")\n",
        "\n",
        "                        # R√©cup√©rer les r√©sultats au fur et √† mesure qu'ils arrivent (ou en ordre si on it√®re sur futures_map.keys())\n",
        "                        # Utiliser as_completed peut donner une impression de r√©activit√© mais m√©lange l'ordre des logs\n",
        "                        # Ici, on attend la fin de toutes et on r√©cup√®re dans l'ordre pour la coh√©rence\n",
        "                        processed_count = 0\n",
        "                        for future in concurrent.futures.as_completed(futures_map):\n",
        "                             j_index = futures_map[future] # R√©cup√©rer l'index original\n",
        "                             try:\n",
        "                                 result = future.result() # R√©cup√®re le r√©sultat (list ou None) ou l√®ve une exception\n",
        "                                 futures_results[j_index] = result\n",
        "                                 if result is None:\n",
        "                                      translation_failed_for_this_audio_chunk = True\n",
        "                                      # Le log d'erreur est d√©j√† dans translate_audio_chunk_segments\n",
        "                                 processed_count += 1\n",
        "                                 # Afficher la progression de la traduction concurrente\n",
        "                                 sys.stdout.write(f\"\\r      >> Progression traduction (t√¢ches termin√©es): {processed_count}/{translation_api_calls} \")\n",
        "                                 sys.stdout.flush()\n",
        "\n",
        "                             except Exception as exc:\n",
        "                                 # G√©rer les exceptions non intercept√©es dans translate_audio_chunk_segments\n",
        "                                 print(f'\\n      >> ‚ùå Erreur lors de la traduction du sous-chunk {j_index + 1}: {exc}')\n",
        "                                 traceback.print_exc() # Pour plus de d√©tails\n",
        "                                 futures_results[j_index] = None # Marquer comme √©chec\n",
        "                                 translation_failed_for_this_audio_chunk = True\n",
        "                                 processed_count += 1\n",
        "                                 sys.stdout.write(f\"\\r      >> Progression traduction (t√¢ches termin√©es): {processed_count}/{translation_api_calls} \")\n",
        "                                 sys.stdout.flush()\n",
        "\n",
        "                        sys.stdout.write(\"\\n\") # Nouvelle ligne apr√®s la progression\n",
        "                        sys.stdout.flush()\n",
        "\n",
        "\n",
        "                    # Agr√©ger les r√©sultats dans l'ordre original des sous-chunks\n",
        "                    for j in range(total_segment_sub_chunks):\n",
        "                         result_sub_chunk = futures_results.get(j)\n",
        "                         if result_sub_chunk is not None:\n",
        "                              aggregated_translated_segments_for_audio_chunk.extend(result_sub_chunk)\n",
        "                         # Si result_sub_chunk est None, l'√©chec a d√©j√† √©t√© marqu√©\n",
        "\n",
        "                # --- 2.2.2 No Segment Chunking (S√©quentiel) ---\n",
        "                else:\n",
        "                     print(f\"   >> Traduction s√©quentielle de {num_transcribed} segments (Chunking Segment d√©sactiv√©).\")\n",
        "                     translation_api_calls = 1\n",
        "                     translated_segments = translate_audio_chunk_segments(\n",
        "                         current_chunk_segments_transcribed,\n",
        "                         gemini_api_key,\n",
        "                         i, total_audio_chunks\n",
        "                     )\n",
        "                     if translated_segments is not None:\n",
        "                         aggregated_translated_segments_for_audio_chunk = translated_segments\n",
        "                     else:\n",
        "                         translation_failed_for_this_audio_chunk = True\n",
        "                         # Le log d'erreur est d√©j√† dans translate_audio_chunk_segments\n",
        "\n",
        "\n",
        "                translation_end_time = time.time()\n",
        "                translation_duration = translation_end_time - translation_start_time\n",
        "                print(f\"   >> Traduction termin√©e pour Audio Chunk {i+1} en {translation_duration:.2f}s ({translation_api_calls} appels API).\")\n",
        "\n",
        "\n",
        "                # --- Post-Translation Handling (Inchang√©) ---\n",
        "                if translation_failed_for_this_audio_chunk:\n",
        "                    failed_translation_audio_chunks.append(i + 1)\n",
        "                    print(f\"   >> ‚ÑπÔ∏è Sauvegarde locale utilisera les segments transcrits originaux pour Audio Chunk {i+1} d√ª √† un √©chec.\")\n",
        "                    all_final_segments_local.extend(current_chunk_segments_transcribed)\n",
        "                else:\n",
        "                     num_translated = len(aggregated_translated_segments_for_audio_chunk)\n",
        "                     total_translated_segments_ok += num_translated\n",
        "                     all_final_segments_local.extend(aggregated_translated_segments_for_audio_chunk)\n",
        "                     # print(f\"   >> {num_translated} segments traduits agr√©g√©s pour Audio Chunk {i+1}.\") # Log un peu redondant\n",
        "\n",
        "\n",
        "            # --- 2.3 Upload Incr√©mental (Inchang√© logiquement, op√®re sur le r√©sultat agr√©g√©) ---\n",
        "            if enable_incremental_upload and aggregated_translated_segments_for_audio_chunk and not translation_failed_for_this_audio_chunk and gemini_api_key:\n",
        "                print(f\"      >> ‚¨ÜÔ∏è Tentative d'upload pour Chunk Audio {i + 1} ({len(aggregated_translated_segments_for_audio_chunk)} segments traduits)...\", flush=True)\n",
        "                upload_payload = {\n",
        "                    \"video_id\": video_info[\"video_id\"], \"description\": video_info[\"description\"],\n",
        "                    \"channel_name\": video_info[\"channel_name\"], \"channel_url\": video_info[\"channel_url\"],\n",
        "                    \"segments\": aggregated_translated_segments_for_audio_chunk,\n",
        "                    \"chunk_index\": i, \"total_chunks\": total_audio_chunks,\n",
        "                    \"title\": video_info[\"title\"] if i == 0 else None,\n",
        "                }\n",
        "                upload_success = False\n",
        "                try:\n",
        "                    response = requests.post(upload_chunk_url, json=upload_payload, timeout=60)\n",
        "                    response.raise_for_status()\n",
        "                    try: server_message = response.json().get(\"message\", response.text[:100])\n",
        "                    except json.JSONDecodeError: server_message = response.text[:100]\n",
        "                    print(f\"      >> ‚úÖ Upload Chunk Audio {i + 1} r√©ussi (Status: {response.status_code}). Serveur: {server_message}...\", flush=True)\n",
        "                    successful_uploads += 1\n",
        "                    upload_success = True\n",
        "                except requests.exceptions.Timeout: print(f\"      >> ‚ùå Upload Chunk Audio {i + 1} √©chou√© (Timeout).\", flush=True)\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"      >> ‚ùå Upload Chunk Audio {i + 1} √©chou√© (Erreur HTTP/R√©seau): {e}\", flush=True)\n",
        "                    if e.response is not None: print(f\"         R√©ponse serveur ({e.response.status_code}): {e.response.text[:200]}...\")\n",
        "                except Exception as e: print(f\"      >> ‚ùå Erreur inattendue pendant upload Chunk Audio {i + 1}: {e}\", flush=True); traceback.print_exc()\n",
        "                if not upload_success: failed_uploads += 1\n",
        "            elif not enable_incremental_upload: print(f\"      >> ‚ÑπÔ∏è Upload incr√©mental d√©sactiv√©.\")\n",
        "            elif not gemini_api_key: print(f\"      >> ‚ÑπÔ∏è Upload ignor√© (traduction d√©sactiv√©e).\")\n",
        "            elif translation_failed_for_this_audio_chunk: print(f\"      >> ‚ÑπÔ∏è Upload ignor√© (√©chec traduction).\")\n",
        "            elif not aggregated_translated_segments_for_audio_chunk: print(f\"      >> ‚ÑπÔ∏è Aucun segment traduit √† uploader.\")\n",
        "\n",
        "            chunk_end_time = time.time()\n",
        "            print(f\"   ‚è±Ô∏è Temps total traitement Chunk Audio {i+1}: {chunk_end_time - chunk_start_time:.2f}s\")\n",
        "\n",
        "\n",
        "    global_end_proc = time.time()\n",
        "\n",
        "    # --- R√©sum√© Traitement ---\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 2: R√âSUM√â DU TRAITEMENT\\n\" + \"=\"*40)\n",
        "    total_proc_time = global_end_proc - global_start_proc\n",
        "    print(f\"‚è±Ô∏è Temps total Traitement & Upload (√âtape 2): {total_proc_time:.2f}s.\")\n",
        "    print(f\"üìä Total Chunks Audio Trait√©s: {total_audio_chunks}\")\n",
        "    print(f\"üìä Total Segments Transcrits: {total_transcribed_segments}\")\n",
        "    if gemini_api_key:\n",
        "        print(f\"üìä Total Segments agr√©g√©s apr√®s traduction r√©ussie: {total_translated_segments_ok}\")\n",
        "        failed_chunks_list = sorted(list(set(failed_translation_audio_chunks)))\n",
        "        if failed_chunks_list:\n",
        "             print(f\"‚ùå Traduction √©chou√©e (partiellement ou totalement) pour {len(failed_chunks_list)} chunk(s) audio: {', '.join(map(str, failed_chunks_list))}\")\n",
        "             print(f\"   (Sauvegarde locale utilise fallback transcrit pour eux)\")\n",
        "        elif total_transcribed_segments > 0 : print(f\"‚úÖ Traduction r√©ussie pour tous les chunks audio avec segments.\")\n",
        "        else: print(f\"‚ÑπÔ∏è Aucune traduction (pas de segments transcrits).\")\n",
        "    else: print(\"‚ÑπÔ∏è Traduction ignor√©e (pas de cl√© API).\")\n",
        "    print(f\"üíæ Total Segments agr√©g√©s pour sauvegarde locale: {len(all_final_segments_local)}\")\n",
        "    if enable_incremental_upload:\n",
        "        print(f\"‚òÅÔ∏è Uploads Incr√©mentaux R√©ussis (par chunk audio): {successful_uploads}\")\n",
        "        print(f\"‚òÅÔ∏è Uploads Incr√©mentaux √âchou√©s (par chunk audio): {failed_uploads}\")\n",
        "    else: print(\"‚òÅÔ∏è Upload Incr√©mental d√©sactiv√©.\")\n",
        "\n",
        "    # ========================================\n",
        "    # √âTAPE 3: SAUVEGARDE LOCALE FINALE (Inchang√©e)\n",
        "    # ========================================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 3: SAUVEGARDE LOCALE FINALE\\n\" + \"=\"*40)\n",
        "    start_step3 = time.time()\n",
        "    if all_final_segments_local:\n",
        "        final_output_data_local = {\"segments\": all_final_segments_local}\n",
        "        if video_info:\n",
        "             metadata_to_include = [\"video_id\", \"title\", \"channel_name\", \"channel_url\", \"duration\", \"original_url\", \"upload_date\"]\n",
        "             final_output_data_local.update({k: v for k, v in video_info.items() if k in metadata_to_include and v is not None})\n",
        "\n",
        "        split_suffix = f\"_split{split_fixed_duration_minutes}min\" if enable_pre_splitting else \"_noSplit\"\n",
        "        vad_suffix = \"_VAD\" if use_vad_during_transcription else \"_noVAD\"\n",
        "        # Suffixe pour chunking de segments et concurrence (si activ√©)\n",
        "        segchunk_suffix = \"\"\n",
        "        if enable_segment_chunking and gemini_api_key:\n",
        "            segchunk_suffix = f\"_segChunk{max_segments_per_translation_chunk}_conc{max_concurrent_translation_tasks}\"\n",
        "        elif gemini_api_key: # Traduction active mais pas de chunking de segments\n",
        "            segchunk_suffix = \"_segFull\"\n",
        "\n",
        "        failed_chunks_list = sorted(list(set(failed_translation_audio_chunks)))\n",
        "        if not gemini_api_key: status_suffix = \"_transcribed_only\"\n",
        "        elif not failed_chunks_list and total_transcribed_segments > 0: status_suffix = \"_fully_translated\"\n",
        "        elif not failed_chunks_list and total_transcribed_segments == 0: status_suffix = \"_no_segments_found\" # Cas o√π il n'y avait rien √† transcrire/traduire\n",
        "        elif failed_chunks_list and total_translated_segments_ok > 0: status_suffix = \"_partially_translated\"\n",
        "        elif failed_chunks_list: status_suffix = \"_translation_failed\" # Aucun segment traduit avec succ√®s, ou tous les chunks avec segments ont √©chou√©\n",
        "        else: status_suffix = \"_unknown_state\"\n",
        "\n",
        "        json_output_filename_final = os.path.join(\n",
        "            OUTPUT_DIR_V2,\n",
        "            f\"{safe_video_id}__{model_size}{split_suffix}{vad_suffix}{segchunk_suffix}{status_suffix}.json\"\n",
        "        )\n",
        "\n",
        "        print(f\"Tentative de sauvegarde du r√©sultat local agr√©g√© vers:\")\n",
        "        print(f\"   {json_output_filename_final}\")\n",
        "        try:\n",
        "            with open(json_output_filename_final, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(final_output_data_local, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"‚úÖ R√©sultat local complet sauvegard√© avec succ√®s.\")\n",
        "            if failed_uploads > 0: print(f\"   -> ‚ö†Ô∏è Attention: {failed_uploads} upload(s) incr√©mentaux ont √©chou√©.\")\n",
        "            if failed_chunks_list: print(f\"   -> ‚ÑπÔ∏è Ce fichier contient les segments transcrits originaux pour les chunks audio dont la traduction a √©chou√©.\")\n",
        "        except IOError as e: print(f\"‚ùå Erreur d'√©criture lors de la sauvegarde JSON locale: {e}\")\n",
        "        except Exception as e: print(f\"‚ùå Erreur inattendue lors de la sauvegarde JSON locale: {e}\"); traceback.print_exc()\n",
        "    else: print(\"‚ùå Aucune donn√©e de segment n'a √©t√© collect√©e. Pas de fichier JSON local sauvegard√©.\")\n",
        "    step3_time = time.time() - start_step3\n",
        "    print(f\"‚è±Ô∏è Temps √âtape 3 (Sauvegarde): {step3_time:.2f}s\")\n",
        "\n",
        "except ValueError as ve: print(f\"\\n‚ùå ERREUR DE CONFIGURATION OU DE PROCESSUS: {ve}\"); traceback.print_exc()\n",
        "except Exception as e: print(f\"\\n‚ùå ERREUR GLOBALE INATTENDUE: {e}\"); traceback.print_exc()\n",
        "finally:\n",
        "    # ===========================\n",
        "    # √âTAPE 4: NETTOYAGE (Optionnel - Inchang√©)\n",
        "    # ===========================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 4: NETTOYAGE (OPTIONNEL)\\n\" + \"=\"*40)\n",
        "    cleanup = False # Mettre √† True pour supprimer les fichiers interm√©diaires\n",
        "    if cleanup:\n",
        "        print(\"--- Nettoyage des fichiers temporaires ---\")\n",
        "        if original_audio_file_path and os.path.exists(original_audio_file_path):\n",
        "            try: os.remove(original_audio_file_path); print(f\"üóëÔ∏è Fichier audio original supprim√©.\")\n",
        "            except Exception as e: print(f\"‚ùå Erreur suppression fichier audio original: {e}\")\n",
        "        if os.path.exists(output_chunk_dir):\n",
        "             try: shutil.rmtree(output_chunk_dir); print(f\"üóëÔ∏è Dossier des chunks audio supprim√©.\")\n",
        "             except Exception as e: print(f\"‚ùå Erreur suppression dossier chunks: {e}\")\n",
        "        print(\"--- Nettoyage termin√© ---\")\n",
        "    else:\n",
        "        print(\"--- Nettoyage d√©sactiv√© ---\")\n",
        "        if original_audio_file_path and os.path.exists(original_audio_file_path): print(f\"‚ÑπÔ∏è Fichier audio original conserv√©: {original_audio_file_path}\")\n",
        "        if os.path.exists(output_chunk_dir) and os.listdir(output_chunk_dir): print(f\"‚ÑπÔ∏è Chunks audio conserv√©s dans: {output_chunk_dir}\")\n",
        "        if json_output_filename_final and os.path.exists(json_output_filename_final): print(f\"‚ÑπÔ∏è Fichier JSON final conserv√©: {json_output_filename_final}\")\n",
        "\n",
        "    print(\"\\nüèÅ Script complet termin√©.\")\n",
        "    total_runtime = time.time() - global_start_proc if 'global_start_proc' in locals() else 0\n",
        "    if total_runtime > 0:\n",
        "        if 'start_step1' in locals():\n",
        "           full_runtime = time.time() - start_step1\n",
        "           print(f\"‚è±Ô∏è Dur√©e totale d'ex√©cution du script (toutes √©tapes): {full_runtime:.2f}s\")\n",
        "        else: print(f\"‚è±Ô∏è Dur√©e d'ex√©cution (√âtape 2 et suivantes): {total_runtime:.2f}s\")\n",
        "\n",
        "    if enable_incremental_upload and video_info and video_info.get(\"video_id\") != \"UNKNOWN_ID\" and successful_uploads > 0:\n",
        "        print(f\"\\nüîó Lien potentiel pour consulter le r√©sultat sur le serveur:\")\n",
        "        print(f\"   https://qingplay.pythonanywhere.com/vid/{video_info['video_id']}\")\n",
        "    elif enable_incremental_upload and failed_uploads > 0: print(f\"\\n‚ö†Ô∏è Certains uploads ont √©chou√©, le r√©sultat sur le serveur peut √™tre incomplet.\")\n",
        "    elif enable_incremental_upload and successful_uploads == 0 and total_transcribed_segments > 0 and gemini_api_key: print(f\"\\n‚ÑπÔ∏è Aucun upload r√©ussi (v√©rifier √©checs traduction/upload).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d48129ecb82e4def940ab46552119416",
            "ce22a87a00ad4eadbda7a9aaa7db0aa5",
            "acdd87c665284f9b9db3f1fe94425f75",
            "7526d70cd55344bd91428697091bb15c",
            "1469cfba43a343e4aa8c8a9089846678",
            "fc4ecb0c285a467d8626dd91f8543ccf",
            "62e4251d8b9d42c194e2e60ca81b621b",
            "24a261c7925f41b2ac97bfac8e307054",
            "f8a050d075ba4f9696e3d61fa8e34766",
            "ff96605356fa4b918fd400a188539eb0",
            "8fed5d7e2321467490d2b803e1db1708",
            "e2f73b55932a4ccab1b9ea4251c74691",
            "9225d6af04494c54b921b856b8aef838",
            "08a3d7afce5841928d4ee049b0138768",
            "5c3e92d4db8548578792c357341bf6cf",
            "8ba5c9795e6a49b4b31a3b450a69c2cc",
            "26bcfdbf9cbc4d4996644bececf55ea7",
            "24159900f732477ea4f43d6ae5b77b4a",
            "28091c5ea0794fb1afe15799e2d3a0f7",
            "7497d7c9fb8f4821981ef6e69e18b0f8",
            "5d9703f174bb4578a87e149082439912",
            "4cd3bdec33a24dd0bda7b91c1278ef36",
            "2e3f0af0cb2b4e8aa6d067bff07ef6c0",
            "06b6e47664d142d487800d35e36e57b6",
            "76b5534fc817401896a81e1ce800a14b",
            "93f1f4f37d534141958c32602e67ff56",
            "c82c25ef01ce4c2d97633ed68b8d417d",
            "4d0454f62e37455ca05d8c5efdd2f1ca",
            "6f8df4fff387417caf6b83bb664a1b40",
            "ef14a0a096b34ec9b143edc0c7ab2ff6",
            "4fd4db0014d341a98b39eddb9a274c41",
            "8f27022e86b841d892ba2ee1dfd415ce",
            "65a6cd188c374d5bbb66a9e21d59c85d",
            "9d2ef692b95d44a4aa1bbf54822518ba",
            "c32f0ba880a64b46a81a35c67eeca347",
            "65b9b14be52741899899da0e14a78690",
            "d29275c8d5ab41339b516e4e67c1bb35",
            "b9c46ffd5a3b41029508664d5a28491b",
            "e95b62c25cb54e99a8ab442393910ad7",
            "400896d7239240b380ca6f1048365368",
            "f94a8299d7264de2b87cba11c7410070",
            "08d364b65a8a4f2989c9889cf3ce1e4c",
            "1830a152ab054383ac8a1e55785cd02b",
            "621db47e2e8e444c8b2baa36055dda26",
            "8a32c8415b4348099f7f878aa0810bb4",
            "dd77afc989e144d3a1a74d1ac9e3c622",
            "d52e687fd3ec45a984478737f24f5748",
            "e62356eee274499cbfdc726991e471fd",
            "a8cd999c6daf474db8c06e37b18c8214",
            "9d6a1badd4214a289c981d3b2bfa3983",
            "c493e4b16a9f42fdb71e62551e6e185d",
            "18751380a2e644958863c0b9e98b47b5",
            "f3c8628bb02b4f02abded625ab3905c3",
            "6be2e376cd924a7caec2e6ed71b08635",
            "ea8471c6aeff43dbb772df06724f7422"
          ]
        },
        "collapsed": true,
        "cellView": "form",
        "id": "uIZnw1qHD5vY",
        "outputId": "2b5b4af2-81f3-4922-af02-99b7191cb916"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Cleanup ---\n",
            "Cleanup done.\n",
            "\n",
            "--- Library Imports & Checks ---\n",
            "‚úÖ PyTorch install√©.\n",
            "‚úÖ faster-whisper install√©.\n",
            "‚úÖ pydub install√©.\n",
            "\n",
            "--- GPU Check ---\n",
            "‚úÖ GPU d√©tect√©: True\n",
            "   GPU Name: Tesla T4\n",
            "   Compute Capability: 7.5\n",
            "\n",
            "--- Directories ---\n",
            "‚úÖ Dossier chunks pr√™t: /content/audio_chunks\n",
            "‚úÖ Dossier sortie pr√™t: /content/audio_output_optimized_v2\n",
            "\n",
            "--- D√©finition Fonctions Transcription ---\n",
            "\n",
            "--- D√©finition Fonctions Traduction ---\n",
            "\n",
            "\n",
            "========================================\n",
            "--- Configuration Principale ---\n",
            "========================================\n",
            "--- Validation Configuration ---\n",
            "‚úÖ Configuration valid√©e.\n",
            "   Chunking Audio: Activ√© (Dur√©e: 10 min)\n",
            "   Mod√®le Whisper: large-v3, VAD: D√©sactiv√©\n",
            "   Traduction : Activ√©e\n",
            "     Chunking Segments: Activ√© (Max Segments/Chunk: 30)\n",
            "     T√¢ches traduction concurrentes max: 14\n",
            "   Upload Incr√©mental: Activ√©\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 1: PR√âPARATION & CHARGEMENT MOD√àLE\n",
            "========================================\n",
            "\n",
            "--- T√©l√©chargement Audio depuis YouTube ---\n",
            "URL: https://www.youtube.com/watch?v=EDf6I36S-aE&pp=0gcJCYQJAYcqIYzv\n",
            "Destination: /content/audio_output_optimized_v2/youtube_audio.mp3\n",
            "‚ÑπÔ∏è R√©cup√©ration m√©tadonn√©es...\n",
            "‚úÖ M√©tadonn√©es r√©cup√©r√©es.\n",
            "üîÑ V√©rif/T√©l√©chargement audio -> youtube_audio.mp3...\n",
            "‚ÑπÔ∏è Fichier 'youtube_audio.mp3' absent. T√©l√©chargement...\n",
            "‚úÖ Audio t√©l√©charg√© avec succ√®s.\n",
            "\n",
            "--- Infos Vid√©o R√©cup√©r√©es ---\n",
            "   ID: EDf6I36S-aE\n",
            "   Titre: üáØüáµ Âú®Êó•Êú¨Â§ßÈò™ÂøÖÂêÉÁöÑÈ£üÁâ© üòù ÂéªÁúã MAMA Awards ü•π Êâ≠ËõãÊâ≠‰∫Ü‰ªÄÈ∫ºÊù±Ë•ø ü§´ ÔΩú MAYHO „Äê ÁæéÂ•ΩÁöÑ‰∏ÄÂ§© „Äë\n",
            "   Cha√Æne: May Ho\n",
            "   Dur√©e: 3083s\n",
            "----------------------------\n",
            "\n",
            "\n",
            "üîä D√©coupage audio 'youtube_audio.mp3' en chunks de 600s...\n",
            "   Chargement de l'audio avec pydub...\n",
            "   Audio charg√© (3082.81s). D√©coupage en cours...\n",
            "‚úÖ D√©coupage termin√©: 6 chunks cr√©√©s dans /content/audio_chunks\n",
            "‚úÖ Pr√©-d√©coupage activ√©. 6 chunks audio √† traiter.\n",
            "Nombre total de fichiers audio √† traiter: 6\n",
            "\n",
            "--- Chargement Mod√®le Whisper ---\n",
            "Mod√®le demand√©: large-v3\n",
            "   Utilisation compute_type: float16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/2.39k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d48129ecb82e4def940ab46552119416"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2f73b55932a4ccab1b9ea4251c74691"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e3f0af0cb2b4e8aa6d067bff07ef6c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d2ef692b95d44a4aa1bbf54822518ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocabulary.json:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a32c8415b4348099f7f878aa0810bb4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mod√®le 'large-v3' charg√© sur cuda (float16) en 20.13s.\n",
            "‚è±Ô∏è Temps √âtape 1 (Pr√©paration): 200.53s\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 2: TRAITEMENT & UPLOAD INTERCAL√âS\n",
            "========================================\n",
            "\n",
            "--- Traitement Chunk Audio 1/6: chunk_0000_start0_000s_dur600_000s.mp3 ---\n",
            "\n",
            "üéôÔ∏è Transcription: chunk_0000_start0_000s_dur600_000s.mp3 (Offset Global: 0.000s)\n",
            "   Infos chunk d√©tect√©es: Lang='zh' (Conf: 0.92), Dur√©e: 600.00s\n",
            "   Traitement des segments...\n",
            "   Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% \n",
            "   üïí Transcription du chunk termin√©e en 99.53s. 301 segments trouv√©s.\n",
            "   -> Transcrit 301 segments.\n",
            "   >> Pr√©paration traduction pour 301 segments en 11 sous-chunk(s).\n",
            "      (Utilisation de max 14 workers concurrents)\n",
            "      >> 11 t√¢ches de traduction soumises...\n",
            "      >> Progression traduction (t√¢ches termin√©es): 11/11 \n",
            "   >> Traduction termin√©e pour Audio Chunk 1 en 11.46s (11 appels API).\n",
            "      >> ‚¨ÜÔ∏è Tentative d'upload pour Chunk Audio 1 (301 segments traduits)...\n",
            "      >> ‚úÖ Upload Chunk Audio 1 r√©ussi (Status: 201). Serveur: New transcript created successfully...\n",
            "   ‚è±Ô∏è Temps total traitement Chunk Audio 1: 111.38s\n",
            "\n",
            "--- Traitement Chunk Audio 2/6: chunk_0001_start600_000s_dur600_000s.mp3 ---\n",
            "\n",
            "üéôÔ∏è Transcription: chunk_0001_start600_000s_dur600_000s.mp3 (Offset Global: 600.000s)\n",
            "   Infos chunk d√©tect√©es: Lang='zh' (Conf: 0.94), Dur√©e: 600.00s\n",
            "   Traitement des segments...\n",
            "   Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% \n",
            "   üïí Transcription du chunk termin√©e en 100.89s. 344 segments trouv√©s.\n",
            "   -> Transcrit 344 segments.\n",
            "   >> Pr√©paration traduction pour 344 segments en 12 sous-chunk(s).\n",
            "      (Utilisation de max 14 workers concurrents)\n",
            "      >> 12 t√¢ches de traduction soumises...\n",
            "      >> Progression traduction (t√¢ches termin√©es): 12/12 \n",
            "   >> Traduction termin√©e pour Audio Chunk 2 en 12.13s (12 appels API).\n",
            "      >> ‚¨ÜÔ∏è Tentative d'upload pour Chunk Audio 2 (344 segments traduits)...\n",
            "      >> ‚úÖ Upload Chunk Audio 2 r√©ussi (Status: 200). Serveur: Segments appended successfully to video_id EDf6I36S-aE...\n",
            "   ‚è±Ô∏è Temps total traitement Chunk Audio 2: 113.40s\n",
            "\n",
            "--- Traitement Chunk Audio 3/6: chunk_0002_start1200_000s_dur600_000s.mp3 ---\n",
            "\n",
            "üéôÔ∏è Transcription: chunk_0002_start1200_000s_dur600_000s.mp3 (Offset Global: 1200.000s)\n",
            "   Infos chunk d√©tect√©es: Lang='zh' (Conf: 0.97), Dur√©e: 600.00s\n",
            "   Traitement des segments...\n",
            "   Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà-] 99% \n",
            "   üïí Transcription du chunk termin√©e en 60.03s. 242 segments trouv√©s.\n",
            "   -> Transcrit 242 segments.\n",
            "   >> Pr√©paration traduction pour 242 segments en 9 sous-chunk(s).\n",
            "      (Utilisation de max 14 workers concurrents)\n",
            "      >> 9 t√¢ches de traduction soumises...\n",
            "      >> Progression traduction (t√¢ches termin√©es): 9/9 \n",
            "   >> Traduction termin√©e pour Audio Chunk 3 en 11.63s (9 appels API).\n",
            "      >> ‚¨ÜÔ∏è Tentative d'upload pour Chunk Audio 3 (242 segments traduits)...\n",
            "      >> ‚úÖ Upload Chunk Audio 3 r√©ussi (Status: 200). Serveur: Segments appended successfully to video_id EDf6I36S-aE...\n",
            "   ‚è±Ô∏è Temps total traitement Chunk Audio 3: 72.05s\n",
            "\n",
            "--- Traitement Chunk Audio 4/6: chunk_0003_start1800_000s_dur600_000s.mp3 ---\n",
            "\n",
            "üéôÔ∏è Transcription: chunk_0003_start1800_000s_dur600_000s.mp3 (Offset Global: 1800.000s)\n",
            "   Infos chunk d√©tect√©es: Lang='zh' (Conf: 0.99), Dur√©e: 600.00s\n",
            "   Traitement des segments...\n",
            "   Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% \n",
            "   üïí Transcription du chunk termin√©e en 71.78s. 247 segments trouv√©s.\n",
            "   -> Transcrit 247 segments.\n",
            "   >> Pr√©paration traduction pour 247 segments en 9 sous-chunk(s).\n",
            "      (Utilisation de max 14 workers concurrents)\n",
            "      >> 9 t√¢ches de traduction soumises...\n",
            "      >> Progression traduction (t√¢ches termin√©es): 9/9 \n",
            "   >> Traduction termin√©e pour Audio Chunk 4 en 11.51s (9 appels API).\n",
            "      >> ‚¨ÜÔ∏è Tentative d'upload pour Chunk Audio 4 (247 segments traduits)...\n",
            "      >> ‚úÖ Upload Chunk Audio 4 r√©ussi (Status: 200). Serveur: Segments appended successfully to video_id EDf6I36S-aE...\n",
            "   ‚è±Ô∏è Temps total traitement Chunk Audio 4: 83.68s\n",
            "\n",
            "--- Traitement Chunk Audio 5/6: chunk_0004_start2400_000s_dur600_000s.mp3 ---\n",
            "\n",
            "üéôÔ∏è Transcription: chunk_0004_start2400_000s_dur600_000s.mp3 (Offset Global: 2400.000s)\n",
            "   Infos chunk d√©tect√©es: Lang='zh' (Conf: 0.97), Dur√©e: 600.00s\n",
            "   Traitement des segments...\n",
            "   Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà-] 99% \n",
            "   üïí Transcription du chunk termin√©e en 86.23s. 307 segments trouv√©s.\n",
            "   -> Transcrit 307 segments.\n",
            "   >> Pr√©paration traduction pour 307 segments en 11 sous-chunk(s).\n",
            "      (Utilisation de max 14 workers concurrents)\n",
            "      >> 11 t√¢ches de traduction soumises...\n",
            "      >> Progression traduction (t√¢ches termin√©es): 11/11 \n",
            "   >> Traduction termin√©e pour Audio Chunk 5 en 11.94s (11 appels API).\n",
            "      >> ‚¨ÜÔ∏è Tentative d'upload pour Chunk Audio 5 (307 segments traduits)...\n",
            "      >> ‚úÖ Upload Chunk Audio 5 r√©ussi (Status: 200). Serveur: Segments appended successfully to video_id EDf6I36S-aE...\n",
            "   ‚è±Ô∏è Temps total traitement Chunk Audio 5: 98.55s\n",
            "\n",
            "--- Traitement Chunk Audio 6/6: chunk_0005_start3000_000s_dur82_809s.mp3 ---\n",
            "\n",
            "üéôÔ∏è Transcription: chunk_0005_start3000_000s_dur82_809s.mp3 (Offset Global: 3000.000s)\n",
            "   Infos chunk d√©tect√©es: Lang='zh' (Conf: 0.97), Dur√©e: 82.81s\n",
            "   Traitement des segments...\n",
            "   Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà----------] 50% \n",
            "   üïí Transcription du chunk termin√©e en 9.46s. 41 segments trouv√©s.\n",
            "   -> Transcrit 41 segments.\n",
            "   >> Pr√©paration traduction pour 41 segments en 2 sous-chunk(s).\n",
            "      (Utilisation de max 14 workers concurrents)\n",
            "      >> 2 t√¢ches de traduction soumises...\n",
            "      >> Progression traduction (t√¢ches termin√©es): 2/2 \n",
            "   >> Traduction termin√©e pour Audio Chunk 6 en 11.83s (2 appels API).\n",
            "      >> ‚¨ÜÔ∏è Tentative d'upload pour Chunk Audio 6 (41 segments traduits)...\n",
            "      >> ‚úÖ Upload Chunk Audio 6 r√©ussi (Status: 200). Serveur: Segments appended successfully to video_id EDf6I36S-aE...\n",
            "   ‚è±Ô∏è Temps total traitement Chunk Audio 6: 21.60s\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 2: R√âSUM√â DU TRAITEMENT\n",
            "========================================\n",
            "‚è±Ô∏è Temps total Traitement & Upload (√âtape 2): 500.65s.\n",
            "üìä Total Chunks Audio Trait√©s: 6\n",
            "üìä Total Segments Transcrits: 1482\n",
            "üìä Total Segments agr√©g√©s apr√®s traduction r√©ussie: 1482\n",
            "‚úÖ Traduction r√©ussie pour tous les chunks audio avec segments.\n",
            "üíæ Total Segments agr√©g√©s pour sauvegarde locale: 1482\n",
            "‚òÅÔ∏è Uploads Incr√©mentaux R√©ussis (par chunk audio): 6\n",
            "‚òÅÔ∏è Uploads Incr√©mentaux √âchou√©s (par chunk audio): 0\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 3: SAUVEGARDE LOCALE FINALE\n",
            "========================================\n",
            "Tentative de sauvegarde du r√©sultat local agr√©g√© vers:\n",
            "   /content/audio_output_optimized_v2/EDf6I36S_aE__large-v3_split10min_noVAD_segChunk30_conc14_fully_translated.json\n",
            "‚úÖ R√©sultat local complet sauvegard√© avec succ√®s.\n",
            "‚è±Ô∏è Temps √âtape 3 (Sauvegarde): 0.02s\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 4: NETTOYAGE (OPTIONNEL)\n",
            "========================================\n",
            "--- Nettoyage d√©sactiv√© ---\n",
            "‚ÑπÔ∏è Fichier audio original conserv√©: /content/audio_output_optimized_v2/youtube_audio.mp3\n",
            "‚ÑπÔ∏è Chunks audio conserv√©s dans: /content/audio_chunks\n",
            "‚ÑπÔ∏è Fichier JSON final conserv√©: /content/audio_output_optimized_v2/EDf6I36S_aE__large-v3_split10min_noVAD_segChunk30_conc14_fully_translated.json\n",
            "\n",
            "üèÅ Script complet termin√©.\n",
            "‚è±Ô∏è Dur√©e totale d'ex√©cution du script (toutes √©tapes): 701.20s\n",
            "\n",
            "üîó Lien potentiel pour consulter le r√©sultat sur le serveur:\n",
            "   https://qingplay.pythonanywhere.com/vid/EDf6I36S-aE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-----------------------------------------------------------------------------**\n",
        "# **Developper testing cells**"
      ],
      "metadata": {
        "id": "EB7bfxLpBhj_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dTbIWD890ZwZ",
        "outputId": "c17ed8f2-94f7-4287-db0d-d9dddda4b5df",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/audio_output/*': No such file or directory\n",
            "rm: cannot remove '/content/audio_output_optimized/*': No such file or directory\n",
            "‚úÖ GPU d√©tect√©. PyTorch utilisera CUDA.\n",
            "‚ÑπÔ∏è  R√©cup√©ration des m√©tadonn√©es de la vid√©o...\n",
            "‚úÖ M√©tadonn√©es r√©cup√©r√©es.\n",
            "üîÑ V√©rification/T√©l√©chargement de l'audio vers /content/audio_output_optimized_v2/youtube_audio_opt.mp3...\n",
            "‚ÑπÔ∏è Fichier audio non trouv√©. Tentative de t√©l√©chargement...\n",
            "‚úÖ Fichier audio t√©l√©charg√© avec succ√®s : /content/audio_output_optimized_v2/youtube_audio_opt.mp3\n",
            "\n",
            "--- Informations Vid√©o R√©cup√©r√©es ---\n",
            "{\n",
            "  \"video_id\": \"GRLdsdBDjE4\",\n",
            "  \"channel_name\": \"May Ho\",\n",
            "  \"channel_url\": \"https://www.youtube.com/@MayHo\",\n",
            "  \"title\": \"„Äê Ê∑±Âú≥ VLOG „Äë ‰∏≠ÂúãÂÖçÁ∞ΩÁ´ãÈ¶¨È£õÂéªÊ∑±Âú≥ÂêÉÂñùÁé©Ê®Ç üòù Â∏∂Ëëó‰Ω†ÂÄëÁöÑÁñëÂïèÊâæ Ulike ÂïèÊ∏ÖÊ•ö üò§ÔΩúMAYHO\",\n",
            "  \"description\": \"ÊØè‰∏ÄÊ®£È£üÁâ©ÈÉΩÂ•ΩÂ•ΩÂêÉÂïäÂïäÂïä ü•∫\\n‚ñ∏  Ë®ÇÈñ±ÊàëÂêß üòâ  http://bit.ly/37IiWLu\\n‚ñ∏ ‰∏äÈõÜÂΩ±ÁâáÔºöhttps://youtu.be/XQbXKy2jViQ?si=hUFWW0tuBf87PdCw\\n\\n- - -\\n\\nÂâõÂÆ£‰Ωà‰∏≠ÂúãÂÖçÁ∞ΩÔºåÁ´ãÈ¶¨Ë≤∑Ê©üÁ•®È£õÂéªÊ∑±Âú≥Áé©ÂòçÔºÅü§£\\nÈùûÂ∏∏Ë¨ùË¨ù Ulike ÁöÑÈÇÄË´ãÔºåÂèÉËßÄ‰ªñÂÄëÁöÑÂ∫óÈù¢ÂèäÂÖ¨Âè∏\\nÊàëÈÇÑÂπ´‰Ω†ÂÄëË´á‰∫ÜË∂ÖÊ£íÁöÑÂÑ™ÊÉ†ÔºåÂ§ßÂÆ∂ÂçÉËê¨Âà•ÈåØÈÅéÂñîÔºÅüòé\\n\\n‰πüË¨ùË¨ù Kris Âíå Â§ßË°õÂ∏∂ÊàëÂÄëÂêÉ‰∫ÜÂæàÂ§öÈÅìÂú∞ÁæéÈ£ü\\nÈÄôÂÄãÊóÖÁ®ãÂæàÁÖßÈ°ßÊàëÂÄëÔºåÊÑõ‰Ω†ÂÄëÊ∑±Ê∑± üíï\\nÊ∑±Âú≥‰πãÊóÖÁµêÊùü‰∫ÜÔºå‰ΩÜÂª£Â∑û‰πãÊóÖÂâõË¶ÅÈñãÂßã\\nÂ§ßÂÆ∂ÊúüÂæÖ‰∏ãÂÄãÂΩ±ÁâáÂêßÔºåÊàëÂÄëÊòéÂπ¥Ë¶ã ü•∞\\n\\n- - -\\n\\n‚ú¶ JmoonÁæéÂÆπ‰ª™\\nShopee Shop Voucher„Äê ULIKEMAY „ÄëÂÖ®Â§ßÂ≠óÊØç\\n\\nÊäòÊâ£ÔºöÈ¶¨Âπ£ 40 / Êñ∞Âπ£20 / Âè∞Âπ£ 600\\nËµ†ÈÄÅ2ÊîØÂáùËÉ∂\\n‰∏ãÂçïÊó∂Â§áÊ≥®„Äê MayHo „Äë\\nÂèØ‰ª•ÂÜçËé∑Âæó‰∏ÄÁõíÈù¢ËÜú*5Áâá ( ‰π∞1Ëµ†3 ! )\\n\\nJmoon MY ShopeeÔºöhttps://bit.ly/3peDOYw\\nJmoon MY LazadaÔºöhttps://bit.ly/3vnkDhR\\nJmoon SG shopeeÔºöhttps://bit.ly/3JIk1bh\\nULIKE TWÂÆòÁΩëÔºöhttps://bit.ly/3XPpum7\\n\\n- - -\\n\\n‚ú¶ UlikeËÑ±ÊØõ‰ª™\\nShopee Shop Voucher„Äê ULIKEMAY „Äë ÂÖ®Â§ßÂ≠óÊØç\\n\\nÊäòÊâ£ÔºöÈ¶¨Âπ£ 40 / Êñ∞Âπ£20 / Âè∞Âπ£ 600\\nËµ†ÈÄÅËòÜËñàËÜ†*1ÔºåÈõªÂãïÁâôÂà∑ÔºåÂàÆÊØõÂàÄÔºåÂ¢®Èè°\\n‰∏ãÂçïÊó∂ËÆ∞ÂæóÂ§áÊ≥® „Äê MayHo „Äë\\nÂèØ‰ª•È°çÂ§ñÂÜçËé∑ÂæóÊåâÊë©Êûï‰∏ÄÂÄã ( ‰π∞1Ëµ†5 !! Ôºâ\\n\\nULIKE MY ShopeeÔºöhttp://bit.ly/3V05tX0\\nULIKE MY LazadaÔºöhttps://bit.ly/3vnkDhR\\nULIKE SG ShopeeÔºöhttp://bit.ly/3ByqUaY\\nULIKE TW ÂÆòÁ∂≤Ôºöhttps://bit.ly/3PVoco0\\n\\nÊúâ‰ªÄÈ∫ºÂïèÈ°åÁöÑË©±ÔºåÂèØ‰ª•DM Shopee / Lazada ÂÆ¢ÊúçÂì¶ ‚ù§Ô∏è\\n\\n- - -\\n\\n‚ú¶ 00:00 Day 1 \\n\\nUlike Ê∑±Âú≥ÁæÖÊπñËê¨Ë±°Âüé‰∏âÊúü\\n\\nÊù±ÈñÄËÄÅË°ó\\n\\nÂ§ßËâØÊµ∑Ë®ò Á≤•Â∫ïÁÅ´Èçã\\n\\nÈÖíÂ∫óÔºöÁÅ£ÁßëÊäÄÂúíÂçÄÈ∫óÁàæÊü•ÈõÖÈ†ìÈ£ØÂ∫ó\\n\\n‚ú¶ 16:50 Day 2 \\n\\nÂçóÂ±±ÂçÄ ÈáëÁôæÂë≥\\n\\n‰∏ÄÊ®πËäôËìâ\\n\\n‚ú¶ 35:54 Day 3 \\n\\nËèØÊΩ§Ëê¨Ë±°Â§©Âú∞\\n\\nÂñúËå∂( Ëê¨Ë±°Â§©Âú∞Â∫ó Ôºâ\\n\\nËî°ÁÄæÊ∏ØÂºèÈªûÂøÉÔºà ËèØÊΩ§Ëê¨Ë±°Â§©Âú∞Â∫ó Ôºâ\\n\\nÊ≠°Ê®ÇÊ∏ØÁÅ£ \\n\\nÈçæÊõ∏Èñ£\\n\\nÈä´Â≠êÁÖ®Ëóï\\n\\n‚ú¶ 48:20 Day 4 \\n\\nÊ∑±Âú≥ÁÅ£Ëê¨Ë±°Âüé\\n\\nÊ∑±Âú≥‰∫∫ÊâçÂÖ¨Âúí\\n\\n- - -\\n\\n‚úß Instagram  http://bit.ly/2OFuK8H\\n‚úß Facebook  http://bit.ly/2rhJjr9\\n‚úß TikTok  https://bit.ly/34FwcQd\\n‚úß Â∞èÁ¥ÖÊõ∏ https://bit.ly/3qJBQvk\\n\\n‚úß @allenkhor  http://bit.ly/377lPF1\\n‚úß ÁëæËê± https://bit.ly/3FOvZMM\\n‚úß ‰ªüÂΩ± https://bit.ly/3lW0CWB\\n\\n‚ñ† ÊîùÂΩ±Âô®Êùê\\n @sonymalaysia  ZV-1 II & ECM-G1\\n @Apple  iPhone 15 Pro Max \\n\\n‚ñ† Â∑•‰ΩúÂêà‰ΩúÈÇÄÁ¥Ñ\\nmayho0110@hotmail.com \\n\\n‚ñ† Music  \\nE's Jammy Jams,TrackTribe,Otis McDonald\\n\\n#MayHo‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã #AllenKhor #Ê∑±Âú≥Vlog #ÁæéÂ•ΩÁöÑ‰∏ÄÂ§© #Ulike #Jmoon\"\n",
            "}\n",
            "-----------------------------------\n",
            "\n",
            "üöÄ Lancement de la transcription optimis√©e pour : /content/audio_output_optimized_v2/youtube_audio_opt.mp3\n",
            "\n",
            "üîÑ Chargement du mod√®le faster-whisper 'large-v3'...\n",
            "‚ÑπÔ∏è GPU d√©tect√©, utilisation de float16.\n",
            "‚úÖ Mod√®le charg√© sur 'cuda' avec compute_type='float16' en 3.09s.\n",
            "\n",
            "üéôÔ∏è D√©but de la transcription (faster-whisper) pour youtube_audio_opt.mp3...\n",
            "   Options: beam_size=5, vad_filter=False\n",
            "‚úÖ Infos d√©tect√©es: Langue='zh' (Prob: 0.95), Dur√©e Totale: 3211.98s\n",
            "--> D√©but du traitement des segments...\n",
            "   Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà-] 99.1% (Segment ~1918, Temps: 3182.24s / 3211.98s)\n",
            "üïí Transcription (faster-whisper) termin√©e en 629.34 secondes.\n",
            "   Nombre total de segments g√©n√©r√©s : 1933\n",
            "\n",
            "--- V√©rification des Timestamps ---\n",
            "‚ÑπÔ∏è VAD √©tait d√©sactiv√©. Les temps proviennent directement du mod√®le. Si incorrects:\n",
            "   - Envisagez un mod√®le plus grand ('large-v3' -> 'large-v3'?) pour une meilleure pr√©cision potentielle.\n",
            "   - V√©rifiez la qualit√© de l'audio source.\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "------------------------------------\n",
            "‚úÖ Transcription sauvegard√©e dans : /content/audio_output_optimized_v2/youtube_audio_opt_transcript_large-v3_noVAD.json\n",
            "\n",
            "üèÅ Script termin√©.\n"
          ]
        }
      ],
      "source": [
        "# @title # **Transcription (Optimized with faster-whisper & GPU support)**\n",
        "\n",
        "!rm -r /content/audio_output/*\n",
        "!rm -r /content/audio_output_optimized/*\n",
        "!rm -r /content/audio_output_optimized_v2/*\n",
        "\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# --- Try importing necessary libraries and provide guidance if missing ---\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    print(\"‚ùå PyTorch n'est pas install√©. Veuillez l'installer.\")\n",
        "    print(\"   - CPU: pip install torch torchvision torchaudio\")\n",
        "    print(\"   - GPU: Voir https://pytorch.org/ pour la commande CUDA appropri√©e.\")\n",
        "    exit()\n",
        "\n",
        "try:\n",
        "    from faster_whisper import WhisperModel\n",
        "except ImportError:\n",
        "    print(\"‚ùå faster-whisper n'est pas install√©. Veuillez ex√©cuter : pip install faster-whisper\")\n",
        "    exit()\n",
        "\n",
        "# --- Check for GPU availability ---\n",
        "IS_GPU_AVAILABLE = torch.cuda.is_available()\n",
        "if IS_GPU_AVAILABLE:\n",
        "    print(\"‚úÖ GPU d√©tect√©. PyTorch utilisera CUDA.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Aucun GPU compatible CUDA d√©tect√©. PyTorch utilisera le CPU.\")\n",
        "    print(\"   La transcription sera plus lente. faster-whisper avec 'int8' peut aider.\")\n",
        "\n",
        "\n",
        "# --- Fonction download_youtube_audio_improved (PAS DE CHANGEMENT ICI) ---\n",
        "# ... (votre fonction download_youtube_audio_improved reste identique) ...\n",
        "def download_youtube_audio_improved(youtube_url, output_path):\n",
        "    \"\"\"\n",
        "    Downloads YouTube audio and reliably extracts metadata using separate yt-dlp calls.\n",
        "    Checks if the audio file already exists before attempting download.\n",
        "\n",
        "    Args:\n",
        "        youtube_url (str): The URL of the YouTube video.\n",
        "        output_path (str): The desired path to save the MP3 audio file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (str, dict) containing the audio file path and video info dictionary,\n",
        "               or (None, None) if a critical error occurs (metadata failure).\n",
        "               If audio download fails but metadata is retrieved, returns (None, video_info).\n",
        "    \"\"\"\n",
        "    video_info = None\n",
        "    audio_file_path = None\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    if not os.path.exists(output_dir):\n",
        "        try:\n",
        "            os.makedirs(output_dir)\n",
        "            print(f\"üìÅ Cr√©ation du r√©pertoire de sortie : {output_dir}\")\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Erreur lors de la cr√©ation du r√©pertoire {output_dir}: {e}\")\n",
        "            return None, None # Cannot proceed without output directory\n",
        "\n",
        "    # --- √âtape 1: R√©cup√©rer les m√©tadonn√©es avec --dump-json ---\n",
        "    print(\"‚ÑπÔ∏è  R√©cup√©ration des m√©tadonn√©es de la vid√©o...\")\n",
        "    metadata_result = None # Initialize\n",
        "    try:\n",
        "        metadata_command = [\n",
        "            \"yt-dlp\",\n",
        "            \"--dump-json\", # Sortir les m√©tadonn√©es en JSON sur stdout\n",
        "            \"--encoding\", \"utf-8\", # Assurer l'encodage correct\n",
        "            youtube_url,\n",
        "        ]\n",
        "        metadata_result = subprocess.run(metadata_command, check=True, capture_output=True, text=True, encoding='utf-8')\n",
        "        metadata = json.loads(metadata_result.stdout)\n",
        "\n",
        "        # Extraire les informations n√©cessaires\n",
        "        channel_name = metadata.get(\"uploader\", \"N/A\")\n",
        "        channel_url = metadata.get(\"uploader_url\", \"N/A\")\n",
        "        title = metadata.get(\"title\", \"N/A\")\n",
        "        description = metadata.get(\"description\", \"N/A\")\n",
        "        video_id = metadata.get(\"id\", None) # ID YouTube r√©el\n",
        "\n",
        "        if not video_id:\n",
        "             print(\"‚ö†Ô∏è Impossible d'extraire l'ID de la vid√©o via yt-dlp.\")\n",
        "             # Essayer avec regex en secours\n",
        "             video_id_match = re.search(r\"v=([a-zA-Z0-9_-]+)\", youtube_url)\n",
        "             video_id = video_id_match.group(1) if video_id_match else \"UNKNOWN\"\n",
        "\n",
        "        video_info = {\n",
        "            \"video_id\": video_id, # ID YouTube r√©el\n",
        "            \"channel_name\": channel_name,\n",
        "            \"channel_url\": channel_url,\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "        }\n",
        "        print(\"‚úÖ M√©tadonn√©es r√©cup√©r√©es.\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Erreur critique lors de la r√©cup√©ration des m√©tadonn√©es avec yt-dlp (Code: {e.returncode}).\")\n",
        "        print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "        stderr_output = e.stderr.strip() if e.stderr else \"N/A\"\n",
        "        print(f\"   Erreur: {stderr_output}\")\n",
        "        return None, None # Erreur critique, impossible de continuer sans m√©tadonn√©es\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Erreur critique lors de l'analyse des m√©tadonn√©es JSON : {e}\")\n",
        "        if metadata_result and metadata_result.stdout:\n",
        "             print(f\"--- Sortie brute de yt-dlp --- \\n{metadata_result.stdout[:500]}...\\n--------------------------\")\n",
        "        return None, None # Erreur critique\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå yt-dlp n'est pas install√© ou introuvable dans le PATH.\")\n",
        "        print(\"   Veuillez l'installer (par exemple avec : pip install yt-dlp)\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue critique lors de la r√©cup√©ration des m√©tadonn√©es : {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None, None # Erreur critique\n",
        "\n",
        "    # --- √âtape 2: T√©l√©charger (ou v√©rifier) l'audio ---\n",
        "    print(f\"üîÑ V√©rification/T√©l√©chargement de l'audio vers {output_path}...\")\n",
        "\n",
        "    # V√©rification pr√©alable de l'existence du fichier\n",
        "    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "        print(f\"‚úÖ Fichier audio '{output_path}' existe d√©j√† et n'est pas vide. Utilisation du fichier existant.\")\n",
        "        audio_file_path = output_path\n",
        "    else:\n",
        "        if os.path.exists(output_path):\n",
        "             print(f\"‚ÑπÔ∏è Fichier audio '{output_path}' existe mais est vide. Tentative de re-t√©l√©chargement...\")\n",
        "        else:\n",
        "             print(f\"‚ÑπÔ∏è Fichier audio non trouv√©. Tentative de t√©l√©chargement...\")\n",
        "\n",
        "        download_result = None # Initialize\n",
        "        try:\n",
        "            download_command = [\n",
        "                \"yt-dlp\",\n",
        "                \"-x\", # Extraire l'audio\n",
        "                \"--audio-format\", \"mp3\",\n",
        "                # '--audio-quality', '0', # Optionnel: Meilleure qualit√© audio (peut √™tre plus lent)\n",
        "                # '--no-overwrites', # On g√®re la v√©rification avant, mais laisser en s√©curit√©\n",
        "                \"--force-overwrites\", # Forcer l'√©crasement si le fichier existant √©tait vide/corrompu\n",
        "                \"-o\", output_path, # Sp√©cifier le chemin de sortie complet\n",
        "                \"--encoding\", \"utf-8\",\n",
        "                # \"-v\", # D√©commenter pour une sortie tr√®s d√©taill√©e (debug)\n",
        "                youtube_url,\n",
        "            ]\n",
        "            download_result = subprocess.run(download_command, check=True, capture_output=True, text=True, encoding='utf-8')\n",
        "\n",
        "            # V√©rifier si le fichier existe et n'est pas vide APRES l'ex√©cution\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f\"‚úÖ Fichier audio t√©l√©charg√© avec succ√®s : {output_path}\")\n",
        "                audio_file_path = output_path\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è yt-dlp a termin√© sans erreur mais le fichier audio '{output_path}' est introuvable ou vide apr√®s la tentative.\")\n",
        "                if download_result:\n",
        "                    print(f\"--- Sortie yt-dlp (stdout) ---\\n{download_result.stdout.strip()}\")\n",
        "                    print(f\"--- Sortie yt-dlp (stderr) ---\\n{download_result.stderr.strip()}\")\n",
        "                # Retourner les m√©tadonn√©es m√™me si le t√©l√©chargement √©choue ici\n",
        "                return None, video_info\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Erreur lors du t√©l√©chargement de l'audio avec yt-dlp (Code: {e.returncode}).\")\n",
        "            print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "            stderr_output = e.stderr.strip() if e.stderr else \"N/A\"\n",
        "            print(f\"   Erreur: {stderr_output}\")\n",
        "            # V√©rifier si le fichier existe quand m√™me (parfois yt-dlp √©choue mais laisse un fichier partiel)\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                 print(f\"‚ÑπÔ∏è Un fichier audio existe √† '{output_path}' malgr√© l'erreur. Il est peut-√™tre incomplet.\")\n",
        "                 audio_file_path = output_path # On le retourne quand m√™me, l'utilisateur verra l'erreur\n",
        "            else:\n",
        "                 # Retourner les m√©tadonn√©es, mais pas de chemin audio\n",
        "                 return None, video_info\n",
        "        except FileNotFoundError:\n",
        "            # Devrait avoir √©t√© captur√© √† l'√©tape 1, mais redondance\n",
        "            print(\"‚ùå yt-dlp n'est pas install√© ou introuvable dans le PATH.\")\n",
        "            return None, video_info # On a peut-√™tre les m√©tadonn√©es\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur inattendue lors du t√©l√©chargement de l'audio : {e}\")\n",
        "            traceback.print_exc()\n",
        "            # V√©rifier si le fichier existe malgr√© l'erreur\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f\"‚ÑπÔ∏è Utilisation du fichier audio existant '{output_path}' malgr√© l'erreur inattendue.\")\n",
        "                audio_file_path = output_path\n",
        "            else:\n",
        "                return None, video_info # Retourner m√©tadonn√©es, mais pas de chemin audio\n",
        "\n",
        "    # Derni√®re v√©rification de l'existence et de la taille du fichier avant de retourner\n",
        "    if audio_file_path and (not os.path.exists(audio_file_path) or os.path.getsize(audio_file_path) == 0):\n",
        "        print(f\"‚ùå ERREUR FINALE: Le chemin audio '{audio_file_path}' indiqu√© n'existe pas ou est vide.\")\n",
        "        return None, video_info\n",
        "\n",
        "    return audio_file_path, video_info\n",
        "\n",
        "\n",
        "# --- Fonction transcribe_audio (MISE √Ä JOUR avec progression et conseils VAD) ---\n",
        "def transcribe_audio_faster(file_path, model_size=\"base\", video_info=None, beam_size=5, vad_filter=True, vad_min_silence_ms=700):\n",
        "    \"\"\"\n",
        "    Transcrit un fichier audio en utilisant faster-whisper optimis√©, affiche la progression\n",
        "    et retourne une liste de segments. Utilise le GPU si disponible.\n",
        "\n",
        "    Args:\n",
        "      file_path (str): Chemin vers le fichier audio (ex : \"audio.mp3\").\n",
        "      model_size (str): Taille du mod√®le (\"tiny\", \"base\", \"small\", \"medium\", \"large-v3\").\n",
        "      video_info (dict, optional): Informations sur la vid√©o. Defaults to None.\n",
        "      beam_size (int): Taille du faisceau pour le d√©codage.\n",
        "      vad_filter (bool): Activer le filtre VAD (Voice Activity Detection).\n",
        "                         Si True, utilise vad_min_silence_ms.\n",
        "                         Si False, d√©sactive VAD (peut √™tre plus lent mais utile pour d√©boguer les temps).\n",
        "      vad_min_silence_ms (int): Dur√©e minimale de silence en ms pour couper un segment avec VAD.\n",
        "                                Uniquement utilis√© si vad_filter=True.\n",
        "                                Augmenter (ex: 1000) peut cr√©er des segments plus longs.\n",
        "                                Diminuer (ex: 500) peut cr√©er des segments plus courts.\n",
        "\n",
        "    Returns:\n",
        "      dict: Un dictionnaire contenant 'segments' (liste de segments)\n",
        "            et potentiellement des informations sur la vid√©o si fournies.\n",
        "            Retourne {'segments': []} en cas d'erreur majeure.\n",
        "    \"\"\"\n",
        "    start_time_load = time.time()\n",
        "    print(f\"\\nüîÑ Chargement du mod√®le faster-whisper '{model_size}'...\")\n",
        "\n",
        "    device = \"cuda\" if IS_GPU_AVAILABLE else \"cpu\"\n",
        "    # Utiliser float16 pour GPU r√©cent, bfloat16 pour Ampere+, float32/int8 pour CPU\n",
        "    if IS_GPU_AVAILABLE:\n",
        "        # V√©rifier la capacit√© de calcul pour bfloat16 (Ampere et plus r√©cent)\n",
        "        if torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "            compute_type = \"bfloat16\"\n",
        "            print(\"‚ÑπÔ∏è GPU compatible Ampere+ d√©tect√©, utilisation de bfloat16.\")\n",
        "        else:\n",
        "            compute_type = \"float16\"\n",
        "            print(\"‚ÑπÔ∏è GPU d√©tect√©, utilisation de float16.\")\n",
        "    else:\n",
        "        compute_type = \"int8\" # int8 est g√©n√©ralement un bon compromis pour CPU\n",
        "\n",
        "    model = None\n",
        "    try:\n",
        "        model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "        load_time = time.time() - start_time_load\n",
        "        print(f\"‚úÖ Mod√®le charg√© sur '{device}' avec compute_type='{compute_type}' en {load_time:.2f}s.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors du chargement du mod√®le faster-whisper ({model_size}, {device}, {compute_type}): {e}\")\n",
        "        # Essayer un fallback plus s√ªr si le premier choix a √©chou√©\n",
        "        fallback_compute_type = None\n",
        "        if IS_GPU_AVAILABLE and compute_type != \"float32\":\n",
        "             fallback_compute_type = \"float32\" # Moins performant mais plus compatible sur GPU\n",
        "        elif not IS_GPU_AVAILABLE and compute_type != \"int8\": # S'il y avait une autre option CPU test√©e\n",
        "             fallback_compute_type = \"int8\"\n",
        "\n",
        "        if fallback_compute_type:\n",
        "            print(f\"‚ÑπÔ∏è Tentative de fallback avec compute_type='{fallback_compute_type}'...\")\n",
        "            try:\n",
        "                model = WhisperModel(model_size, device=device, compute_type=fallback_compute_type)\n",
        "                load_time = time.time() - start_time_load\n",
        "                print(f\"‚úÖ Mod√®le charg√© sur '{device}' avec compute_type='{fallback_compute_type}' en {load_time:.2f}s.\")\n",
        "            except Exception as e2:\n",
        "                print(f\"‚ùå √âchec du chargement m√™me avec fallback '{fallback_compute_type}': {e2}\")\n",
        "                traceback.print_exc()\n",
        "                return {\"segments\": []}\n",
        "        else:\n",
        "            traceback.print_exc()\n",
        "            return {\"segments\": []} # √âchec initial sans option de fallback √©vidente\n",
        "\n",
        "    print(f\"\\nüéôÔ∏è D√©but de la transcription (faster-whisper) pour {os.path.basename(file_path)}...\")\n",
        "    print(f\"   Options: beam_size={beam_size}, vad_filter={vad_filter}\" + (f\", vad_min_silence_ms={vad_min_silence_ms}\" if vad_filter else \"\"))\n",
        "    start_time_transcribe = time.time()\n",
        "    transcript_segments_data = []\n",
        "    total_audio_duration = 0\n",
        "    last_printed_progress = -1 # Pour √©viter d'imprimer 0% plusieurs fois\n",
        "\n",
        "    try:\n",
        "        # Configurer les param√®tres VAD si activ√©\n",
        "        vad_parameters = None\n",
        "        if vad_filter:\n",
        "            vad_parameters = dict(min_silence_duration_ms=vad_min_silence_ms)\n",
        "            # Vous pouvez ajouter d'autres param√®tres VAD ici si n√©cessaire, ex:\n",
        "            # vad_parameters[\"threshold\"] = 0.5 # Seuil de d√©tection vocale (0 √† 1)\n",
        "\n",
        "        # Lancer la transcription - segments_generator est un it√©rateur !\n",
        "        segments_generator, info = model.transcribe(\n",
        "            file_path,\n",
        "            beam_size=beam_size,\n",
        "            vad_filter=vad_filter,\n",
        "            vad_parameters=vad_parameters\n",
        "            # word_timestamps=False # Mettre √† True pour des temps au niveau du mot (change la structure)\n",
        "        )\n",
        "\n",
        "        detected_lang = info.language\n",
        "        lang_prob = info.language_probability\n",
        "        total_audio_duration = info.duration # Dur√©e totale d√©tect√©e par faster-whisper\n",
        "\n",
        "        print(f\"‚úÖ Infos d√©tect√©es: Langue='{detected_lang}' (Prob: {lang_prob:.2f}), Dur√©e Totale: {total_audio_duration:.2f}s\")\n",
        "        if total_audio_duration <= 0:\n",
        "             print(\"‚ö†Ô∏è Dur√©e audio d√©tect√©e nulle ou n√©gative. Le calcul de progression sera impr√©cis.\")\n",
        "\n",
        "        print(\"--> D√©but du traitement des segments...\")\n",
        "\n",
        "        # --- Boucle principale avec progression ---\n",
        "        segment_count = 0\n",
        "        for segment in segments_generator:\n",
        "            segment_count += 1\n",
        "            seg_start = segment.start\n",
        "            seg_end = segment.end\n",
        "            duration = max(0, seg_end - seg_start)\n",
        "            text = segment.text.strip() if segment.text else \"\"\n",
        "\n",
        "            # Calculer le pourcentage de progression bas√© sur la FIN du segment actuel\n",
        "            current_progress = 0\n",
        "            if total_audio_duration > 0:\n",
        "                current_progress = min(100.0, (seg_end / total_audio_duration) * 100)\n",
        "\n",
        "            # Afficher la progression (avec une barre simple et moins fr√©quente)\n",
        "            # Imprime tous les 5% ou pour le dernier segment\n",
        "            rounded_progress = int(current_progress)\n",
        "            if rounded_progress > last_printed_progress and (rounded_progress % 5 == 0 or rounded_progress >= 99):\n",
        "                progress_bar_length = 20\n",
        "                filled_length = int(progress_bar_length * current_progress / 100)\n",
        "                bar = '‚ñà' * filled_length + '-' * (progress_bar_length - filled_length)\n",
        "                # Utiliser \\r pour revenir au d√©but de la ligne et √©craser le pr√©c√©dent message\n",
        "                print(f\"\\r   Progression: [{bar}] {current_progress:.1f}% (Segment ~{segment_count}, Temps: {seg_end:.2f}s / {total_audio_duration:.2f}s)\", end=\"\")\n",
        "                last_printed_progress = rounded_progress\n",
        "\n",
        "            transcript_segments_data.append({\n",
        "                \"text\": text,\n",
        "                \"start\": round(seg_start, 3),\n",
        "                \"duration\": round(duration, 3),\n",
        "                # On peut garder le % par segment si utile, sinon on l'a d√©j√† affich√©\n",
        "                \"progress_percentage_at_segment_end\": round(current_progress, 2)\n",
        "            })\n",
        "\n",
        "        # Assurer un saut de ligne apr√®s la barre de progression finale\n",
        "        print() # New line after the loop finishes\n",
        "\n",
        "        transcription_time = time.time() - start_time_transcribe\n",
        "        print(f\"üïí Transcription (faster-whisper) termin√©e en {transcription_time:.2f} secondes.\")\n",
        "        print(f\"   Nombre total de segments g√©n√©r√©s : {segment_count}\")\n",
        "\n",
        "        if segment_count == 0:\n",
        "            print(\"‚ö†Ô∏è Aucun segment transcrit trouv√©.\")\n",
        "            # Si aucun segment, v√©rifier si VAD √©tait actif.\n",
        "            if vad_filter:\n",
        "                print(\"   -> Conseil: Essayez avec 'vad_filter=False' ou ajustez 'vad_min_silence_ms'.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Erreur: Fichier audio introuvable √† '{file_path}'\")\n",
        "        return {\"segments\": []}\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erreur majeure lors de la transcription avec faster-whisper : {e}\")\n",
        "        print(\"--- Traceback de l'erreur ---\")\n",
        "        traceback.print_exc()\n",
        "        print(\"---------------------------\")\n",
        "        return {\"segments\": []}\n",
        "\n",
        "    # --- Conseils si les temps semblent incorrects ---\n",
        "    print(\"\\n--- V√©rification des Timestamps ---\")\n",
        "    if vad_filter:\n",
        "        print(\"‚ÑπÔ∏è VAD √©tait activ√©. Si les temps des segments semblent incorrects (trop longs, d√©cal√©s) :\")\n",
        "        print(f\"   - Essayez d'ajuster 'vad_min_silence_ms' (actuellement: {vad_min_silence_ms}). Augmenter (ex: 1000, 1500) regroupe plus, diminuer (ex: 500, 300) segmente plus.\")\n",
        "        print(f\"   - Essayez de relancer avec 'vad_filter=False' pour voir les segments bruts du mod√®le (peut √™tre plus lent).\")\n",
        "    else:\n",
        "         print(\"‚ÑπÔ∏è VAD √©tait d√©sactiv√©. Les temps proviennent directement du mod√®le. Si incorrects:\")\n",
        "         print(f\"   - Envisagez un mod√®le plus grand ('{model_size}' -> 'large-v3'?) pour une meilleure pr√©cision potentielle.\")\n",
        "         print(f\"   - V√©rifiez la qualit√© de l'audio source.\")\n",
        "    print(\"------------------------------------\\n\")\n",
        "\n",
        "\n",
        "    # Construire la sortie finale\n",
        "    output = {\"segments\": transcript_segments_data}\n",
        "    if video_info:\n",
        "        output_video_info = {k: v for k, v in video_info.items() if v is not None}\n",
        "        output.update(output_video_info)\n",
        "\n",
        "    return output\n",
        "\n",
        "# --- Exemple d'utilisation (Adapt√©) ---\n",
        "youtube_url = \"https://www.youtube.com/watch?v=GRLdsdBDjE4&pp=ygUObWF5aG8gc2hlbnpoZW4%3D\" #@param {\"type\":\"string\"}\n",
        "output_dir = \"/content/audio_output_optimized_v2\" # Nouveau dossier pour √©viter conflits\n",
        "output_filename = \"youtube_audio_opt.mp3\"\n",
        "output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "# --- Param√®tres de Transcription ---\n",
        "model_size = \"large-v3\" #@param [\"tiny\",\"base\",\"small\",\"medium\",\"large-v3\"]\n",
        "\n",
        "# --- NOUVEAUX param√®tres pour le contr√¥le VAD et timing ---\n",
        "use_vad = False # @param {type:\"boolean\"}\n",
        "# Cette valeur est IGNOR√âE si use_vad est False\n",
        "vad_silence_duration_ms = 200 # @param {type:\"slider\", min:100, max:2000, step:50}\n",
        "\n",
        "beam_search_size = 5 # Taille standard pour un bon √©quilibre vitesse/qualit√©\n",
        "\n",
        "# --- Ex√©cution Principale ---\n",
        "file_path = None\n",
        "video_info = None\n",
        "json_output_filename = None # Initialiser pour le bloc finally\n",
        "\n",
        "try:\n",
        "    # 1. T√©l√©charger l'audio et r√©cup√©rer les infos vid√©o\n",
        "    file_path, video_info = download_youtube_audio_improved(youtube_url, output_path)\n",
        "\n",
        "    # 2. V√©rifier si le t√©l√©chargement et les m√©tadonn√©es ont r√©ussi\n",
        "    if file_path and video_info:\n",
        "        print(\"\\n--- Informations Vid√©o R√©cup√©r√©es ---\")\n",
        "        print(json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "        print(\"-----------------------------------\\n\")\n",
        "\n",
        "        print(f\"üöÄ Lancement de la transcription optimis√©e pour : {file_path}\")\n",
        "        # 3. Lancer la transcription avec faster-whisper ET les nouveaux param√®tres\n",
        "        transcript = transcribe_audio_faster(\n",
        "            file_path,\n",
        "            model_size=model_size,\n",
        "            video_info=video_info,\n",
        "            beam_size=beam_search_size,\n",
        "            vad_filter=use_vad, # Utiliser le param√®tre d√©fini ci-dessus\n",
        "            vad_min_silence_ms=vad_silence_duration_ms # Utiliser le param√®tre d√©fini ci-dessus\n",
        "        )\n",
        "\n",
        "        # (Le reste du code pour sauvegarder le JSON est inchang√©)\n",
        "        print(\"\\n------------------------------------\")\n",
        "\n",
        "        # 4. Sauvegarder la transcription si elle n'est pas vide\n",
        "        if transcript and transcript.get(\"segments\"): # V√©rifier que des segments existent\n",
        "            # Ajouter les param√®tres utilis√©s au nom de fichier pour r√©f√©rence\n",
        "            vad_suffix = f\"vad{vad_silence_duration_ms}\" if use_vad else \"noVAD\"\n",
        "            json_output_filename = os.path.splitext(output_path)[0] + f\"_transcript_{model_size}_{vad_suffix}.json\"\n",
        "            try:\n",
        "                with open(json_output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(transcript, f, indent=2, ensure_ascii=False)\n",
        "                print(f\"‚úÖ Transcription sauvegard√©e dans : {json_output_filename}\")\n",
        "            except IOError as e:\n",
        "                print(f\"‚ùå Erreur lors de la sauvegarde du fichier JSON : {e}\")\n",
        "            except Exception as e:\n",
        "                 print(f\"‚ùå Erreur inattendue lors de la sauvegarde JSON : {e}\")\n",
        "                 traceback.print_exc() # Voir l'erreur\n",
        "        elif transcript: # Si transcript existe mais pas de segments\n",
        "             print(\"‚ÑπÔ∏è La transcription a √©t√© ex√©cut√©e mais n'a produit aucun segment. Aucun fichier JSON sauvegard√©.\")\n",
        "        else: # Si transcript est None ou vide\n",
        "             print(\"‚ùå La transcription a √©chou√© ou a retourn√© un r√©sultat vide. Aucun fichier JSON sauvegard√©.\")\n",
        "\n",
        "\n",
        "    elif video_info:\n",
        "        print(\"\\n‚ö†Ô∏è Le t√©l√©chargement/acc√®s au fichier audio a √©chou√©, mais les m√©tadonn√©es ont √©t√© r√©cup√©r√©es.\")\n",
        "        print(\"--- Informations Vid√©o ---\")\n",
        "        print(json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "        print(\"------------------------\")\n",
        "        print(\"üö´ Transcription annul√©e car le fichier audio est manquant ou inaccessible.\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Impossible de r√©cup√©rer les m√©tadonn√©es et/ou de t√©l√©charger/trouver le fichier audio.\")\n",
        "        print(\"   V√©rifiez l'URL YouTube, votre connexion internet et l'installation de yt-dlp.\")\n",
        "        print(\"üö´ Transcription annul√©e.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Une erreur globale inattendue s'est produite : {e}\")\n",
        "    print(\"--- Traceback de l'erreur ---\")\n",
        "    traceback.print_exc()\n",
        "    print(\"---------------------------\")\n",
        "\n",
        "finally:\n",
        "    # --- Nettoyage (Optionnel - Inchang√©) ---\n",
        "    # D√©commentez pour supprimer l'audio apr√®s traitement\n",
        "    # print(\"\\n--- Nettoyage ---\")\n",
        "    # if file_path and os.path.exists(file_path):\n",
        "    #     try:\n",
        "    #         # Utilisez 'rm -f' pour forcer la suppression sous Linux/Colab si n√©cessaire\n",
        "    #         # subprocess.run(['rm', '-f', file_path], check=True)\n",
        "    #         os.remove(file_path) # Essayez d'abord os.remove\n",
        "    #         print(f\"üóëÔ∏è Fichier audio supprim√© : {file_path}\")\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"‚ùå Erreur lors de la suppression du fichier audio {file_path}: {e}\")\n",
        "    # elif file_path:\n",
        "    #     print(f\"‚ÑπÔ∏è Fichier audio {file_path} non trouv√© pour suppression.\")\n",
        "    # else:\n",
        "    #      print(f\"‚ÑπÔ∏è Aucun fichier audio √† supprimer.\")\n",
        "    pass # Ne rien faire par d√©faut\n",
        "\n",
        "print(\"\\nüèÅ Script termin√©.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # **Traduction Parall√©lis√©e**\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import math # Pour calculer le nombre total de chunks\n",
        "import concurrent.futures # Pour la parall√©lisation\n",
        "import sys # Pour sys.stdout.flush() si besoin dans certains environnements\n",
        "\n",
        "# --- Fonction Utilitaires ---\n",
        "def chunk_list(lst, n):\n",
        "    \"\"\"Divise une liste en sous-listes de taille n.\"\"\"\n",
        "    if not isinstance(lst, list):\n",
        "        raise TypeError(\"L'entr√©e doit √™tre une liste.\")\n",
        "    if n <= 0:\n",
        "        raise ValueError(\"La taille du chunk doit √™tre positive.\")\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i+n]\n",
        "\n",
        "def extract_json_from_response(text_response):\n",
        "    \"\"\"\n",
        "    Tente d'extraire une cha√Æne JSON valide √† partir d'une r√©ponse textuelle,\n",
        "    en g√©rant les blocs de code markdown potentiels (```json ... ```).\n",
        "    \"\"\"\n",
        "    # 1. Essayer de trouver un bloc de code JSON d√©marqu√©\n",
        "    match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', text_response, re.DOTALL)\n",
        "    if match:\n",
        "        # print(\"‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\") # Moins de logs en parall√®le\n",
        "        return match.group(1).strip()\n",
        "\n",
        "    # 2. Si pas de bloc, chercher un JSON qui commence par '[' et finit par ']' (liste)\n",
        "    #    Ou commence par '{' et finit par '}' (objet - moins probable ici mais par s√©curit√©)\n",
        "    match_list = re.search(r'(\\[[\\s\\S]*\\])', text_response, re.DOTALL)\n",
        "    match_obj = re.search(r'(\\{[\\s\\S]*\\})', text_response, re.DOTALL)\n",
        "\n",
        "    if match_list and match_obj:\n",
        "        json_string = match_list.group(1) if match_list.start() < match_obj.start() else match_obj.group(1)\n",
        "        # print(\"‚ÑπÔ∏è JSON d√©tect√© via d√©limiteurs [...] ou {...}.\")\n",
        "        return json_string.strip()\n",
        "    elif match_list:\n",
        "        # print(\"‚ÑπÔ∏è JSON d√©tect√© via d√©limiteurs [...].\")\n",
        "        return match_list.group(1).strip()\n",
        "    elif match_obj:\n",
        "         # print(\"‚ÑπÔ∏è JSON d√©tect√© via d√©limiteurs {...}.\")\n",
        "         return match_obj.group(1).strip()\n",
        "\n",
        "    # 3. Si rien ne fonctionne, retourner le texte original\n",
        "    # print(\"‚ö†Ô∏è Impossible d'isoler un bloc JSON sp√©cifique. Tentative d'analyse du texte brut.\")\n",
        "    return text_response.strip()\n",
        "\n",
        "\n",
        "# --- Fonction de Traduction (pour un seul chunk - reste inchang√©e) ---\n",
        "def translate_chunk_with_gemini(transcript_chunk, api_key, chunk_index, total_chunks):\n",
        "    \"\"\"\n",
        "    Envoie un chunk de transcription √† l'API Gemini pour traduction.\n",
        "    (Cette fonction est appel√©e par les threads)\n",
        "\n",
        "    Args:\n",
        "        transcript_chunk (list): Un morceau (chunk) de la transcription (liste de segments).\n",
        "        api_key (str): Cl√© API Gemini.\n",
        "        chunk_index (int): Index global du chunk (pour l'affichage).\n",
        "        total_chunks (int): Nombre total de chunks (pour l'affichage).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (chunk_index, list | None) - Retourne l'index du chunk et le r√©sultat (liste traduite ou None)\n",
        "               pour pouvoir r√©assembler dans le bon ordre.\n",
        "    \"\"\"\n",
        "    if not transcript_chunk:\n",
        "        print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] est vide, ignor√©.\")\n",
        "        # Retourne l'index et une liste vide pour indiquer qu'il a √©t√© trait√© (mais √©tait vide)\n",
        "        return chunk_index, []\n",
        "\n",
        "    start_time = time.time()\n",
        "    # Utiliser sys.stdout.flush() peut aider si les logs n'apparaissent pas imm√©diatement en environnement multi-thread√©\n",
        "    print(f\"üîÑ [Chunk {chunk_index+1}/{total_chunks}] D√©but traduction ({len(transcript_chunk)} segments)...\", flush=True)\n",
        "\n",
        "    # Utiliser un mod√®le r√©cent et appropri√©\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}\"\n",
        "\n",
        "    try:\n",
        "        transcript_str = json.dumps(transcript_chunk, ensure_ascii=False, indent=2)\n",
        "    except TypeError as e:\n",
        "         print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur de s√©rialisation JSON du chunk : {e}\", flush=True)\n",
        "         return chunk_index, None\n",
        "\n",
        "    prompt = (\n",
        "        \"You are an expert multilingual translator. Below is a JSON array representing segments of an audio transcript.\\n\"\n",
        "        \"For EACH segment object in the array, please perform the following:\\n\"\n",
        "        \"1. Identify the original language of the 'text' field.\\n\"\n",
        "        \"2. Translate the content of the 'text' field into English and add it as a new key-value pair: 'text_english': \\\"<english_translation>\\\".\\n\"\n",
        "        \"3. Translate the content of the 'text' field into French and add it as a new key-value pair: 'text_french': \\\"<french_translation>\\\".\\n\"\n",
        "        \"4. IMPORTANT: Preserve ALL other existing keys and their values ('start', 'duration', 'progress_percentage', etc.) exactly as they are.\\n\"\n",
        "        \"5. Return ONLY the complete, modified JSON array. Do not include any explanatory text before or after the JSON array itself. Respond only with the JSON.\\n\\n\"\n",
        "        \"Here is the JSON array:\\n\"\n",
        "        f\"```json\\n{transcript_str}\\n```\"\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": 0.3,\n",
        "            \"maxOutputTokens\": 8192,\n",
        "             # Sp√©cifier explicitement le format de sortie JSON (si le mod√®le le supporte bien)\n",
        "             # NOTE : Pas tous les mod√®les ou versions supportent response_mime_type de mani√®re fiable.\n",
        "             # Si cela cause des erreurs, commentez la ligne suivante.\n",
        "             \"response_mime_type\": \"application/json\",\n",
        "        },\n",
        "         \"safetySettings\": [\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n",
        "        ]\n",
        "    }\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "    # --- Boucle de tentatives avec backoff exponentiel l√©ger ---\n",
        "    max_retries = 3\n",
        "    base_delay = 2 # secondes\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=payload, timeout=180) # Timeout long\n",
        "            response.raise_for_status() # L√®ve une exception pour 4xx/5xx\n",
        "\n",
        "            response_data = response.json()\n",
        "\n",
        "            # V√©rification des erreurs Gemini\n",
        "            if not response_data.get(\"candidates\"):\n",
        "                prompt_feedback = response_data.get(\"promptFeedback\", {})\n",
        "                block_reason = prompt_feedback.get(\"blockReason\")\n",
        "                safety_ratings = prompt_feedback.get(\"safetyRatings\")\n",
        "                error_message = f\"Aucun candidat retourn√©.\"\n",
        "                if block_reason: error_message += f\" Raison blocage: {block_reason}.\"\n",
        "                if safety_ratings: error_message += f\" Ratings: {safety_ratings}\"\n",
        "                # Si bloqu√© pour s√©curit√©, ne pas retenter\n",
        "                if block_reason == 'SAFETY':\n",
        "                     print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Blocage API Gemini (S√©curit√©). {error_message}\", flush=True)\n",
        "                     return chunk_index, None\n",
        "                # Pour d'autres erreurs sans candidat, on peut retenter\n",
        "                if attempt < max_retries - 1:\n",
        "                    delay = base_delay * (2 ** attempt)\n",
        "                    print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Erreur API Gemini: {error_message}. Tentative {attempt+2}/{max_retries} apr√®s {delay}s...\", flush=True)\n",
        "                    time.sleep(delay)\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur API Gemini: {error_message}. Echec apr√®s {max_retries} tentatives.\", flush=True)\n",
        "                    return chunk_index, None\n",
        "\n",
        "            candidate = response_data[\"candidates\"][0]\n",
        "            finish_reason = candidate.get(\"finishReason\")\n",
        "\n",
        "            if finish_reason not in [\"STOP\", \"MAX_TOKENS\"]:\n",
        "                print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Fin de g√©n√©ration anormale: {finish_reason}.\", flush=True)\n",
        "                # Retenter si ce n'est pas une erreur fatale\n",
        "                if finish_reason == 'SAFETY': # Ne pas retenter si bloqu√© pour s√©curit√©\n",
        "                     print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Blocage API Gemini (S√©curit√© - finish_reason).\", flush=True)\n",
        "                     return chunk_index, None\n",
        "                if attempt < max_retries - 1:\n",
        "                    delay = base_delay * (2 ** attempt)\n",
        "                    print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Tentative {attempt+2}/{max_retries} apr√®s {delay}s...\", flush=True)\n",
        "                    time.sleep(delay)\n",
        "                    continue\n",
        "                else:\n",
        "                     print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec apr√®s {max_retries} tentatives (finish_reason={finish_reason}).\", flush=True)\n",
        "                     return chunk_index, None # Echec final si raison anormale\n",
        "\n",
        "\n",
        "            if \"content\" not in candidate or \"parts\" not in candidate[\"content\"] or not candidate[\"content\"][\"parts\"]:\n",
        "                 print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Structure de r√©ponse inattendue (manque content/parts).\", flush=True)\n",
        "                 # Retenter pourrait aider si c'est une erreur transitoire\n",
        "                 if attempt < max_retries - 1:\n",
        "                    delay = base_delay * (2 ** attempt)\n",
        "                    print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Tentative {attempt+2}/{max_retries} apr√®s {delay}s...\", flush=True)\n",
        "                    time.sleep(delay)\n",
        "                    continue\n",
        "                 else:\n",
        "                     print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec apr√®s {max_retries} tentatives (structure r√©ponse).\", flush=True)\n",
        "                     return chunk_index, None\n",
        "\n",
        "            raw_text_response = candidate[\"content\"][\"parts\"][0][\"text\"]\n",
        "            json_string = extract_json_from_response(raw_text_response)\n",
        "            result_json = json.loads(json_string) # Peut lever JSONDecodeError\n",
        "\n",
        "            if not isinstance(result_json, list):\n",
        "                 print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Le r√©sultat d√©cod√© n'est pas une liste.\", flush=True)\n",
        "                 # Probablement une erreur du LLM, retenter ne sert √† rien ici\n",
        "                 return chunk_index, None\n",
        "            if len(result_json) != len(transcript_chunk):\n",
        "                 print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Le nombre de segments retourn√©s ({len(result_json)}) != entr√©e ({len(transcript_chunk)}). Probablement tronqu√© (MAX_TOKENS?).\", flush=True)\n",
        "                 # On accepte le r√©sultat partiel mais on logue l'avertissement\n",
        "                 # return chunk_index, None # Ou consid√©rer comme un √©chec si on veut √™tre strict\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"‚úÖ [Chunk {chunk_index+1}/{total_chunks}] Traduction r√©ussie en {elapsed_time:.2f} secondes (Tentative {attempt+1}).\", flush=True)\n",
        "            return chunk_index, result_json # Succ√®s\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Timeout lors de la requ√™te (Tentative {attempt+1}).\", flush=True)\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s timeout.\", flush=True)\n",
        "                return chunk_index, None\n",
        "            delay = base_delay * (2 ** attempt)\n",
        "            print(f\"   Retentative dans {delay}s...\", flush=True)\n",
        "            time.sleep(delay)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            # G√©rer les erreurs 429 (Too Many Requests) sp√©cifiquement si possible\n",
        "            status_code = e.response.status_code if e.response is not None else None\n",
        "            if status_code == 429:\n",
        "                print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Erreur 429 (Too Many Requests) (Tentative {attempt+1}).\", flush=True)\n",
        "                # Attendre plus longtemps et retenter\n",
        "                delay = 10 * (attempt + 1) # Attente plus longue pour 429\n",
        "                print(f\"   Attente de {delay}s avant la prochaine tentative...\", flush=True)\n",
        "                time.sleep(delay)\n",
        "                if attempt == max_retries - 1:\n",
        "                     print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s erreur 429.\", flush=True)\n",
        "                     return chunk_index, None\n",
        "            else:\n",
        "                print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur r√©seau/HTTP (Tentative {attempt+1}): {e}\", flush=True)\n",
        "                # Pour les erreurs non-429, un backoff standard suffit\n",
        "                if attempt == max_retries - 1:\n",
        "                    print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s erreur r√©seau/HTTP.\", flush=True)\n",
        "                    return chunk_index, None\n",
        "                delay = base_delay * (2 ** attempt)\n",
        "                print(f\"   Retentative dans {delay}s...\", flush=True)\n",
        "                time.sleep(delay)\n",
        "\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur analyse JSON r√©ponse (Tentative {attempt+1}): {e}\", flush=True)\n",
        "            print(f\"--- R√©ponse textuelle brute re√ßue (Chunk {chunk_index+1}) ---\")\n",
        "            print(raw_text_response[:500] + \"...\" if len(raw_text_response) > 500 else raw_text_response)\n",
        "            print(\"--- Fin R√©ponse textuelle brute ---\", flush=True)\n",
        "            # Retenter peut aider si la r√©ponse √©tait corrompue transitoirement\n",
        "            if attempt == max_retries - 1:\n",
        "                 print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s erreur JSON.\", flush=True)\n",
        "                 return chunk_index, None\n",
        "            delay = base_delay * (2 ** attempt)\n",
        "            print(f\"   Retentative dans {delay}s...\", flush=True)\n",
        "            time.sleep(delay)\n",
        "\n",
        "        except (KeyError, IndexError) as e:\n",
        "            print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur acc√®s cl√©s r√©ponse API (Tentative {attempt+1}): {e}\", flush=True)\n",
        "             # Retenter peut aider si la r√©ponse √©tait malform√©e transitoirement\n",
        "            if attempt == max_retries - 1:\n",
        "                 print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s erreur structure r√©ponse.\", flush=True)\n",
        "                 return chunk_index, None\n",
        "            delay = base_delay * (2 ** attempt)\n",
        "            print(f\"   Retentative dans {delay}s...\", flush=True)\n",
        "            time.sleep(delay)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur inattendue (Tentative {attempt+1}): {e.__class__.__name__}: {e}\", flush=True)\n",
        "            # Retenter pour erreurs g√©n√©riques\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s erreur inattendue.\", flush=True)\n",
        "                return chunk_index, None\n",
        "            delay = base_delay * (2 ** attempt)\n",
        "            print(f\"   Retentative dans {delay}s...\", flush=True)\n",
        "            time.sleep(delay)\n",
        "\n",
        "    # Si on sort de la boucle sans succ√®s\n",
        "    print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s {max_retries} tentatives.\", flush=True)\n",
        "    return chunk_index, None\n",
        "\n",
        "\n",
        "# --- Fonction pour traiter un lot (batch) de chunks en parall√®le ---\n",
        "def translate_batch(batch_chunks_with_indices, api_key, total_chunks, max_workers=14):\n",
        "    \"\"\"\n",
        "    Traite un lot de chunks en parall√®le en utilisant ThreadPoolExecutor.\n",
        "\n",
        "    Args:\n",
        "        batch_chunks_with_indices (list): Liste de tuples (index_global, chunk_data).\n",
        "        api_key (str): Cl√© API Gemini.\n",
        "        total_chunks (int): Nombre total de chunks dans la transcription compl√®te.\n",
        "        max_workers (int): Nombre maximum de threads √† utiliser.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionnaire {index_global: resultat_traduction} pour ce lot.\n",
        "              resultat_traduction est soit la liste des segments traduits, soit None en cas d'√©chec.\n",
        "    \"\"\"\n",
        "    batch_results = {}\n",
        "    # Utilise un ThreadPoolExecutor pour g√©rer les threads\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Soumet chaque t√¢che de traduction de chunk au pool de threads\n",
        "        # future_to_index mappe l'objet Future (repr√©sentant l'ex√©cution) √† l'index global du chunk\n",
        "        future_to_index = {\n",
        "            executor.submit(translate_chunk_with_gemini, chunk_data, api_key, index, total_chunks): index\n",
        "            for index, chunk_data in batch_chunks_with_indices\n",
        "        }\n",
        "\n",
        "        # R√©cup√®re les r√©sultats au fur et √† mesure que les t√¢ches se terminent\n",
        "        for future in concurrent.futures.as_completed(future_to_index):\n",
        "            original_index = future_to_index[future]\n",
        "            try:\n",
        "                # Obtient le r√©sultat de la fonction (qui est un tuple: index, data)\n",
        "                index_result, translated_data = future.result()\n",
        "                # Stocke le r√©sultat (translated_data) en utilisant l'index original comme cl√©\n",
        "                batch_results[original_index] = translated_data\n",
        "            except Exception as exc:\n",
        "                # Si l'ex√©cution de la t√¢che elle-m√™me a lev√© une exception impr√©vue\n",
        "                print(f\"‚ÄºÔ∏è [Chunk {original_index+1}/{total_chunks}] a g√©n√©r√© une exception dans le thread: {exc}\", flush=True)\n",
        "                batch_results[original_index] = None # Marque comme √©chou√©\n",
        "\n",
        "    # Retourne le dictionnaire des r√©sultats pour ce lot, index√© par l'index global du chunk\n",
        "    return batch_results\n",
        "\n",
        "\n",
        "# --- Script Principal d'Ex√©cution ---\n",
        "\n",
        "# !!! IMPORTANT: Utiliser Colab Secrets ou une m√©thode s√©curis√©e pour la cl√© API. !!!\n",
        "# from google.colab import userdata\n",
        "# api_key = userdata.get('GEMINI_API_KEY')\n",
        "api_key = \"AIzaSyBiZONd6VA8y9zAd8vueZRo_IrPnn7iHlw\" #@param {type:\"string\"}\n",
        "\n",
        "# Nombre de chunks √† envoyer en parall√®le (limit√© par l'API et vos ressources)\n",
        "# Gemini free tier = 15 RPM (Requests Per Minute)\n",
        "# On prend 14 pour laisser une petite marge.\n",
        "PARALLEL_CHUNKS = 14 #@param {type:\"integer\"}\n",
        "\n",
        "# D√©lai d'attente entre les lots (en secondes) pour respecter la limite de 15 RPM\n",
        "# Si on envoie 14 requ√™tes, on attend 60s pour pouvoir en envoyer 14 autres la minute suivante.\n",
        "# Mettre un peu plus pour √™tre s√ªr (ex: 61 ou 62)\n",
        "RATE_LIMIT_DELAY = 61 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "if api_key == \"VOTRE_CLE_API_GEMINI_ICI\" or not api_key:\n",
        "     print(\"üõë Veuillez fournir votre cl√© API Gemini dans la variable 'api_key'.\")\n",
        "     # raise ValueError(\"Cl√© API Gemini manquante.\")\n",
        "else:\n",
        "    # Assurer l'existence et le format de 'transcript'\n",
        "    if 'transcript' not in locals() or not isinstance(transcript, dict) or 'segments' not in transcript or not isinstance(transcript['segments'], list):\n",
        "        print(\"‚ùå La variable 'transcript' n'est pas d√©finie ou n'a pas le format attendu.\")\n",
        "        print(\"   Assurez-vous que la cellule de transcription a √©t√© ex√©cut√©e avec succ√®s auparavant.\")\n",
        "        print(\"   Format attendu: {'segments': [{'text': ..., 'start': ..., ...}, ...], ...}\")\n",
        "        transcript = {\"segments\": []} # Cr√©er un transcript vide pour √©viter les erreurs\n",
        "\n",
        "    source_segments = transcript.get(\"segments\", [])\n",
        "    if not source_segments:\n",
        "        print(\"‚ÑπÔ∏è La transcription source ne contient aucun segment. Aucune traduction √† effectuer.\")\n",
        "        translated_transcript_data = transcript.copy()\n",
        "        translated_transcript_data['segments'] = []\n",
        "    else:\n",
        "        print(f\"\\nPr√©paration de la traduction pour {len(source_segments)} segments...\")\n",
        "\n",
        "        # Taille du chunk (nombre de segments par requ√™te API) - Ajustable\n",
        "        chunk_size = 50 # @param {type:\"integer\"}\n",
        "        if chunk_size <= 0:\n",
        "            print(\"‚ö†Ô∏è Taille de chunk invalide, utilisation de la valeur par d√©faut 50.\")\n",
        "            chunk_size = 50\n",
        "\n",
        "        # Cr√©er une liste de tous les chunks avec leur index global\n",
        "        all_chunks_with_indices = list(enumerate(chunk_list(source_segments, chunk_size)))\n",
        "        total_chunks = len(all_chunks_with_indices)\n",
        "\n",
        "        print(f\"D√©coupage en {total_chunks} chunks de {chunk_size} segments maximum.\")\n",
        "        print(f\"Traitement par lots de {PARALLEL_CHUNKS} chunks en parall√®le.\")\n",
        "\n",
        "        # Dictionnaire pour stocker tous les r√©sultats, index√©s par l'index du chunk\n",
        "        all_results_dict = {}\n",
        "        failed_chunk_indices = []\n",
        "        successful_chunk_count = 0\n",
        "\n",
        "        start_total_time = time.time()\n",
        "        batch_num = 0\n",
        "        total_batches = math.ceil(total_chunks / PARALLEL_CHUNKS)\n",
        "\n",
        "        # Boucler sur les lots (batches) de chunks\n",
        "        for i in range(0, total_chunks, PARALLEL_CHUNKS):\n",
        "            batch_num += 1\n",
        "            # S√©lectionner le lot actuel de chunks (jusqu'√† PARALLEL_CHUNKS)\n",
        "            current_batch_with_indices = all_chunks_with_indices[i : i + PARALLEL_CHUNKS]\n",
        "            batch_start_index = current_batch_with_indices[0][0] + 1 # Index du 1er chunk du lot\n",
        "            batch_end_index = current_batch_with_indices[-1][0] + 1 # Index du dernier chunk du lot\n",
        "\n",
        "            print(f\"\\n--- Traitement du Lot {batch_num}/{total_batches} (Chunks {batch_start_index} √† {batch_end_index}) ---\", flush=True)\n",
        "\n",
        "            # Traduire le lot en parall√®le\n",
        "            batch_start_time = time.time()\n",
        "            batch_results = translate_batch(current_batch_with_indices, api_key, total_chunks, PARALLEL_CHUNKS)\n",
        "            batch_end_time = time.time()\n",
        "            print(f\"--- Lot {batch_num}/{total_batches} termin√© en {batch_end_time - batch_start_time:.2f} secondes ---\", flush=True)\n",
        "\n",
        "\n",
        "            # Mettre √† jour les r√©sultats globaux et compter les succ√®s/√©checs pour ce lot\n",
        "            for index, result_data in batch_results.items():\n",
        "                all_results_dict[index] = result_data # Stocker le r√©sultat (liste ou None)\n",
        "                if result_data is not None:\n",
        "                    # V√©rification suppl√©mentaire optionnelle des cl√©s (peut ralentir un peu)\n",
        "                    if result_data and isinstance(result_data, list) and result_data[0]:\n",
        "                         if 'text_english' not in result_data[0] or 'text_french' not in result_data[0]:\n",
        "                               print(f\"‚ö†Ô∏è [Chunk {index+1}/{total_chunks}] Cl√©s 'text_english'/'text_french' manquantes dans le r√©sultat retourn√©. Format peut-√™tre incorrect.\", flush=True)\n",
        "                               # On le compte quand m√™me comme 'r√©ussi' car on a re√ßu une liste, mais avec un avertissement\n",
        "                               successful_chunk_count += 1\n",
        "                         else:\n",
        "                               successful_chunk_count += 1\n",
        "                    elif not result_data: # Cas du chunk source vide trait√© correctement\n",
        "                         successful_chunk_count += 1\n",
        "                    # else: # Cas o√π result_data est None (d√©j√† trait√© ci-dessous)\n",
        "                    #     pass\n",
        "                else:\n",
        "                    failed_chunk_indices.append(index + 1) # Ajouter l'index (base 1) du chunk √©chou√©\n",
        "\n",
        "            # --- Pause pour la limite de taux ---\n",
        "            # Si ce n'est pas le dernier lot, attendre avant de lancer le suivant\n",
        "            if i + PARALLEL_CHUNKS < total_chunks:\n",
        "                print(f\"\\n‚è∏Ô∏è Respect de la limite de taux : Attente de {RATE_LIMIT_DELAY} secondes avant le prochain lot...\", flush=True)\n",
        "                time.sleep(RATE_LIMIT_DELAY)\n",
        "\n",
        "        end_total_time = time.time()\n",
        "\n",
        "        # --- Assemblage final des r√©sultats dans le bon ordre ---\n",
        "        all_translated_segments = []\n",
        "        print(\"\\nAssemblage des r√©sultats...\", flush=True)\n",
        "        for index in range(total_chunks):\n",
        "            if index in all_results_dict and all_results_dict[index] is not None:\n",
        "                all_translated_segments.extend(all_results_dict[index])\n",
        "            # else: # Si le chunk a √©chou√© (index pas dans dict ou valeur None), on ne l'ajoute pas\n",
        "                # print(f\"‚ÑπÔ∏è Chunk {index+1} manquant (√©chec ou vide).\") # Optionnel: loguer les chunks manquants\n",
        "        print(\"Assemblage termin√©.\", flush=True)\n",
        "\n",
        "\n",
        "        # --- R√©sum√© Final ---\n",
        "        print(\"\\n--- R√©sum√© de la Traduction ---\")\n",
        "        print(f\"Temps total de traduction : {end_total_time - start_total_time:.2f} secondes.\")\n",
        "        print(f\"Chunks trait√©s avec succ√®s : {successful_chunk_count}/{total_chunks}\")\n",
        "        if failed_chunk_indices:\n",
        "            print(f\"Chunks ayant √©chou√© ({len(failed_chunk_indices)}): {', '.join(map(str, sorted(failed_chunk_indices)))}\")\n",
        "        # Comparer le nombre de segments attendus et obtenus\n",
        "        final_segment_count = len(all_translated_segments)\n",
        "        print(f\"Nombre total de segments dans la sortie finale : {final_segment_count}\")\n",
        "        if final_segment_count < len(source_segments):\n",
        "             print(f\"‚ö†Ô∏è {len(source_segments) - final_segment_count} segments sont manquants par rapport √† l'original (dus aux chunks √©chou√©s ou potentiellement tronqu√©s).\")\n",
        "        elif final_segment_count > len(source_segments):\n",
        "             print(f\"‚ö†Ô∏è Le nombre de segments finaux ({final_segment_count}) est sup√©rieur √† l'original ({len(source_segments)}). V√©rifiez les doublons potentiels ou erreurs d'assemblage.\")\n",
        "        print(\"------------------------------\\n\")\n",
        "\n",
        "\n",
        "        # Cr√©er le dictionnaire final\n",
        "        translated_transcript_data = transcript.copy()\n",
        "        translated_transcript_data[\"segments\"] = all_translated_segments\n",
        "\n",
        "        # Affichage JSON final (optionnel, peut √™tre tr√®s long)\n",
        "        # try:\n",
        "        #    print(\"--- D√©but de la Transcription JSON Traduite (Aper√ßu) ---\")\n",
        "        #    # Afficher seulement les premi√®res N segments pour √©viter un output trop massif\n",
        "        #    preview_segments = translated_transcript_data[\"segments\"][:5] # Afficher les 5 premiers\n",
        "        #    temp_preview = translated_transcript_data.copy()\n",
        "        #    temp_preview[\"segments\"] = preview_segments\n",
        "        #    print(json.dumps(temp_preview, indent=2, ensure_ascii=False))\n",
        "        #    if len(translated_transcript_data[\"segments\"]) > 5:\n",
        "        #        print(f\"\\n   ... et {len(translated_transcript_data['segments']) - 5} autres segments.\")\n",
        "        #    print(\"--- Fin de la Transcription JSON Traduite (Aper√ßu) ---\")\n",
        "        # except Exception as e:\n",
        "        #      print(f\"‚ùå Erreur lors de la g√©n√©ration de l'aper√ßu JSON : {e}\")\n",
        "\n",
        "        # Sauvegarde JSON (si n√©cessaire)\n",
        "        # json_output_filename_translated = \"transcript_translated_parallel.json\"\n",
        "        # try:\n",
        "        #     with open(json_output_filename_translated, \"w\", encoding=\"utf-8\") as f:\n",
        "        #         json.dump(translated_transcript_data, f, indent=2, ensure_ascii=False)\n",
        "        #     print(f\"‚úÖ Transcription traduite sauvegard√©e dans : {json_output_filename_translated}\")\n",
        "        # except IOError as e:\n",
        "        #     print(f\"‚ùå Erreur lors de la sauvegarde du fichier JSON traduit : {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "1r0oScNyF5g2",
        "outputId": "97a85753-62c8-49a8-e8b0-ac3b8274f102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pr√©paration de la traduction pour 1933 segments...\n",
            "D√©coupage en 39 chunks de 50 segments maximum.\n",
            "Traitement par lots de 14 chunks en parall√®le.\n",
            "\n",
            "--- Traitement du Lot 1/3 (Chunks 1 √† 14) ---\n",
            "üîÑ [Chunk 1/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 2/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 3/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 4/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 5/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 6/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 7/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 8/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 9/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 10/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 11/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 12/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 13/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 14/39] D√©but traduction (50 segments)...\n",
            "‚úÖ [Chunk 14/39] Traduction r√©ussie en 21.54 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 8/39] Traduction r√©ussie en 21.57 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 7/39] Traduction r√©ussie en 21.63 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 10/39] Traduction r√©ussie en 21.88 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 6/39] Traduction r√©ussie en 21.97 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 5/39] Traduction r√©ussie en 22.16 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 12/39] Traduction r√©ussie en 22.34 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 13/39] Traduction r√©ussie en 22.56 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 11/39] Traduction r√©ussie en 22.63 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 1/39] Traduction r√©ussie en 22.80 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 4/39] Traduction r√©ussie en 23.18 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 3/39] Traduction r√©ussie en 23.51 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 2/39] Traduction r√©ussie en 27.60 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 9/39] Traduction r√©ussie en 28.69 secondes (Tentative 1).\n",
            "--- Lot 1/3 termin√© en 28.73 secondes ---\n",
            "\n",
            "‚è∏Ô∏è Respect de la limite de taux : Attente de 61 secondes avant le prochain lot...\n",
            "\n",
            "--- Traitement du Lot 2/3 (Chunks 15 √† 28) ---\n",
            "üîÑ [Chunk 15/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 16/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 17/39] D√©but traduction (50 segments)...üîÑ [Chunk 18/39] D√©but traduction (50 segments)...\n",
            "\n",
            "üîÑ [Chunk 19/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 20/39] D√©but traduction (50 segments)...üîÑ [Chunk 21/39] D√©but traduction (50 segments)...\n",
            "\n",
            "üîÑ [Chunk 22/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 23/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 24/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 25/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 26/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 27/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 28/39] D√©but traduction (50 segments)...\n",
            "‚úÖ [Chunk 28/39] Traduction r√©ussie en 21.28 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 15/39] Traduction r√©ussie en 21.64 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 21/39] Traduction r√©ussie en 21.68 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 20/39] Traduction r√©ussie en 21.84 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 23/39] Traduction r√©ussie en 22.01 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 18/39] Traduction r√©ussie en 22.60 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 26/39] Traduction r√©ussie en 22.71 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 17/39] Traduction r√©ussie en 22.79 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 22/39] Traduction r√©ussie en 22.83 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 19/39] Traduction r√©ussie en 22.91 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 16/39] Traduction r√©ussie en 23.34 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 25/39] Traduction r√©ussie en 24.54 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 27/39] Traduction r√©ussie en 27.57 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 24/39] Traduction r√©ussie en 27.98 secondes (Tentative 1).\n",
            "--- Lot 2/3 termin√© en 28.05 secondes ---\n",
            "\n",
            "‚è∏Ô∏è Respect de la limite de taux : Attente de 61 secondes avant le prochain lot...\n",
            "\n",
            "--- Traitement du Lot 3/3 (Chunks 29 √† 39) ---\n",
            "üîÑ [Chunk 29/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 30/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 31/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 32/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 33/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 34/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 35/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 36/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 37/39] D√©but traduction (50 segments)...üîÑ [Chunk 38/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 39/39] D√©but traduction (33 segments)...\n",
            "\n",
            "‚úÖ [Chunk 39/39] Traduction r√©ussie en 15.26 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 29/39] Traduction r√©ussie en 22.04 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 30/39] Traduction r√©ussie en 22.66 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 35/39] Traduction r√©ussie en 23.14 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 33/39] Traduction r√©ussie en 23.25 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 34/39] Traduction r√©ussie en 23.34 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 31/39] Traduction r√©ussie en 23.64 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 37/39] Traduction r√©ussie en 23.95 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 32/39] Traduction r√©ussie en 27.84 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 38/39] Traduction r√©ussie en 29.04 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 36/39] Traduction r√©ussie en 29.22 secondes (Tentative 1).\n",
            "--- Lot 3/3 termin√© en 29.26 secondes ---\n",
            "\n",
            "Assemblage des r√©sultats...\n",
            "Assemblage termin√©.\n",
            "\n",
            "--- R√©sum√© de la Traduction ---\n",
            "Temps total de traduction : 208.05 secondes.\n",
            "Chunks trait√©s avec succ√®s : 39/39\n",
            "Nombre total de segments dans la sortie finale : 1933\n",
            "------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OIUGwSN_2bA9",
        "outputId": "a2272657-86b3-458d-bbdc-1015dba0a9cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Pr√©paration de la traduction pour 701 segments...\n",
            "D√©coupage en 15 chunks de 50 segments maximum.\n",
            "üîÑ [Chunk 1/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 1/15] Traduction r√©ussie en 27.64 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 2/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 2/15] Traduction r√©ussie en 28.44 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 3/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 3/15] Traduction r√©ussie en 28.99 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 4/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 4/15] Traduction r√©ussie en 28.63 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 5/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 5/15] Traduction r√©ussie en 28.15 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 6/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 6/15] Traduction r√©ussie en 28.24 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 7/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 7/15] Traduction r√©ussie en 27.59 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 8/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 8/15] Traduction r√©ussie en 28.72 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 9/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 9/15] Traduction r√©ussie en 27.16 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 10/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 10/15] Traduction r√©ussie en 28.05 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 11/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 11/15] Traduction r√©ussie en 28.05 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 12/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 12/15] Traduction r√©ussie en 27.56 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 13/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 13/15] Traduction r√©ussie en 27.98 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 14/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 14/15] Traduction r√©ussie en 26.84 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 15/15] Envoi √† l'API Gemini (1 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 15/15] Traduction r√©ussie en 2.01 secondes.\n",
            "--------------------\n",
            "\n",
            "--- R√©sum√© de la Traduction ---\n",
            "Temps total de traduction : 394.06 secondes.\n",
            "Chunks trait√©s avec succ√®s : 15/15\n",
            "Nombre total de segments trait√©s (potentiellement traduits) : 701/701\n",
            "------------------------------\n",
            "\n",
            "--- Fin de la Transcription JSON ---\n"
          ]
        }
      ],
      "source": [
        "# @title # **Transcription & Translation (Optimized)**\n",
        "\n",
        "import whisper\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import concurrent.futures\n",
        "import math\n",
        "import requests\n",
        "import torch # For GPU check\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "\n",
        "# --- Configuration ---\n",
        "WHISPER_MODEL_SIZE = \"base\" # @param [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\", \"large-v3\"]\n",
        "# Nombre de segments √† envoyer √† Gemini en une seule requ√™te API\n",
        "TRANSLATION_CHUNK_SIZE = 50 # @param {type:\"integer\"}\n",
        "# Nombre de requ√™tes de traduction simultan√©es vers l'API Gemini\n",
        "MAX_TRANSLATION_WORKERS = 10 # @param {type:\"integer\"}\n",
        "# Cl√© API Gemini - !! NE PAS METTRE EN DUR DANS LE CODE PARTAG√â / PRODUCTION !!\n",
        "# Utiliser Colab Secrets: from google.colab import userdata; api_key = userdata.get('GEMINI_API_KEY')\n",
        "GEMINI_API_KEY = \"VOTRE_CLE_API_GEMINI_ICI\" #@param {type:\"string\"} # REMPLACEZ PAR VOTRE VRAIE CLE\n",
        "\n",
        "# --- Fonction download_youtube_audio_improved (Mostly Unchanged, added typing) ---\n",
        "def download_youtube_audio_improved(youtube_url: str, output_path: str) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Downloads YouTube audio and reliably extracts metadata using separate yt-dlp calls.\n",
        "\n",
        "    Args:\n",
        "        youtube_url: The URL of the YouTube video.\n",
        "        output_path: The desired path to save the MP3 audio file.\n",
        "\n",
        "    Returns:\n",
        "        A tuple (audio_file_path, video_info_dict).\n",
        "        Returns (None, video_info) if audio download fails but metadata is retrieved.\n",
        "        Returns (None, None) if metadata retrieval fails.\n",
        "    \"\"\"\n",
        "    video_info = None\n",
        "    audio_file_path = None\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    if not os.path.exists(output_dir):\n",
        "        try:\n",
        "            os.makedirs(output_dir)\n",
        "            print(f\"üìÅ Cr√©ation du r√©pertoire de sortie : {output_dir}\")\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Erreur lors de la cr√©ation du r√©pertoire {output_dir}: {e}\")\n",
        "            return None, None # Cannot proceed without output directory\n",
        "\n",
        "    # --- Step 1: Get Metadata ---\n",
        "    print(\"‚ÑπÔ∏è  R√©cup√©ration des m√©tadonn√©es de la vid√©o...\")\n",
        "    try:\n",
        "        metadata_command = [\n",
        "            \"yt-dlp\", \"--dump-json\", \"--encoding\", \"utf-8\", youtube_url,\n",
        "        ]\n",
        "        metadata_result = subprocess.run(metadata_command, check=True, capture_output=True, text=True, encoding='utf-8', errors='replace')\n",
        "        metadata = json.loads(metadata_result.stdout)\n",
        "\n",
        "        video_info = {\n",
        "            \"video_id\": metadata.get(\"id\", \"N/A\"),\n",
        "            \"channel_name\": metadata.get(\"uploader\", \"N/A\"),\n",
        "            \"channel_url\": metadata.get(\"uploader_url\", \"N/A\"),\n",
        "            \"title\": metadata.get(\"title\", \"N/A\"),\n",
        "            \"description\": metadata.get(\"description\", \"N/A\"),\n",
        "        }\n",
        "        # Fallback for video_id if yt-dlp fails to get it\n",
        "        if video_info[\"video_id\"] == \"N/A\":\n",
        "             video_id_match = re.search(r\"v=([a-zA-Z0-9_-]+)\", youtube_url)\n",
        "             video_info[\"video_id\"] = video_id_match.group(1) if video_id_match else \"UNKNOWN\"\n",
        "             print(f\"‚ö†Ô∏è ID vid√©o non trouv√© via yt-dlp, fallback regex: {video_info['video_id']}\")\n",
        "\n",
        "        print(\"‚úÖ M√©tadonn√©es r√©cup√©r√©es.\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Erreur yt-dlp (m√©tadonn√©es) (Code: {e.returncode}).\")\n",
        "        print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "        # Try to decode stderr even if it has errors\n",
        "        error_output = e.stderr.strip() if e.stderr else \"(no stderr)\"\n",
        "        print(f\"   Erreur: {error_output}\")\n",
        "        return None, None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Erreur analyse JSON m√©tadonn√©es yt-dlp : {e}\")\n",
        "        print(f\"--- Sortie brute yt-dlp ---\\n{metadata_result.stdout[:500]}...\\n---\")\n",
        "        return None, None\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå yt-dlp non trouv√©. Installez avec: pip install yt-dlp\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue (m√©tadonn√©es) : {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # --- Step 2: Download Audio ---\n",
        "    print(f\"üîÑ T√©l√©chargement/V√©rification audio vers {output_path}...\")\n",
        "    try:\n",
        "        # Check if file exists AND is non-empty before trying download\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "             print(f\"‚ÑπÔ∏è Fichier audio '{output_path}' existe d√©j√† et n'est pas vide. Utilisation du fichier existant.\")\n",
        "             audio_file_path = output_path\n",
        "        else:\n",
        "            if os.path.exists(output_path):\n",
        "                 print(f\"‚ÑπÔ∏è Fichier audio '{output_path}' existe mais est vide. Tentative de t√©l√©chargement.\")\n",
        "\n",
        "            download_command = [\n",
        "                \"yt-dlp\", \"-x\", \"--audio-format\", \"mp3\",\n",
        "                # '--audio-quality', '0', # Optional: Best audio quality\n",
        "                \"-o\", output_path,\n",
        "                \"--encoding\", \"utf-8\",\n",
        "                # \"-v\", # Uncomment for verbose debug output\n",
        "                youtube_url,\n",
        "            ]\n",
        "            # Capture both stdout and stderr\n",
        "            download_result = subprocess.run(download_command, check=True, capture_output=True, text=True, encoding='utf-8', errors='replace')\n",
        "\n",
        "            # Double-check file existence and size after download attempt\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f\"‚úÖ Fichier audio t√©l√©charg√©/v√©rifi√© : {output_path}\")\n",
        "                audio_file_path = output_path\n",
        "            else:\n",
        "                print(f\"‚ùå ERREUR: yt-dlp termin√© sans erreur mais fichier '{output_path}' introuvable ou vide apr√®s tentative.\")\n",
        "                print(f\"--- Sortie yt-dlp (stdout) ---\\n{download_result.stdout.strip()}\")\n",
        "                print(f\"--- Sortie yt-dlp (stderr) ---\\n{download_result.stderr.strip()}\")\n",
        "                return None, video_info # Return metadata even if download failed\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Log error even if the file exists (e.g., if download failed mid-way but left a file)\n",
        "        print(f\"‚ùå Erreur yt-dlp (t√©l√©chargement) (Code: {e.returncode}).\")\n",
        "        print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "        error_output = e.stderr.strip() if e.stderr else \"(no stderr)\"\n",
        "        print(f\"   Erreur: {error_output}\")\n",
        "        # Check if file exists despite error (maybe it completed partially?)\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "             print(f\"‚ö†Ô∏è Fichier '{output_path}' existe malgr√© l'erreur yt-dlp. Utilisation prudente.\")\n",
        "             audio_file_path = output_path # Use existing file cautiously\n",
        "        else:\n",
        "             return None, video_info # Failed, return metadata only\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå yt-dlp non trouv√©. Installez avec: pip install yt-dlp\")\n",
        "        return None, video_info\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue (t√©l√©chargement) : {e}\")\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "            print(f\"‚ö†Ô∏è Utilisation du fichier audio existant '{output_path}' malgr√© l'erreur inattendue.\")\n",
        "            audio_file_path = output_path\n",
        "        else:\n",
        "            return None, video_info\n",
        "\n",
        "    # Final check\n",
        "    if audio_file_path and (not os.path.exists(audio_file_path) or os.path.getsize(audio_file_path) == 0):\n",
        "        print(f\"‚ùå ERREUR FINALE: Chemin audio '{audio_file_path}' retourn√© mais fichier introuvable ou vide.\")\n",
        "        return None, video_info\n",
        "\n",
        "    return audio_file_path, video_info\n",
        "\n",
        "\n",
        "# --- Fonction transcribe_audio (Optimized for GPU, clearer logging) ---\n",
        "def transcribe_audio(file_path: str, model_size: str = \"base\", video_info: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Transcribes an audio file using Whisper, prioritizing GPU.\n",
        "\n",
        "    Args:\n",
        "      file_path: Path to the audio file.\n",
        "      model_size: Whisper model size.\n",
        "      video_info: Optional dictionary with video metadata.\n",
        "\n",
        "    Returns:\n",
        "      Dictionary containing 'segments' list and potentially video info.\n",
        "      Returns {'segments': []} on critical error.\n",
        "    \"\"\"\n",
        "    start_load_time = time.time()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"üîÑ Chargement du mod√®le Whisper '{model_size}' sur '{device}'...\")\n",
        "    try:\n",
        "        model = whisper.load_model(model_size, device=device)\n",
        "        load_time = time.time() - start_load_time\n",
        "        print(f\"‚úÖ Mod√®le charg√© en {load_time:.2f} secondes.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors du chargement du mod√®le : {e}\")\n",
        "        return {\"segments\": []}\n",
        "\n",
        "    print(f\"üéôÔ∏è D√©but de la transcription pour {os.path.basename(file_path)}...\")\n",
        "    start_transcribe_time = time.time()\n",
        "    try:\n",
        "        # Use fp16=False if on CPU or if encountering precision issues on some GPUs\n",
        "        use_fp16 = True if device == \"cuda\" else False\n",
        "        result = model.transcribe(file_path, verbose=False, fp16=use_fp16) # verbose=True for debug timestamps\n",
        "        transcription_time = time.time() - start_transcribe_time\n",
        "        print(f\"üïí Transcription termin√©e en {transcription_time:.2f} secondes.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Erreur: Fichier audio introuvable √† '{file_path}'\")\n",
        "        return {\"segments\": []}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors de la transcription : {e}\")\n",
        "        # Consider specific errors, e.g., out-of-memory on GPU\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"   Suggestion: Essayez un mod√®le plus petit ou r√©duisez la charge sur le GPU.\")\n",
        "        return {\"segments\": []}\n",
        "\n",
        "    source_segments = result.get(\"segments\", [])\n",
        "    if not source_segments:\n",
        "        print(\"‚ö†Ô∏è Aucun segment trouv√© dans la transcription.\")\n",
        "        output = {\"segments\": []}\n",
        "        if video_info:\n",
        "            output.update({k: v for k, v in video_info.items() if v is not None})\n",
        "        return output\n",
        "\n",
        "    # Determine total duration more robustly\n",
        "    total_audio_duration = source_segments[-1].get(\"end\") if source_segments else 0\n",
        "    if total_audio_duration <= 0:\n",
        "         print(\"‚ö†Ô∏è Impossible de d√©terminer la dur√©e totale √† partir des segments.\")\n",
        "         # Could try 'result.get(\"duration\")' if available, but segment end is often more reliable\n",
        "\n",
        "    transcript_segments = []\n",
        "    print(f\"Traitement et formatage de {len(source_segments)} segments...\")\n",
        "    for i, segment in enumerate(source_segments):\n",
        "        seg_start = segment.get(\"start\", 0.0)\n",
        "        seg_end = segment.get(\"end\", 0.0)\n",
        "        duration = max(0, seg_end - seg_start)\n",
        "        text = segment.get(\"text\", \"\").strip()\n",
        "\n",
        "        progress_percentage = (seg_end / total_audio_duration) * 100 if total_audio_duration > 0 else 0\n",
        "\n",
        "        transcript_segments.append({\n",
        "            \"id\": i, # Add a simple ID for potential reference\n",
        "            \"text\": text,\n",
        "            \"start\": round(seg_start, 3),\n",
        "            \"end\": round(seg_end, 3), # Adding end time can be useful\n",
        "            \"duration\": round(duration, 3),\n",
        "            \"progress_percentage\": round(progress_percentage, 2)\n",
        "        })\n",
        "        # Log progress less frequently\n",
        "        # if (i + 1) % 50 == 0 or (i + 1) == len(source_segments):\n",
        "        #      print(f\"  Segment {i+1}/{len(source_segments)} format√© ({progress_percentage:.1f}%).\")\n",
        "\n",
        "    output = {\"segments\": transcript_segments}\n",
        "    if video_info:\n",
        "        # Filter out None values from video_info before updating\n",
        "        output_video_info = {k: v for k, v in video_info.items() if v is not None}\n",
        "        output.update(output_video_info)\n",
        "\n",
        "    print(f\"‚úÖ Formatage des segments termin√©.\")\n",
        "    return output\n",
        "\n",
        "\n",
        "# --- Fonction Utilitaires Traduction (Unchanged) ---\n",
        "def chunk_list(lst: list, n: int) -> list:\n",
        "    \"\"\"Divise une liste en sous-listes de taille n.\"\"\"\n",
        "    if not isinstance(lst, list):\n",
        "        raise TypeError(\"L'entr√©e doit √™tre une liste.\")\n",
        "    if n <= 0:\n",
        "        raise ValueError(\"La taille du chunk doit √™tre positive.\")\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i+n]\n",
        "\n",
        "def extract_json_from_response(text_response: str) -> str:\n",
        "    \"\"\"\n",
        "    Tente d'extraire une cha√Æne JSON valide √† partir d'une r√©ponse textuelle,\n",
        "    en g√©rant les blocs de code markdown potentiels (```json ... ```).\n",
        "    \"\"\"\n",
        "    match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', text_response, re.DOTALL)\n",
        "    if match:\n",
        "        # print(\"‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\")\n",
        "        return match.group(1).strip()\n",
        "\n",
        "    # Tentative plus simple : chercher le premier '[' et le dernier ']'\n",
        "    start = text_response.find('[')\n",
        "    end = text_response.rfind(']')\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "         # print(\"‚ÑπÔ∏è JSON d√©tect√© via d√©limiteurs [...] simples.\")\n",
        "         return text_response[start:end+1].strip()\n",
        "\n",
        "    # Fallback : essayer de trouver un objet JSON\n",
        "    start = text_response.find('{')\n",
        "    end = text_response.rfind('}')\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "        # print(\"‚ÑπÔ∏è JSON d√©tect√© via d√©limiteurs {...} simples.\")\n",
        "         return text_response[start:end+1].strip()\n",
        "\n",
        "    # print(\"‚ö†Ô∏è Impossible d'isoler un bloc JSON sp√©cifique. Tentative d'analyse du texte brut.\")\n",
        "    return text_response.strip()\n",
        "\n",
        "\n",
        "# --- Fonction de Traduction de Chunk (pour ex√©cution concurrente) ---\n",
        "def translate_chunk_with_gemini(\n",
        "    transcript_chunk: List[Dict[str, Any]],\n",
        "    api_key: str,\n",
        "    chunk_index: int,\n",
        "    total_chunks: int\n",
        ") -> Optional[List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Sends a transcription chunk to the Gemini API for translation. Designed for concurrency.\n",
        "\n",
        "    Args:\n",
        "        transcript_chunk: A piece (chunk) of the transcription (list of segments).\n",
        "        api_key: Gemini API Key.\n",
        "        chunk_index: Index of the current chunk (for logging).\n",
        "        total_chunks: Total number of chunks (for logging).\n",
        "\n",
        "    Returns:\n",
        "        The chunk modified with translations, or None on failure.\n",
        "    \"\"\"\n",
        "    if not transcript_chunk:\n",
        "        print(f\"üí® [Chunk {chunk_index+1}/{total_chunks}] Vide, ignor√©.\")\n",
        "        return [] # Return empty list for consistency\n",
        "\n",
        "    start_time = time.time()\n",
        "    log_prefix = f\"[Chunk {chunk_index+1:03d}/{total_chunks:03d}]\" # Padded index\n",
        "    print(f\"‚û°Ô∏è {log_prefix} Envoi vers Gemini API ({len(transcript_chunk)} segments)...\")\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}\"\n",
        "\n",
        "    try:\n",
        "        transcript_str = json.dumps(transcript_chunk, ensure_ascii=False, indent=None) # No indent for smaller payload\n",
        "    except TypeError as e:\n",
        "         print(f\"‚ùå {log_prefix} Erreur s√©rialisation JSON du chunk : {e}\")\n",
        "         return None\n",
        "\n",
        "    # Precise prompt\n",
        "    prompt = (\n",
        "        \"You are an expert multilingual translator specializing in transcription segments.\\n\"\n",
        "        \"Input: A JSON array of transcript segments, each with 'text', 'start', 'end', 'duration', 'progress_percentage'.\\n\"\n",
        "        \"Task: For EACH segment in the input array:\\n\"\n",
        "        \"1. Identify the original language of the 'text'.\\n\"\n",
        "        \"2. Translate 'text' into English. Add the result as 'text_english': \\\"<english_translation>\\\".\\n\"\n",
        "        \"3. Translate 'text' into French. Add the result as 'text_french': \\\"<french_translation>\\\".\\n\"\n",
        "        \"4. IMPORTANT: Preserve ALL original keys ('id', 'text', 'start', 'end', 'duration', 'progress_percentage') and their exact values.\\n\"\n",
        "        \"Output Format: Return ONLY the modified JSON array (a valid JSON list of objects). Do NOT include any introductory text, explanations, or markdown formatting (like ```json).\\n\\n\"\n",
        "        \"Input JSON array:\\n\"\n",
        "        f\"{transcript_str}\" # Directly embed the JSON string\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": 0.2, # Lower temp for more deterministic translation\n",
        "            \"maxOutputTokens\": 8192, # Generous limit\n",
        "            \"responseMimeType\": \"application/json\", # Request JSON directly if model supports it\n",
        "        },\n",
        "         \"safetySettings\": [ # Standard safety settings\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n",
        "        ]\n",
        "    }\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    raw_text_response = \"\" # Initialize in case of early error\n",
        "\n",
        "    try:\n",
        "        # Increased timeout for potentially longer API calls\n",
        "        response = requests.post(url, headers=headers, json=payload, timeout=240)\n",
        "\n",
        "        # Check for HTTP errors first\n",
        "        response.raise_for_status()\n",
        "\n",
        "        response_data = response.json()\n",
        "\n",
        "        # --- Gemini API Response Validation ---\n",
        "        if not response_data.get(\"candidates\"):\n",
        "            prompt_feedback = response_data.get(\"promptFeedback\", {})\n",
        "            block_reason = prompt_feedback.get(\"blockReason\")\n",
        "            safety_ratings = prompt_feedback.get(\"safetyRatings\")\n",
        "            error_message = \"Aucun candidat retourn√© par l'API.\"\n",
        "            if block_reason: error_message += f\" Raison blocage: {block_reason}.\"\n",
        "            if safety_ratings: error_message += f\" Safety Ratings: {safety_ratings}\"\n",
        "            print(f\"‚ùå {log_prefix} Erreur API Gemini: {error_message}\")\n",
        "            # print(f\"DEBUG: R√©ponse compl√®te re√ßue: {json.dumps(response_data, indent=2)}\") # Debug\n",
        "            return None\n",
        "\n",
        "        candidate = response_data[\"candidates\"][0]\n",
        "        finish_reason = candidate.get(\"finishReason\")\n",
        "        if finish_reason not in [\"STOP\", \"MAX_TOKENS\"]: # MAX_TOKENS might be ok if JSON is parseable\n",
        "             print(f\"‚ö†Ô∏è {log_prefix} Fin de g√©n√©ration anormale: {finish_reason}.\")\n",
        "             # If blocked by safety, log it\n",
        "             if finish_reason == \"SAFETY\":\n",
        "                 safety_ratings = candidate.get(\"safetyRatings\")\n",
        "                 print(f\"   Safety Ratings: {safety_ratings}\")\n",
        "\n",
        "\n",
        "        if \"content\" not in candidate or \"parts\" not in candidate[\"content\"]:\n",
        "             print(f\"‚ùå {log_prefix} Structure r√©ponse inattendue (manque content/parts).\")\n",
        "             # print(f\"DEBUG: Candidat re√ßu: {json.dumps(candidate, indent=2)}\") # Debug\n",
        "             return None\n",
        "\n",
        "        # Since we requested application/json, the content should ideally be parsed JSON already\n",
        "        # However, Gemini might still wrap it or return text if it fails.\n",
        "        raw_text_response = candidate[\"content\"][\"parts\"][0].get(\"text\", \"\")\n",
        "        if not raw_text_response:\n",
        "             print(f\"‚ùå {log_prefix} R√©ponse de l'API vide.\")\n",
        "             return None\n",
        "\n",
        "        # --- JSON Parsing ---\n",
        "        # Attempt to parse the raw text response directly first\n",
        "        result_json = None\n",
        "        try:\n",
        "            result_json = json.loads(raw_text_response)\n",
        "            # print(f\"‚ÑπÔ∏è {log_prefix} R√©ponse API directement pars√©e comme JSON.\")\n",
        "        except json.JSONDecodeError:\n",
        "            # If direct parsing fails, try extracting from potential markdown/text\n",
        "            # print(f\"‚ÑπÔ∏è {log_prefix} R√©ponse non-JSON direct, tentative d'extraction...\")\n",
        "            json_string = extract_json_from_response(raw_text_response)\n",
        "            try:\n",
        "                result_json = json.loads(json_string)\n",
        "            except json.JSONDecodeError as e_inner:\n",
        "                print(f\"‚ùå {log_prefix} √âchec final de l'analyse JSON : {e_inner}\")\n",
        "                print(f\"--- R√©ponse texte brute re√ßue (max 500 chars) ---\")\n",
        "                print(raw_text_response[:500] + (\"...\" if len(raw_text_response) > 500 else \"\"))\n",
        "                print(\"--- Fin R√©ponse texte brute ---\")\n",
        "                return None\n",
        "\n",
        "        # --- Validation of Parsed JSON ---\n",
        "        if not isinstance(result_json, list):\n",
        "             print(f\"‚ùå {log_prefix} R√©sultat d√©cod√© n'est pas une liste JSON.\")\n",
        "             # print(f\"DEBUG: JSON Pars√©: {result_json}\") # Debug\n",
        "             return None\n",
        "\n",
        "        if len(result_json) != len(transcript_chunk):\n",
        "             print(f\"‚ö†Ô∏è {log_prefix} Taille de retour ({len(result_json)}) != taille d'entr√©e ({len(transcript_chunk)}). Possible troncature ou erreur IA.\")\n",
        "             # Decide how to handle: return None (safer) or return partial result?\n",
        "             # For now, treat as failure to ensure data integrity.\n",
        "             return None\n",
        "\n",
        "        # Optional: Deeper validation (check for expected keys in first element)\n",
        "        if result_json and isinstance(result_json[0], dict):\n",
        "            if 'text_english' not in result_json[0] or 'text_french' not in result_json[0]:\n",
        "                 print(f\"‚ö†Ô∏è {log_prefix} Cl√©s 'text_english'/'text_french' manquantes dans le premier segment traduit.\")\n",
        "                 # Treat as failure? Or proceed with missing data? Let's treat as failure.\n",
        "                 return None\n",
        "        elif not result_json: # Empty list returned for empty input\n",
        "             pass # This is fine if the input chunk was empty\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"‚úÖ {log_prefix} Traduction r√©ussie ({elapsed_time:.2f}s).\")\n",
        "        return result_json\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"‚ùå {log_prefix} Timeout de la requ√™te API ({response.request.url}).\")\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur R√©seau/HTTP : {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e: # Error parsing the initial response from requests\n",
        "        print(f\"‚ùå {log_prefix} Erreur analyse JSON de la r√©ponse HTTP initiale : {e}\")\n",
        "        try:\n",
        "            print(f\"--- R√©ponse HTTP brute (max 500 chars) --- \\n{response.text[:500]}...\\n---\")\n",
        "        except NameError:\n",
        "             print(\" (Impossible d'afficher la r√©ponse HTTP)\") # response might not be defined\n",
        "        return None\n",
        "    except (KeyError, IndexError) as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur acc√®s aux cl√©s de la r√©ponse API : {e}\")\n",
        "        try:\n",
        "            print(f\"--- R√©ponse JSON brute re√ßue ---\")\n",
        "            print(json.dumps(response_data, indent=2, ensure_ascii=False))\n",
        "            print(\"--- Fin R√©ponse JSON brute ---\")\n",
        "        except NameError:\n",
        "             print(\"(Impossible d'afficher les donn√©es JSON de la r√©ponse)\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur inattendue : {e.__class__.__name__}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Fonction d'Orchestration de la Traduction Concurrent ---\n",
        "def translate_transcript_concurrently(\n",
        "    source_segments: List[Dict[str, Any]],\n",
        "    api_key: str,\n",
        "    chunk_size: int,\n",
        "    max_workers: int\n",
        ") -> Tuple[List[Dict[str, Any]], int, int]:\n",
        "    \"\"\"\n",
        "    Translates transcription segments concurrently using ThreadPoolExecutor.\n",
        "\n",
        "    Args:\n",
        "        source_segments: List of segments from Whisper.\n",
        "        api_key: Gemini API Key.\n",
        "        chunk_size: Number of segments per API call.\n",
        "        max_workers: Maximum number of concurrent API calls.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (list_of_translated_segments, successful_chunk_count, failed_chunk_count)\n",
        "    \"\"\"\n",
        "    if not source_segments:\n",
        "        return [], 0, 0\n",
        "\n",
        "    print(f\"\\nüöÄ Lancement de la traduction concurrente ({max_workers} workers max)...\")\n",
        "    all_translated_segments = []\n",
        "    failed_chunks_count = 0\n",
        "    successful_chunks_count = 0\n",
        "\n",
        "    # Create chunks with indices for proper ordering later\n",
        "    chunks = list(chunk_list(source_segments, chunk_size))\n",
        "    total_chunks = len(chunks)\n",
        "    # Store future results mapped by original chunk index\n",
        "    future_to_chunk_index = {}\n",
        "    results = [None] * total_chunks # Pre-allocate results list\n",
        "\n",
        "    start_total_time = time.time()\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit all jobs\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            future = executor.submit(\n",
        "                translate_chunk_with_gemini,\n",
        "                chunk,\n",
        "                api_key,\n",
        "                i, # Pass chunk index\n",
        "                total_chunks\n",
        "            )\n",
        "            future_to_chunk_index[future] = i\n",
        "\n",
        "        # Process completed jobs\n",
        "        for future in concurrent.futures.as_completed(future_to_chunk_index):\n",
        "            chunk_index = future_to_chunk_index[future]\n",
        "            try:\n",
        "                translated_chunk = future.result()\n",
        "                if translated_chunk is not None:\n",
        "                    results[chunk_index] = translated_chunk # Store result in correct position\n",
        "                    successful_chunks_count += 1\n",
        "                else:\n",
        "                    # Failure already logged in translate_chunk_with_gemini\n",
        "                    failed_chunks_count += 1\n",
        "                    # results[chunk_index] remains None\n",
        "            except Exception as exc:\n",
        "                # Catch exceptions raised *during* future.result() if not caught inside the worker\n",
        "                print(f\"‚ùå [Chunk {chunk_index+1:03d}/{total_chunks:03d}] Erreur lors de la r√©cup√©ration du r√©sultat: {exc}\")\n",
        "                failed_chunks_count += 1\n",
        "                # results[chunk_index] remains None\n",
        "\n",
        "    end_total_time = time.time()\n",
        "\n",
        "    # Combine results in the correct order, skipping None entries (failed chunks)\n",
        "    for chunk_result in results:\n",
        "        if chunk_result is not None:\n",
        "            all_translated_segments.extend(chunk_result)\n",
        "\n",
        "    print(\"\\n--- R√©sum√© Traduction Concurrente ---\")\n",
        "    print(f\"Temps total de traduction : {end_total_time - start_total_time:.2f} secondes.\")\n",
        "    print(f\"Chunks trait√©s : {successful_chunks_count}/{total_chunks} succ√®s, {failed_chunks_count}/{total_chunks} √©checs.\")\n",
        "    print(f\"Segments sources : {len(source_segments)}\")\n",
        "    print(f\"Segments traduits (r√©cup√©r√©s) : {len(all_translated_segments)}\")\n",
        "    print(\"-------------------------------------\\n\")\n",
        "\n",
        "    return all_translated_segments, successful_chunks_count, failed_chunks_count\n",
        "\n",
        "# --- Script Principal d'Ex√©cution ---\n",
        "def main():\n",
        "    start_pipeline_time = time.time()\n",
        "\n",
        "    # --- Configuration & Validation ---\n",
        "    youtube_url = \"https://www.youtube.com/watch?v=7q88I_hs3Uw\"#@param {\"type\":\"string\"}\n",
        "    output_dir = \"/content/audio_output\"\n",
        "    output_filename = \"youtube_audio.mp3\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    json_transcript_filename = os.path.splitext(output_path)[0] + \"_transcript.json\"\n",
        "    json_translated_filename = os.path.splitext(output_path)[0] + \"_transcript_translated.json\"\n",
        "\n",
        "    if not GEMINI_API_KEY or GEMINI_API_KEY == \"VOTRE_CLE_API_GEMINI_ICI\":\n",
        "         print(\"üõë ERREUR: Cl√© API Gemini manquante ou invalide.\")\n",
        "         print(\"   Veuillez la d√©finir dans la variable GEMINI_API_KEY en haut du script ou via Colab Secrets.\")\n",
        "         return # Stop execution\n",
        "\n",
        "    if TRANSLATION_CHUNK_SIZE <= 0:\n",
        "        print(f\"‚ö†Ô∏è Taille de chunk de traduction invalide ({TRANSLATION_CHUNK_SIZE}), utilisation de 50.\")\n",
        "        chunk_size = 50\n",
        "    else:\n",
        "        chunk_size = TRANSLATION_CHUNK_SIZE\n",
        "\n",
        "    if MAX_TRANSLATION_WORKERS <= 0:\n",
        "        print(f\"‚ö†Ô∏è Nombre de workers de traduction invalide ({MAX_TRANSLATION_WORKERS}), utilisation de 5.\")\n",
        "        max_workers = 5\n",
        "    else:\n",
        "        max_workers = MAX_TRANSLATION_WORKERS\n",
        "\n",
        "    # --- √âtape 1: T√©l√©chargement ---\n",
        "    print(\"-\" * 30)\n",
        "    print(\"√âTAPE 1: T√âL√âCHARGEMENT AUDIO & METADONN√âES\")\n",
        "    print(\"-\" * 30)\n",
        "    audio_file_path, video_info = download_youtube_audio_improved(youtube_url, output_path)\n",
        "\n",
        "    if not audio_file_path:\n",
        "        print(\"\\n‚ùå √âchec critique: Impossible de t√©l√©charger ou trouver le fichier audio.\")\n",
        "        if video_info:\n",
        "            print(\"   M√©tadonn√©es r√©cup√©r√©es:\", json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "        else:\n",
        "            print(\"   √âchec √©galement de la r√©cup√©ration des m√©tadonn√©es.\")\n",
        "        print(\"Pipeline arr√™t√©.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Informations Vid√©o R√©cup√©r√©es ---\")\n",
        "    print(json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "    print(\"-----------------------------------\\n\")\n",
        "\n",
        "    # --- √âtape 2: Transcription ---\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"√âTAPE 2: TRANSCRIPTION AUDIO (Mod√®le: {WHISPER_MODEL_SIZE})\")\n",
        "    print(\"-\" * 30)\n",
        "    transcript_data = transcribe_audio(audio_file_path, model_size=WHISPER_MODEL_SIZE, video_info=video_info)\n",
        "\n",
        "    if not transcript_data or not transcript_data.get(\"segments\"):\n",
        "        print(\"\\n‚ùå √âchec critique: La transcription n'a retourn√© aucun segment.\")\n",
        "        print(\"Pipeline arr√™t√©.\")\n",
        "        # Optional: Save metadata even if transcription failed?\n",
        "        # final_data = {\"segments\": []}\n",
        "        # if video_info: final_data.update(video_info)\n",
        "        # ... save final_data ...\n",
        "        return\n",
        "\n",
        "    # Sauvegarder la transcription originale (non traduite)\n",
        "    try:\n",
        "        with open(json_transcript_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(transcript_data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"üíæ Transcription originale sauvegard√©e dans : {json_transcript_filename}\")\n",
        "    except IOError as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur lors de la sauvegarde de la transcription originale : {e}\")\n",
        "    except Exception as e:\n",
        "         print(f\"‚ö†Ô∏è Erreur inattendue lors de la sauvegarde JSON (original) : {e}\")\n",
        "\n",
        "\n",
        "    source_segments = transcript_data.get(\"segments\", []) # Should exist based on check above\n",
        "\n",
        "    # --- √âtape 3: Traduction Concurrente ---\n",
        "    print(\"-\" * 30)\n",
        "    print(\"√âTAPE 3: TRADUCTION DES SEGMENTS\")\n",
        "    print(\"-\" * 30)\n",
        "    translated_segments, successful_chunks, failed_chunks = translate_transcript_concurrently(\n",
        "        source_segments,\n",
        "        GEMINI_API_KEY,\n",
        "        chunk_size=chunk_size,\n",
        "        max_workers=max_workers\n",
        "    )\n",
        "\n",
        "    # --- Assemblage Final et Sauvegarde ---\n",
        "    final_translated_data = transcript_data.copy() # Conserve les m√©tadonn√©es\n",
        "    final_translated_data[\"segments\"] = translated_segments # Remplace par les segments traduits\n",
        "\n",
        "    if failed_chunks > 0:\n",
        "        print(f\"‚ö†Ô∏è {failed_chunks} chunks de traduction ont √©chou√©. Le r√©sultat final peut √™tre incomplet.\")\n",
        "    elif not translated_segments and source_segments:\n",
        "         print(f\"‚ö†Ô∏è La traduction n'a retourn√© aucun segment alors que la source en contenait. V√©rifiez les logs API.\")\n",
        "    else:\n",
        "        print(\"‚úÖ Traduction termin√©e.\")\n",
        "\n",
        "\n",
        "    # Sauvegarder le r√©sultat final traduit\n",
        "    try:\n",
        "        with open(json_translated_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(final_translated_data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"üíæ Transcription traduite sauvegard√©e dans : {json_translated_filename}\")\n",
        "    except IOError as e:\n",
        "        print(f\"‚ùå Erreur lors de la sauvegarde du fichier JSON traduit : {e}\")\n",
        "    except TypeError as e:\n",
        "         print(f\"‚ùå Erreur de Type lors de la s√©rialisation finale en JSON : {e}\")\n",
        "         print(\"   Cela peut arriver si des donn√©es non s√©rialisables sont dans les r√©sultats.\")\n",
        "    except Exception as e:\n",
        "         print(f\"‚ùå Erreur inattendue lors de la sauvegarde JSON (traduit) : {e}\")\n",
        "\n",
        "\n",
        "    # --- Nettoyage Optionnel ---\n",
        "    # D√©commentez pour supprimer le fichier audio apr√®s traitement\n",
        "    # print(\"\\n--- Nettoyage ---\")\n",
        "    # if os.path.exists(audio_file_path):\n",
        "    #     try:\n",
        "    #         os.remove(audio_file_path)\n",
        "    #         print(f\"üóëÔ∏è Fichier audio supprim√© : {audio_file_path}\")\n",
        "    #     except OSError as e:\n",
        "    #         print(f\"‚ùå Erreur suppression fichier audio {audio_file_path}: {e}\")\n",
        "    # else:\n",
        "    #     print(f\"‚ÑπÔ∏è Fichier audio {audio_file_path} non trouv√© pour suppression.\")\n",
        "\n",
        "    end_pipeline_time = time.time()\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"üèÅ Pipeline Termin√© en {end_pipeline_time - start_pipeline_time:.2f} secondes.\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure prerequisite libraries are installed (useful in environments like Colab)\n",
        "    try:\n",
        "        import yt_dlp\n",
        "    except ImportError:\n",
        "        print(\"Tentative d'installation de yt-dlp...\")\n",
        "        subprocess.run(['pip', 'install', '-q', 'yt-dlp'])\n",
        "        print(\"yt-dlp install√©.\")\n",
        "\n",
        "    try:\n",
        "        import whisper\n",
        "    except ImportError:\n",
        "        print(\"Tentative d'installation de openai-whisper...\")\n",
        "        # Note: Whisper installation can be heavy. Consider git+https for latest version if needed.\n",
        "        subprocess.run(['pip', 'install', '-q', 'openai-whisper'])\n",
        "        print(\"openai-whisper install√©.\")\n",
        "        # May need ffmpeg too\n",
        "        try:\n",
        "             subprocess.run(['ffmpeg', '-version'], check=True, capture_output=True)\n",
        "        except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "             print(\"ffmpeg non trouv√© ou ne fonctionne pas. Installation n√©cessaire (apt-get install ffmpeg sous Debian/Ubuntu).\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        import requests\n",
        "    except ImportError:\n",
        "         print(\"Tentative d'installation de requests...\")\n",
        "         subprocess.run(['pip', 'install', '-q', 'requests'])\n",
        "         print(\"requests install√©.\")\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "    except ImportError:\n",
        "         print(\"Tentative d'installation de torch...\")\n",
        "         # Installation can vary depending on CUDA version. Provide a common one.\n",
        "         subprocess.run(['pip', 'install', '-q', 'torch'])\n",
        "         print(\"torch install√©.\")\n",
        "\n",
        "\n",
        "    # Run the main processing pipeline\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61on6U-xAlrX",
        "collapsed": true,
        "outputId": "1ea5f944-825f-42ec-cd6b-d42789caee44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request successful!\n",
            "Status code: 201\n",
            "Response body: {'id': 15, 'message': 'Transcript created successfully'}\n",
            "https://qingplay.pythonanywhere.com/vid/GRLdsdBDjE4\n"
          ]
        }
      ],
      "source": [
        "# @title # **Upload**\n",
        "\n",
        "url = \"https://qingplay.pythonanywhere.com/upload_transcripts\"  #@param {\"type\":\"string\"}\n",
        "\n",
        "# Prepare the data to be sent as JSON\n",
        "payload = {\n",
        "    \"video_id\": translated_transcript_data[\"video_id\"],\n",
        "    \"description\": translated_transcript_data[\"description\"],\n",
        "    \"channel_name\": translated_transcript_data[\"channel_name\"],\n",
        "    \"channel_url\": translated_transcript_data[\"channel_url\"],\n",
        "    \"title\": translated_transcript_data[\"title\"],\n",
        "    \"segments\": translated_transcript_data[\"segments\"]\n",
        "}\n",
        "\n",
        "# Send the POST request\n",
        "try:\n",
        "    response = requests.post(url, json=payload)  # Use json= to send JSON data\n",
        "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(\"Request successful!\")\n",
        "    print(\"Status code:\", response.status_code)\n",
        "    print(\"Response body:\", response.json())\n",
        "    print(\"https://qingplay.pythonanywhere.com/vid/\"+translated_transcript_data[\"video_id\"])\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Request failed: {e}\")\n",
        "    if response is not None:  # Print the error returned by the server.\n",
        "        print(f\"Response text: {response.text}\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Failed to decode JSON: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvSHsfCquSRV"
      },
      "outputs": [],
      "source": [
        "from flask import Flask\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "\n",
        "@app.route(\"/\")\n",
        "def hello_world():\n",
        "  return \"<p>Hello, World!</p>\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r audio_output"
      ],
      "metadata": {
        "id": "vxqAYnDpNE6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # **Transcription & Translation (Optimized with Streaming & Rate Limiting) - HARDCODED API KEY VERSION**\n",
        "\n",
        "# --- Core Python Libraries ---\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import asyncio # For async operations\n",
        "import concurrent.futures # Keep for potential future use, but not core here\n",
        "\n",
        "# --- Third-party Libraries ---\n",
        "import requests # For synchronous checks if needed later\n",
        "import torch # For GPU check\n",
        "import aiohttp # For async HTTP requests to Gemini\n",
        "from aiolimiter import AsyncLimiter # For precise rate limiting\n",
        "import nest_asyncio # For Colab/Jupyter\n",
        "\n",
        "# --- Typing ---\n",
        "from typing import List, Dict, Tuple, Optional, Any, AsyncGenerator, Set\n",
        "\n",
        "# --- Apply nest_asyncio ---\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Whisper Implementation ---\n",
        "# Separate the check/install logic from the actual import used later\n",
        "\n",
        "try:\n",
        "    # Just check if it *can* be imported for the initial message\n",
        "    import faster_whisper\n",
        "    print(\"‚úÖ faster-whisper est d√©j√† install√©.\")\n",
        "    # We will import specific names later, outside the try/except\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Importation de faster-whisper √©chou√©e. Tentative d'installation...\")\n",
        "    print(\"‚ÄºÔ∏è IMPORTANT: Si cela √©choue, ex√©cutez cette commande dans une cellule Colab s√©par√©e AVANT ce script:\")\n",
        "    print(\"   !pip install -q faster-whisper ctranslate2>=3.10.0,<4.0.0\")\n",
        "    try:\n",
        "        # Attempt installation\n",
        "        subprocess.run(['pip', 'install', '-q', 'faster-whisper', 'ctranslate2>=3.10.0,<4.0.0'], check=True)\n",
        "        import faster_whisper # Verify again after install\n",
        "        print(\"‚úÖ faster-whisper install√© avec succ√®s apr√®s tentative.\")\n",
        "    except (ImportError, subprocess.CalledProcessError) as install_err:\n",
        "        print(f\"‚ùå √âchec critique de l'installation/importation de faster-whisper : {install_err}\")\n",
        "        print(\"   Assurez-vous d'avoir ex√©cut√© la commande pip manuellement dans une autre cellule.\")\n",
        "        print(\"   V√©rifiez la compatibilit√© CTranslate2 (CPU/GPU, CUDA).\")\n",
        "        exit(1) # Cannot proceed\n",
        "\n",
        "# --- NOW, import the necessary components ---\n",
        "# This assumes the above block succeeded or exited.\n",
        "try:\n",
        "    from faster_whisper import WhisperModel, AutoModel\n",
        "    print(\"‚úÖ Noms requis (WhisperModel, AutoModel) import√©s depuis faster-whisper.\")\n",
        "except ImportError as e:\n",
        "     print(f\"‚ùå Erreur finale lors de l'importation de WhisperModel/AutoModel: {e}\")\n",
        "     print(\"   M√™me si l'installation a sembl√© r√©ussir, l'importation sp√©cifique a √©chou√©.\")\n",
        "     print(\"   Essayez de red√©marrer l'environnement d'ex√©cution et r√©installez.\")\n",
        "     exit(1)\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "WHISPER_MODEL_SIZE = \"medium\" # @param [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\", \"large-v3\"]\n",
        "WHISPER_COMPUTE_TYPE = \"float16\" # @param [\"default\", \"float16\", \"int8\", \"int8_float16\"]\n",
        "\n",
        "TRANSLATION_CHUNK_SIZE = 30 # @param {type:\"integer\"} # Segments per API call\n",
        "MAX_CONCURRENT_TRANSLATIONS = 5 # @param {type:\"integer\"} # Max simultaneous API calls\n",
        "GEMINI_RATE_LIMIT_PER_MINUTE = 15 # @param {type:\"integer\"} # Based on Gemini Free Tier (adjust if needed)\n",
        "\n",
        "# --- Gemini API Key (HARDCODED) ---\n",
        "GEMINI_API_KEY = \"AIzaSyBiZONd6VA8y9zAd8vueZRo_IrPnn7iHlw\" # <--- REMPLACEZ CECI\n",
        "\n",
        "if GEMINI_API_KEY == \"AIzaSyBiZONd6VA8y9zAd8vueZRo_IrPnn7iHlw\":\n",
        "    print(\"üõë ALERTE: Vous n'avez pas remplac√© 'VOTRE_CLE_API_GEMINI_ICI' par votre cl√© API r√©elle.\")\n",
        "    # exit(1) # Optional: Stop execution\n",
        "\n",
        "\n",
        "# --- Global Variables / State ---\n",
        "gemini_rate_limiter = AsyncLimiter(GEMINI_RATE_LIMIT_PER_MINUTE, 60)\n",
        "translation_semaphore = asyncio.Semaphore(MAX_CONCURRENT_TRANSLATIONS)\n",
        "\n",
        "\n",
        "# --- Fonction download_youtube_audio_improved ---\n",
        "# ... (keep function as is) ...\n",
        "def download_youtube_audio_improved(youtube_url: str, output_path: str) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Downloads YouTube audio and reliably extracts metadata using separate yt-dlp calls.\n",
        "\n",
        "    Args:\n",
        "        youtube_url: The URL of the YouTube video.\n",
        "        output_path: The desired path to save the MP3 audio file.\n",
        "\n",
        "    Returns:\n",
        "        A tuple (audio_file_path, video_info_dict).\n",
        "        Returns (None, video_info) if audio download fails but metadata is retrieved.\n",
        "        Returns (None, None) if metadata retrieval fails.\n",
        "    \"\"\"\n",
        "    video_info = None\n",
        "    audio_file_path = None\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    if not os.path.exists(output_dir):\n",
        "        try:\n",
        "            os.makedirs(output_dir)\n",
        "            print(f\"üìÅ Cr√©ation du r√©pertoire de sortie : {output_dir}\")\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Erreur lors de la cr√©ation du r√©pertoire {output_dir}: {e}\")\n",
        "            return None, None # Cannot proceed without output directory\n",
        "\n",
        "    # --- Step 1: Get Metadata ---\n",
        "    print(\"‚ÑπÔ∏è  R√©cup√©ration des m√©tadonn√©es de la vid√©o...\")\n",
        "    try:\n",
        "        metadata_command = [\n",
        "            \"yt-dlp\", \"--dump-json\", \"--encoding\", \"utf-8\", youtube_url,\n",
        "        ]\n",
        "        metadata_result = subprocess.run(metadata_command, check=True, capture_output=True, text=True, encoding='utf-8', errors='replace', timeout=120)\n",
        "        metadata = json.loads(metadata_result.stdout)\n",
        "\n",
        "        video_info = {\n",
        "            \"video_id\": metadata.get(\"id\", \"N/A\"),\n",
        "            \"channel_name\": metadata.get(\"uploader\", \"N/A\"),\n",
        "            \"channel_url\": metadata.get(\"uploader_url\", \"N/A\"),\n",
        "            \"title\": metadata.get(\"title\", \"N/A\"),\n",
        "            \"description\": metadata.get(\"description\", \"N/A\"),\n",
        "            \"duration\": metadata.get(\"duration\"), # Keep duration if available\n",
        "            \"upload_date\": metadata.get(\"upload_date\"), # Keep upload date\n",
        "            \"thumbnail\": metadata.get(\"thumbnail\"), # Keep thumbnail URL\n",
        "        }\n",
        "        if video_info[\"video_id\"] == \"N/A\":\n",
        "             video_id_match = re.search(r\"v=([a-zA-Z0-9_-]+)\", youtube_url)\n",
        "             video_info[\"video_id\"] = video_id_match.group(1) if video_id_match else \"UNKNOWN\"\n",
        "             print(f\"‚ö†Ô∏è ID vid√©o non trouv√© via yt-dlp, fallback regex: {video_info['video_id']}\")\n",
        "\n",
        "        print(\"‚úÖ M√©tadonn√©es r√©cup√©r√©es.\")\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(f\"‚ùå Timeout lors de la r√©cup√©ration des m√©tadonn√©es yt-dlp.\")\n",
        "        return None, None\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Erreur yt-dlp (m√©tadonn√©es) (Code: {e.returncode}).\")\n",
        "        print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "        error_output = e.stderr.strip() if e.stderr else \"(no stderr)\"\n",
        "        print(f\"   Erreur: {error_output}\")\n",
        "        return None, None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Erreur analyse JSON m√©tadonn√©es yt-dlp : {e}\")\n",
        "        print(f\"--- Sortie brute yt-dlp ---\\n{metadata_result.stdout[:500]}...\\n---\")\n",
        "        return None, None\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå yt-dlp non trouv√©. Installez avec: pip install yt-dlp\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue (m√©tadonn√©es) : {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # --- Step 2: Download Audio ---\n",
        "    print(f\"üîÑ T√©l√©chargement/V√©rification audio vers {output_path}...\")\n",
        "    try:\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "             print(f\"‚ÑπÔ∏è Fichier audio '{output_path}' existe d√©j√† et n'est pas vide. Utilisation du fichier existant.\")\n",
        "             audio_file_path = output_path\n",
        "        else:\n",
        "            if os.path.exists(output_path):\n",
        "                 print(f\"‚ÑπÔ∏è Fichier audio '{output_path}' existe mais est vide. Tentative de t√©l√©chargement.\")\n",
        "\n",
        "            download_command = [\n",
        "                \"yt-dlp\", \"-x\", \"--audio-format\", \"mp3\",\n",
        "                \"-o\", output_path,\n",
        "                \"--encoding\", \"utf-8\",\n",
        "                youtube_url,\n",
        "            ]\n",
        "            download_result = subprocess.run(download_command, check=True, capture_output=True, text=True, encoding='utf-8', errors='replace', timeout=600) # 10 min timeout\n",
        "\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f\"‚úÖ Fichier audio t√©l√©charg√©/v√©rifi√© : {output_path}\")\n",
        "                audio_file_path = output_path\n",
        "            else:\n",
        "                print(f\"‚ùå ERREUR: yt-dlp termin√© sans erreur mais fichier '{output_path}' introuvable ou vide apr√®s tentative.\")\n",
        "                print(f\"--- Sortie yt-dlp (stdout) ---\\n{download_result.stdout.strip()}\")\n",
        "                print(f\"--- Sortie yt-dlp (stderr) ---\\n{download_result.stderr.strip()}\")\n",
        "                return None, video_info\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "         print(f\"‚ùå Timeout lors du t√©l√©chargement audio yt-dlp.\")\n",
        "         if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "             print(f\"‚ö†Ô∏è Fichier partiel '{output_path}' existe malgr√© le timeout. Suppression...\")\n",
        "             try: os.remove(output_path)\n",
        "             except OSError: pass\n",
        "         return None, video_info\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Erreur yt-dlp (t√©l√©chargement) (Code: {e.returncode}).\")\n",
        "        print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "        error_output = e.stderr.strip() if e.stderr else \"(no stderr)\"\n",
        "        print(f\"   Erreur: {error_output}\")\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "             print(f\"‚ö†Ô∏è Fichier '{output_path}' existe malgr√© l'erreur yt-dlp. Utilisation prudente.\")\n",
        "             audio_file_path = output_path\n",
        "        else:\n",
        "             return None, video_info\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå yt-dlp non trouv√©. Installez avec: pip install yt-dlp\")\n",
        "        return None, video_info\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue (t√©l√©chargement) : {e}\")\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "            print(f\"‚ö†Ô∏è Utilisation du fichier audio existant '{output_path}' malgr√© l'erreur inattendue.\")\n",
        "            audio_file_path = output_path\n",
        "        else:\n",
        "            return None, video_info\n",
        "\n",
        "    if audio_file_path and (not os.path.exists(audio_file_path) or os.path.getsize(audio_file_path) == 0):\n",
        "        print(f\"‚ùå ERREUR FINALE: Chemin audio '{audio_file_path}' retourn√© mais fichier introuvable ou vide.\")\n",
        "        return None, video_info\n",
        "\n",
        "    return audio_file_path, video_info\n",
        "\n",
        "# --- Fonction Utilitaires ---\n",
        "# ... (keep function as is) ...\n",
        "def extract_json_from_response(text_response: str) -> str:\n",
        "    \"\"\"\n",
        "    Tente d'extraire une cha√Æne JSON valide √† partir d'une r√©ponse textuelle,\n",
        "    en g√©rant les blocs de code markdown potentiels (```json ... ```).\n",
        "    \"\"\"\n",
        "    match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', text_response, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    start = text_response.find('[')\n",
        "    end = text_response.rfind(']')\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "         return text_response[start:end+1].strip()\n",
        "    start = text_response.find('{')\n",
        "    end = text_response.rfind('}')\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "         return text_response[start:end+1].strip()\n",
        "    return text_response.strip()\n",
        "\n",
        "\n",
        "# --- Fonction de Traduction Async ---\n",
        "# ... (keep function as is) ...\n",
        "async def translate_chunk_gemini_async(\n",
        "    session: aiohttp.ClientSession,\n",
        "    transcript_chunk: List[Dict[str, Any]],\n",
        "    api_key: str, # La cl√© est pass√©e ici\n",
        "    chunk_index: int, # For logging\n",
        "    total_segments_in_chunk: int # For logging\n",
        ") -> Optional[List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Sends a transcription chunk to the Gemini API for translation asynchronously.\n",
        "    Designed to be called by concurrent workers. Returns translated chunk or None on failure.\n",
        "    \"\"\"\n",
        "    if not transcript_chunk:\n",
        "        # print(f\"üí® [Chunk {chunk_index+1:03d}] Vide, ignor√©.\") # Too verbose\n",
        "        return []\n",
        "\n",
        "    start_time = time.time()\n",
        "    log_prefix = f\"[Chunk {chunk_index+1:03d}]\" # Padded index\n",
        "\n",
        "    # V√©rification de la cl√© API avant de faire l'appel\n",
        "    if not api_key or api_key == \"VOTRE_CLE_API_GEMINI_ICI\":\n",
        "        print(f\"‚ùå {log_prefix} Cl√© API Gemini manquante ou invalide. Impossible de traduire.\")\n",
        "        return None # √âchec du chunk\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}\"\n",
        "\n",
        "    try:\n",
        "        transcript_str = json.dumps(transcript_chunk, ensure_ascii=False, indent=None)\n",
        "    except TypeError as e:\n",
        "         print(f\"‚ùå {log_prefix} Erreur s√©rialisation JSON du chunk : {e}\")\n",
        "         return None\n",
        "\n",
        "    prompt = (\n",
        "        \"You are an expert multilingual translator specializing in transcription segments.\\n\"\n",
        "        \"Input: A JSON array of transcript segments, each with 'id', 'text', 'start', 'end', 'duration', 'progress_percentage'.\\n\"\n",
        "        \"Task: For EACH segment in the input array:\\n\"\n",
        "        \"1. Identify the original language of the 'text'.\\n\"\n",
        "        \"2. Translate 'text' into English. Add the result as 'text_english': \\\"<english_translation>\\\".\\n\"\n",
        "        \"3. Translate 'text' into French. Add the result as 'text_french': \\\"<french_translation>\\\".\\n\"\n",
        "        \"4. IMPORTANT: Preserve ALL original keys ('id', 'text', 'start', 'end', 'duration', 'progress_percentage') and their exact values.\\n\"\n",
        "        \"Output Format: Return ONLY the modified JSON array (a valid JSON list of objects). Do NOT include any introductory text, explanations, or markdown formatting (like ```json).\\n\\n\"\n",
        "        \"Input JSON array:\\n\"\n",
        "        f\"{transcript_str}\"\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": 0.2,\n",
        "            \"maxOutputTokens\": 8192,\n",
        "            \"responseMimeType\": \"application/json\", # Explicitly request JSON\n",
        "        },\n",
        "         \"safetySettings\": [ # Standard safety settings\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n",
        "        ]\n",
        "    }\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    response_text = \"\"\n",
        "    response_json = None\n",
        "\n",
        "    try:\n",
        "        async with gemini_rate_limiter: # Wait if rate limit exceeded\n",
        "             async with translation_semaphore: # Wait if max concurrent requests reached\n",
        "                # print(f\"üîí {log_prefix} Rate limit/semaphore acquis. Envoi...\") # Verbose\n",
        "                req_start_time = time.time()\n",
        "                async with session.post(url, headers=headers, json=payload, timeout=aiohttp.ClientTimeout(total=240)) as response:\n",
        "                    req_duration = time.time() - req_start_time\n",
        "                    # print(f\"‚òÅÔ∏è {log_prefix} R√©ponse re√ßue ({response.status}) en {req_duration:.2f}s.\") # Verbose\n",
        "\n",
        "                    response_text = await response.text()\n",
        "                    response.raise_for_status() # Raise HTTP errors (4xx, 5xx)\n",
        "                    response_json = await response.json(content_type=None) # Allow flexible content type\n",
        "\n",
        "        # --- Process Response ---\n",
        "        if not response_json:\n",
        "             print(f\"‚ùå {log_prefix} R√©ponse JSON vide ou invalide re√ßue.\")\n",
        "             print(f\"   R√©ponse texte brute: {response_text[:200]}...\")\n",
        "             return None\n",
        "\n",
        "        if not response_json.get(\"candidates\"):\n",
        "            prompt_feedback = response_json.get(\"promptFeedback\", {})\n",
        "            block_reason = prompt_feedback.get(\"blockReason\")\n",
        "            safety_ratings = prompt_feedback.get(\"safetyRatings\")\n",
        "            error_message = \"Aucun candidat retourn√© par l'API.\"\n",
        "            if block_reason: error_message += f\" Raison blocage: {block_reason}.\"\n",
        "            if safety_ratings: error_message += f\" Safety Ratings: {safety_ratings}\"\n",
        "            print(f\"‚ùå {log_prefix} Erreur API Gemini: {error_message}\")\n",
        "            # print(f\"DEBUG: R√©ponse compl√®te: {json.dumps(response_json, indent=2)}\") # Debug only\n",
        "            return None\n",
        "\n",
        "        candidate = response_json[\"candidates\"][0]\n",
        "        finish_reason = candidate.get(\"finishReason\")\n",
        "        if finish_reason not in [\"STOP\", \"MAX_TOKENS\"]:\n",
        "             print(f\"‚ö†Ô∏è {log_prefix} Fin de g√©n√©ration anormale: {finish_reason}.\")\n",
        "             if finish_reason == \"SAFETY\":\n",
        "                 safety_ratings = candidate.get(\"safetyRatings\")\n",
        "                 print(f\"   Safety Ratings: {safety_ratings}\")\n",
        "\n",
        "        if \"content\" not in candidate or \"parts\" not in candidate[\"content\"]:\n",
        "             print(f\"‚ùå {log_prefix} Structure r√©ponse inattendue (manque content/parts).\")\n",
        "             # print(f\"DEBUG: Candidat re√ßu: {json.dumps(candidate, indent=2)}\") # Debug only\n",
        "             return None\n",
        "\n",
        "        raw_llm_output_text = candidate[\"content\"][\"parts\"][0].get(\"text\", \"\")\n",
        "        if not raw_llm_output_text:\n",
        "             print(f\"‚ùå {log_prefix} Contenu de la r√©ponse API vide.\")\n",
        "             return None\n",
        "\n",
        "        # --- JSON Parsing from LLM output ---\n",
        "        parsed_result_json = None\n",
        "        try:\n",
        "            # Try direct parsing first (due to responseMimeType)\n",
        "            parsed_result_json = json.loads(raw_llm_output_text)\n",
        "        except json.JSONDecodeError:\n",
        "            # print(f\"‚ÑπÔ∏è {log_prefix} R√©ponse non-JSON direct, tentative d'extraction...\") # Verbose\n",
        "            json_string = extract_json_from_response(raw_llm_output_text)\n",
        "            try:\n",
        "                parsed_result_json = json.loads(json_string)\n",
        "            except json.JSONDecodeError as e_inner:\n",
        "                print(f\"‚ùå {log_prefix} √âchec final de l'analyse JSON du contenu LLM : {e_inner}\")\n",
        "                print(f\"--- Contenu texte brut LLM re√ßu (max 500 chars) ---\\n{raw_llm_output_text[:500]}...\\n---\")\n",
        "                return None\n",
        "\n",
        "        # --- Validation of Parsed JSON ---\n",
        "        if not isinstance(parsed_result_json, list):\n",
        "             print(f\"‚ùå {log_prefix} R√©sultat d√©cod√© du LLM n'est pas une liste JSON.\")\n",
        "             # print(f\"DEBUG: JSON Pars√©: {parsed_result_json}\") # Debug only\n",
        "             return None\n",
        "\n",
        "        if len(parsed_result_json) != total_segments_in_chunk:\n",
        "             print(f\"‚ö†Ô∏è {log_prefix} Taille de retour LLM ({len(parsed_result_json)}) != taille d'entr√©e ({total_segments_in_chunk}). Possible troncature ou erreur IA. Chunk √©chou√©.\")\n",
        "             return None # Treat as failure\n",
        "\n",
        "        # Deeper validation (optional but good)\n",
        "        if parsed_result_json: # If list is not empty\n",
        "            first_segment = parsed_result_json[0]\n",
        "            if not isinstance(first_segment, dict) or \\\n",
        "               'text_english' not in first_segment or \\\n",
        "               'text_french' not in first_segment or \\\n",
        "               'id' not in first_segment: # Check essential keys\n",
        "                 print(f\"‚ö†Ô∏è {log_prefix} Cl√©s essentielles manquantes ('id', 'text_english', 'text_french') dans le premier segment traduit. Chunk √©chou√©.\")\n",
        "                 return None\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"‚úÖ {log_prefix} Traduction r√©ussie ({total_segments_in_chunk} segments, {elapsed_time:.2f}s total).\")\n",
        "        return parsed_result_json\n",
        "\n",
        "    except aiohttp.ClientResponseError as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur HTTP {e.status} de l'API Gemini : {e.message}\")\n",
        "        print(f\"   Headers: {e.headers}\")\n",
        "        print(f\"   R√©ponse Texte (max 500): {response_text[:500]}...\")\n",
        "        # Si l'erreur est 400 Bad Request, et contient \"API key not valid\"\n",
        "        if e.status == 400 and \"api key not valid\" in response_text.lower():\n",
        "             print(f\"   ‚ÄºÔ∏è ERREUR SP√âCIFIQUE: La cl√© API '{api_key[:4]}...{api_key[-4:]}' semble invalide. V√©rifiez la cl√© dans le code.\")\n",
        "        elif e.status == 429:\n",
        "             print(f\"   ‚ÄºÔ∏è ERREUR SP√âCIFIQUE: Rate limit (Quota) d√©pass√©. R√©duisez GEMINI_RATE_LIMIT_PER_MINUTE ou MAX_CONCURRENT_TRANSLATIONS.\")\n",
        "\n",
        "        return None\n",
        "    except asyncio.TimeoutError:\n",
        "        print(f\"‚ùå {log_prefix} Timeout de la requ√™te API Gemini ({url}).\")\n",
        "        return None\n",
        "    except aiohttp.ClientError as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur Client aiohttp : {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur analyse JSON de la r√©ponse principale: {e}\")\n",
        "        if response_text: print(f\"--- R√©ponse texte brute (max 500 chars) ---\\n{response_text[:500]}...\\n---\")\n",
        "        return None\n",
        "    except (KeyError, IndexError) as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur acc√®s aux cl√©s/index de la r√©ponse API : {e}\")\n",
        "        if response_json: print(f\"--- R√©ponse JSON brute re√ßue ---\\n{json.dumps(response_json, indent=2, ensure_ascii=False)}\\n---\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur inattendue dans translate_chunk : {e.__class__.__name__}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Fonction d'Orchestration Async ---\n",
        "# ... (keep function as is) ...\n",
        "async def process_audio_pipeline_async(\n",
        "    audio_file_path: str,\n",
        "    whisper_model_size: str,\n",
        "    whisper_compute_type: str,\n",
        "    api_key: str, # La cl√© est pass√©e ici\n",
        "    chunk_size: int,\n",
        "    num_workers_to_start: int, # Explicitly pass worker count\n",
        "    video_info: Optional[Dict[str, Any]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Asynchronous pipeline to transcribe and translate audio.\n",
        "    \"\"\"\n",
        "    start_pipeline_time = time.time()\n",
        "    print(\"\\nüöÄ D√©marrage du pipeline asynchrone...\")\n",
        "\n",
        "    # --- V√©rification cl√© API au d√©but du pipeline ---\n",
        "    if not api_key or api_key == \"VOTRE_CLE_API_GEMINI_ICI\":\n",
        "        print(\"‚ùå ERREUR PIPELINE: Cl√© API Gemini non valide ou non d√©finie dans le code.\")\n",
        "        return {\"segments\": [], **(video_info or {})} # Return empty data\n",
        "\n",
        "    # --- Load Whisper Model ---\n",
        "    start_load_time = time.time()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if whisper_compute_type == \"default\":\n",
        "        compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "    else:\n",
        "        compute_type = whisper_compute_type\n",
        "\n",
        "    print(f\"üîÑ Chargement du mod√®le faster-whisper '{whisper_model_size}' (compute_type={compute_type}) sur '{device}'...\")\n",
        "    try:\n",
        "        # >>> AutoModel SHOULD BE DEFINED HERE <<<\n",
        "        # Check if it exists just before using it for debugging\n",
        "        # if 'AutoModel' not in globals() and 'AutoModel' not in locals():\n",
        "        #      print(\"DEBUG: AutoModel is STILL not defined right before usage!\")\n",
        "        #      # You might want to raise an error here or handle it\n",
        "        # else:\n",
        "        #      print(\"DEBUG: AutoModel appears to be defined now.\")\n",
        "\n",
        "        # This is the line that caused the original error\n",
        "        model = AutoModel.from_pretrained(whisper_model_size, device=device, compute_type=compute_type)\n",
        "        load_time = time.time() - start_load_time\n",
        "        print(f\"‚úÖ Mod√®le charg√© en {load_time:.2f} secondes.\")\n",
        "    except NameError as ne:\n",
        "         # Catch the specific error if it still happens\n",
        "         print(f\"‚ùå ERREUR CRITIQUE (NameError): {ne}. 'AutoModel' n'est toujours pas d√©fini.\")\n",
        "         print(\"   Cela indique un probl√®me persistant avec l'importation de faster_whisper.\")\n",
        "         print(\"   V√©rifiez l'installation manuelle et l'environnement.\")\n",
        "         return {\"segments\": [], **(video_info or {})}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur critique lors du chargement du mod√®le faster-whisper: {e}\")\n",
        "        if \"ctranslate2\" in str(e).lower():\n",
        "            print(\"   Suggestion: Assurez-vous que CTranslate2 est install√© et compatible.\")\n",
        "        # Print more details for other errors\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {\"segments\": [], **(video_info or {})} # Return empty data\n",
        "\n",
        "    # --- Setup Async Components ---\n",
        "    translation_tasks: Set[asyncio.Task] = set()\n",
        "    all_translated_segments_list: List[Dict[str, Any]] = []\n",
        "    chunk_queue = asyncio.Queue(maxsize=num_workers_to_start * 2) # Bounded queue\n",
        "    processed_chunk_count = 0\n",
        "    failed_chunk_count = 0\n",
        "    total_segments_yielded = 0\n",
        "    current_chunk_index = 0\n",
        "\n",
        "    # --- Create aiohttp Session ---\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "\n",
        "        # --- Define the Translation Worker ---\n",
        "        async def translation_worker(worker_id: int):\n",
        "            nonlocal processed_chunk_count, failed_chunk_count\n",
        "            print(f\"üë∑ [Worker-{worker_id}] D√©marr√©.\")\n",
        "            while True:\n",
        "                try:\n",
        "                    chunk_data = await chunk_queue.get()\n",
        "                    if chunk_data is None: # Sentinel\n",
        "                        chunk_queue.task_done()\n",
        "                        break\n",
        "\n",
        "                    chunk_idx, segment_chunk = chunk_data\n",
        "                    log_prefix_worker = f\"[Worker-{worker_id} | Chunk {chunk_idx+1:03d}]\"\n",
        "\n",
        "                    translated_chunk = await translate_chunk_gemini_async(\n",
        "                        session, segment_chunk, api_key, chunk_idx, len(segment_chunk)\n",
        "                    )\n",
        "\n",
        "                    if translated_chunk is not None:\n",
        "                        all_translated_segments_list.extend(translated_chunk)\n",
        "                        processed_chunk_count += 1\n",
        "                    else:\n",
        "                        print(f\"üëé {log_prefix_worker} √âchec de la traduction.\")\n",
        "                        failed_chunk_count += 1\n",
        "\n",
        "                    chunk_queue.task_done()\n",
        "\n",
        "                except asyncio.CancelledError:\n",
        "                    print(f\"üö´ [Worker-{worker_id}] T√¢che annul√©e.\")\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå [Worker-{worker_id}] Erreur inattendue : {e}\")\n",
        "                    try: chunk_queue.task_done()\n",
        "                    except ValueError: pass\n",
        "                    await asyncio.sleep(1)\n",
        "\n",
        "            print(f\"üèÅ [Worker-{worker_id}] Termin√©.\")\n",
        "\n",
        "        # --- Start Translation Workers ---\n",
        "        for i in range(num_workers_to_start):\n",
        "            task = asyncio.create_task(translation_worker(i + 1))\n",
        "            translation_tasks.add(task)\n",
        "\n",
        "        # --- Transcribe and Produce Chunks ---\n",
        "        print(f\"\\nüéôÔ∏è D√©but de la transcription (yield) et mise en file d'attente des chunks...\")\n",
        "        start_transcribe_time = time.time()\n",
        "        current_chunk: List[Dict[str, Any]] = []\n",
        "        total_audio_duration_est = video_info.get(\"duration\") if video_info else None\n",
        "\n",
        "        try:\n",
        "            # Use the loaded 'model' variable here\n",
        "            segments_generator, info = model.transcribe(audio_file_path, beam_size=5, word_timestamps=False)\n",
        "\n",
        "            detected_language = info.language\n",
        "            lang_prob = info.language_probability\n",
        "            duration_detected = info.duration\n",
        "            print(f\"   Langue d√©tect√©e: {detected_language} (Probabilit√©: {lang_prob:.2f})\")\n",
        "            print(f\"   Dur√©e audio d√©tect√©e par Whisper: {duration_detected:.2f}s\")\n",
        "            if total_audio_duration_est is None:\n",
        "                 total_audio_duration_est = duration_detected\n",
        "                 print(f\"   Utilisation de la dur√©e d√©tect√©e ({total_audio_duration_est:.1f}s) pour le %.\")\n",
        "\n",
        "            if not total_audio_duration_est or total_audio_duration_est <= 0:\n",
        "                print(\"‚ö†Ô∏è Dur√©e audio nulle ou invalide, calcul du pourcentage d√©sactiv√©.\")\n",
        "                total_audio_duration_est = None\n",
        "\n",
        "            segment_id_counter = 0\n",
        "            last_log_time = time.time()\n",
        "\n",
        "            async for segment in segments_generator:\n",
        "                seg_start = segment.start\n",
        "                seg_end = segment.end\n",
        "                text = segment.text.strip()\n",
        "                if not text: continue\n",
        "\n",
        "                duration = max(0, seg_end - seg_start)\n",
        "                progress_percentage = 0.0\n",
        "                if total_audio_duration_est and total_audio_duration_est > 0:\n",
        "                    progress_percentage = min(100.0, max(0.0, (seg_end / total_audio_duration_est) * 100))\n",
        "\n",
        "                total_segments_yielded += 1\n",
        "\n",
        "                formatted_segment = {\n",
        "                    \"id\": segment_id_counter, \"text\": text,\n",
        "                    \"start\": round(seg_start, 3), \"end\": round(seg_end, 3),\n",
        "                    \"duration\": round(duration, 3),\n",
        "                    \"progress_percentage\": round(progress_percentage, 2)\n",
        "                }\n",
        "                segment_id_counter += 1\n",
        "                current_chunk.append(formatted_segment)\n",
        "\n",
        "                current_time = time.time()\n",
        "                if current_time - last_log_time > 10.0:\n",
        "                     q_size = chunk_queue.qsize()\n",
        "                     active_translators = num_workers_to_start - translation_semaphore._value if hasattr(translation_semaphore, '_value') else 'N/A'\n",
        "                     progress_str = f\"{progress_percentage:.1f}%\" if total_audio_duration_est else f\"{seg_end:.1f}s\"\n",
        "                     print(f\"   -> Transcription: {progress_str} | Segments: {total_segments_yielded} | Queue: {q_size} | Translators Active ~{active_translators}\")\n",
        "                     last_log_time = current_time\n",
        "\n",
        "                if len(current_chunk) >= chunk_size:\n",
        "                    await chunk_queue.put((current_chunk_index, current_chunk))\n",
        "                    current_chunk_index += 1\n",
        "                    current_chunk = []\n",
        "\n",
        "            if current_chunk:\n",
        "                 await chunk_queue.put((current_chunk_index, current_chunk))\n",
        "                 current_chunk_index += 1\n",
        "\n",
        "            transcription_time = time.time() - start_transcribe_time\n",
        "            print(f\"\\n‚úÖ Transcription termin√©e en {transcription_time:.2f}s. {total_segments_yielded} segments g√©n√©r√©s.\")\n",
        "            print(f\"   Total chunks envoy√©s √† la traduction : {current_chunk_index}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur critique pendant la transcription/production de chunks: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        finally:\n",
        "            print(\"\\nüö¶ Envoi des signaux d'arr√™t aux workers...\")\n",
        "            for _ in range(num_workers_to_start):\n",
        "                try: await chunk_queue.put(None)\n",
        "                except Exception as qe: print(f\"Erreur envoi sentinel: {qe}\")\n",
        "\n",
        "            print(\"‚è≥ Attente de la fin du traitement des chunks...\")\n",
        "            await chunk_queue.join()\n",
        "            print(\"‚úÖ File d'attente vide.\")\n",
        "\n",
        "            print(\"‚è≥ Attente de la terminaison des workers...\")\n",
        "            done, pending = await asyncio.wait(translation_tasks, timeout=60)\n",
        "\n",
        "            if pending:\n",
        "                print(f\"‚ö†Ô∏è {len(pending)} workers n'ont pas termin√©, annulation...\")\n",
        "                for task in pending: task.cancel()\n",
        "                await asyncio.gather(*pending, return_exceptions=True)\n",
        "\n",
        "            print(\"‚úÖ Tous les workers de traduction ont termin√©.\")\n",
        "\n",
        "    # --- Assemble Final Result ---\n",
        "    pipeline_duration = time.time() - start_pipeline_time\n",
        "    print(\"\\n--- R√©sum√© du Pipeline Async ---\")\n",
        "    print(f\"Temps total d'ex√©cution : {pipeline_duration:.2f} secondes.\")\n",
        "    # ... (rest of summary prints) ...\n",
        "    print(f\"Segments transcrits : {total_segments_yielded}\")\n",
        "    print(f\"Chunks vis√©s pour traduction : {current_chunk_index}\") # Chunks created\n",
        "    print(f\"Chunks trait√©s avec succ√®s : {processed_chunk_count}\")\n",
        "    print(f\"Chunks √©chou√©s : {failed_chunk_count}\")\n",
        "    print(f\"Segments traduits r√©cup√©r√©s : {len(all_translated_segments_list)}\")\n",
        "\n",
        "\n",
        "    final_data = video_info.copy() if video_info else {}\n",
        "    all_translated_segments_list.sort(key=lambda x: x.get('id', float('inf')))\n",
        "    final_data[\"segments\"] = all_translated_segments_list\n",
        "\n",
        "    if failed_chunk_count > 0:\n",
        "         missed_segments_approx = failed_chunk_count * chunk_size\n",
        "         print(f\"‚ö†Ô∏è ATTENTION: {failed_chunk_count} chunks ({missed_segments_approx} segments approx.) ont √©chou√©.\")\n",
        "         print(f\"   Segments sources: {total_segments_yielded}, Segments finaux: {len(all_translated_segments_list)}\")\n",
        "\n",
        "    return final_data\n",
        "\n",
        "\n",
        "# --- Script Principal d'Ex√©cution ---\n",
        "# ... (keep main function as is, it calls the async pipeline) ...\n",
        "def main():\n",
        "    start_overall_time = time.time()\n",
        "\n",
        "    # --- Configuration & Validation ---\n",
        "    youtube_url = \"https://www.youtube.com/watch?v=7q88I_hs3Uw\"#@param {\"type\":\"string\"}\n",
        "    output_dir = \"/content/audio_output\"\n",
        "    output_filename = \"youtube_audio.mp3\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    json_translated_filename = os.path.splitext(output_path)[0] + \"_transcript_translated.json\"\n",
        "\n",
        "    # --- API Key Check (Simplified for hardcoded key) ---\n",
        "    if not GEMINI_API_KEY or GEMINI_API_KEY == \"VOTRE_CLE_API_GEMINI_ICI\":\n",
        "         print(\"üõë ERREUR: La cl√© API Gemini est manquante ou est toujours le placeholder.\")\n",
        "         print(\"   Veuillez √©diter la variable GEMINI_API_KEY en haut du script avec votre cl√© r√©elle.\")\n",
        "         return # Stop execution\n",
        "    else:\n",
        "         print(\"üîë Cl√© API Gemini charg√©e depuis le code source (hardcod√©e).\")\n",
        "\n",
        "\n",
        "    # --- Parameter Validation & Global Updates ---\n",
        "    global TRANSLATION_CHUNK_SIZE, MAX_CONCURRENT_TRANSLATIONS, GEMINI_RATE_LIMIT_PER_MINUTE\n",
        "    global translation_semaphore, gemini_rate_limiter # Allow modification\n",
        "\n",
        "    chunk_size = TRANSLATION_CHUNK_SIZE\n",
        "    if not isinstance(chunk_size, int) or chunk_size <= 0:\n",
        "        print(f\"‚ö†Ô∏è Taille de chunk invalide ({chunk_size}), utilisation de 30.\")\n",
        "        chunk_size = 30\n",
        "\n",
        "    max_workers_local = MAX_CONCURRENT_TRANSLATIONS\n",
        "    if not isinstance(max_workers_local, int) or max_workers_local <= 0:\n",
        "        print(f\"‚ö†Ô∏è Nombre de workers invalide ({max_workers_local}), utilisation de 5.\")\n",
        "        max_workers_local = 5\n",
        "\n",
        "    rate_limit_local = GEMINI_RATE_LIMIT_PER_MINUTE\n",
        "    if not isinstance(rate_limit_local, int) or rate_limit_local <= 0:\n",
        "        print(f\"‚ö†Ô∏è Rate limit invalide ({rate_limit_local}), utilisation de 15.\")\n",
        "        rate_limit_local = 15\n",
        "\n",
        "    # Re-initialize globals based on potentially validated values\n",
        "    print(f\"üîß Configuration appliqu√©e : Chunk={chunk_size}, Workers={max_workers_local}, RateLimit={rate_limit_local}/min\")\n",
        "    translation_semaphore = asyncio.Semaphore(max_workers_local)\n",
        "    gemini_rate_limiter = AsyncLimiter(rate_limit_local, 60)\n",
        "\n",
        "\n",
        "    # --- √âtape 1: T√©l√©chargement (Synchrone) ---\n",
        "    print(\"\\n\" + \"-\" * 30)\n",
        "    print(\"√âTAPE 1: T√âL√âCHARGEMENT AUDIO & METADONN√âES\")\n",
        "    print(\"-\" * 30)\n",
        "    audio_file_path, video_info = download_youtube_audio_improved(youtube_url, output_path)\n",
        "\n",
        "    if not audio_file_path:\n",
        "        print(\"\\n‚ùå √âchec critique: Impossible de t√©l√©charger ou trouver le fichier audio.\")\n",
        "        if video_info: print(\"   M√©tadonn√©es partielles:\", json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "        print(\"Pipeline arr√™t√©.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Informations Vid√©o R√©cup√©r√©es ---\")\n",
        "    if video_info: print(json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "    else: print(\"(Aucune m√©tadonn√©e r√©cup√©r√©e)\")\n",
        "    print(\"-----------------------------------\\n\")\n",
        "\n",
        "    # --- √âtape 2 & 3: Transcription et Traduction Async ---\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"√âTAPE 2 & 3: TRANSCRIPTION & TRADUCTION ASYNCHRONE\")\n",
        "    print(f\"  Mod√®le Whisper: {WHISPER_MODEL_SIZE}, Compute: {WHISPER_COMPUTE_TYPE}, Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
        "    print(f\"  Chunk Size: {chunk_size}, Max Concurrent API: {max_workers_local}, Rate Limit: {rate_limit_local}/min\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Run the async pipeline, passing the hardcoded API key\n",
        "    final_translated_data = asyncio.run(process_audio_pipeline_async(\n",
        "        audio_file_path=audio_file_path,\n",
        "        whisper_model_size=WHISPER_MODEL_SIZE,\n",
        "        whisper_compute_type=WHISPER_COMPUTE_TYPE,\n",
        "        api_key=GEMINI_API_KEY, # Passer la cl√© API globale ici\n",
        "        chunk_size=chunk_size,\n",
        "        num_workers_to_start=max_workers_local, # Pass validated worker count\n",
        "        video_info=video_info\n",
        "    ))\n",
        "\n",
        "    # --- Sauvegarde Finale ---\n",
        "    print(\"\\n\" + \"-\" * 30)\n",
        "    print(\"√âTAPE 4: SAUVEGARDE DES R√âSULTATS\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    if final_translated_data and final_translated_data.get(\"segments\"):\n",
        "        try:\n",
        "            with open(json_translated_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(final_translated_data, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"üíæ Transcription traduite sauvegard√©e dans : {json_translated_filename}\")\n",
        "            print(f\"   Nombre total de segments dans le fichier final : {len(final_translated_data['segments'])}\")\n",
        "        except IOError as e:\n",
        "            print(f\"‚ùå Erreur lors de la sauvegarde du fichier JSON traduit : {e}\")\n",
        "        except TypeError as e:\n",
        "             print(f\"‚ùå Erreur de Type lors de la s√©rialisation finale en JSON : {e}\")\n",
        "        except Exception as e:\n",
        "             print(f\"‚ùå Erreur inattendue lors de la sauvegarde JSON (traduit) : {e}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Aucune donn√©e traduite √† sauvegarder (pipeline √©chou√© ou aucun segment g√©n√©r√©/traduit).\")\n",
        "\n",
        "\n",
        "    # --- Nettoyage Optionnel ---\n",
        "    # print(\"\\n--- Nettoyage ---\")\n",
        "    # try:\n",
        "    #     if audio_file_path and os.path.exists(audio_file_path):\n",
        "    #         os.remove(audio_file_path)\n",
        "    #         print(f\"üóëÔ∏è Fichier audio supprim√© : {audio_file_path}\")\n",
        "    # except OSError as e:\n",
        "    #     print(f\"‚ùå Erreur suppression fichier audio {audio_file_path}: {e}\")\n",
        "\n",
        "\n",
        "    end_overall_time = time.time()\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"üèÅ Pipeline Global Termin√© en {end_overall_time - start_overall_time:.2f} secondes.\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "\n",
        "# --- Bloc d'ex√©cution principal ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- V√©rification Initiale des D√©pendances ---\")\n",
        "    # --- Dependency Checks ---\n",
        "    required_libs = {'yt-dlp', 'requests', 'torch', 'faster_whisper', 'ctranslate2', 'aiohttp', 'aiolimiter', 'nest_asyncio'} # Added nest_asyncio\n",
        "    installed_libs = set()\n",
        "    try: import yt_dlp; installed_libs.add('yt-dlp')\n",
        "    except ImportError: pass\n",
        "    try: import requests; installed_libs.add('requests')\n",
        "    except ImportError: pass\n",
        "    try: import torch; installed_libs.add('torch')\n",
        "    except ImportError: pass\n",
        "    try: import faster_whisper; installed_libs.add('faster_whisper')\n",
        "    except ImportError: pass # Handled above\n",
        "    try: import ctranslate2; installed_libs.add('ctranslate2')\n",
        "    except ImportError: pass\n",
        "    try: import aiohttp; installed_libs.add('aiohttp')\n",
        "    except ImportError: pass\n",
        "    try: import aiolimiter; installed_libs.add('aiolimiter')\n",
        "    except ImportError: pass\n",
        "    try: import nest_asyncio; installed_libs.add('nest_asyncio')\n",
        "    except ImportError: pass\n",
        "\n",
        "\n",
        "    missing_libs = required_libs - installed_libs\n",
        "    # Don't try to re-install faster_whisper/ctranslate2 if the top-level import failed\n",
        "    missing_libs -= {'faster_whisper', 'ctranslate2'}\n",
        "\n",
        "\n",
        "    if missing_libs:\n",
        "        print(f\"Biblioth√®ques manquantes d√©tect√©es: {', '.join(missing_libs)}\")\n",
        "        print(\"Tentative d'installation via pip...\")\n",
        "        install_cmd = ['pip', 'install', '-q'] + list(missing_libs)\n",
        "        try:\n",
        "            subprocess.run(install_cmd, check=True)\n",
        "            print(\"Installation termin√©e. Re-v√©rification...\")\n",
        "            recheck_failed = False\n",
        "            # Minimal re-check (add more if needed)\n",
        "            if 'yt-dlp' in missing_libs:\n",
        "                 try: import yt_dlp\n",
        "                 except ImportError: recheck_failed = True; print(\"Echec import yt-dlp post-install\")\n",
        "            if 'aiohttp' in missing_libs:\n",
        "                 try: import aiohttp\n",
        "                 except ImportError: recheck_failed = True; print(\"Echec import aiohttp post-install\")\n",
        "            if 'nest_asyncio' in missing_libs:\n",
        "                 try: import nest_asyncio\n",
        "                 except ImportError: recheck_failed = True; print(\"Echec import nest_asyncio post-install\")\n",
        "\n",
        "            if recheck_failed:\n",
        "                 print(\"‚ÄºÔ∏è Certaines installations semblent avoir √©chou√©. Le script risque de ne pas fonctionner.\")\n",
        "            else:\n",
        "                 print(\"‚úÖ Biblioth√®ques suppl√©mentaires install√©es.\")\n",
        "\n",
        "        except subprocess.CalledProcessError as install_err:\n",
        "            print(f\"‚ùå √âchec de l'installation pip : {install_err}\")\n",
        "            print(\"   Veuillez essayer d'installer manuellement les biblioth√®ques manquantes.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur inattendue lors de l'installation pip : {e}\")\n",
        "\n",
        "    # Check ffmpeg\n",
        "    try:\n",
        "         result = subprocess.run(['ffmpeg', '-version'], check=True, capture_output=True, timeout=5, text=True, errors='ignore')\n",
        "         print(f\"‚úÖ ffmpeg trouv√©: {result.stdout.splitlines()[0]}\")\n",
        "    except Exception as ffmpeg_err:\n",
        "         print(f\"‚ö†Ô∏è ffmpeg non trouv√© ou ne fonctionne pas ({ffmpeg_err}). Requis par faster-whisper.\")\n",
        "         print(\"   Installation sugg√©r√©e: sudo apt update && sudo apt install ffmpeg\")\n",
        "\n",
        "\n",
        "    # Check for API Key placeholder *before* running main logic\n",
        "    if not GEMINI_API_KEY or GEMINI_API_KEY == \"VOTRE_CLE_API_GEMINI_ICI\":\n",
        "        print(\"\\n‚ö†Ô∏è ALERTE: Cl√© API Gemini non configur√©e ou placeholder non remplac√© dans le script.\")\n",
        "        print(\"   Veuillez √©diter le script et remplacer 'VOTRE_CLE_API_GEMINI_ICI'.\")\n",
        "    else:\n",
        "        print(\"\\nüîë Cl√© API Gemini trouv√©e (hardcod√©e) dans le script.\")\n",
        "    print(\"-------------------------------------------\")\n",
        "\n",
        "    # Run the main processing pipeline\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0tw7nuUHQmV",
        "outputId": "37d6b4f2-ee08-48df-8561-e170b7451c93",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ faster-whisper est d√©j√† install√©.\n",
            "‚ùå Erreur finale lors de l'importation de WhisperModel/AutoModel: cannot import name 'AutoModel' from 'faster_whisper' (/usr/local/lib/python3.11/dist-packages/faster_whisper/__init__.py)\n",
            "   M√™me si l'installation a sembl√© r√©ussir, l'importation sp√©cifique a √©chou√©.\n",
            "   Essayez de red√©marrer l'environnement d'ex√©cution et r√©installez.\n",
            "üõë ALERTE: Vous n'avez pas remplac√© 'VOTRE_CLE_API_GEMINI_ICI' par votre cl√© API r√©elle.\n",
            "--- V√©rification Initiale des D√©pendances ---\n",
            "‚úÖ ffmpeg trouv√©: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "\n",
            "üîë Cl√© API Gemini trouv√©e (hardcod√©e) dans le script.\n",
            "-------------------------------------------\n",
            "üîë Cl√© API Gemini charg√©e depuis le code source (hardcod√©e).\n",
            "üîß Configuration appliqu√©e : Chunk=50, Workers=5, RateLimit=15/min\n",
            "\n",
            "------------------------------\n",
            "√âTAPE 1: T√âL√âCHARGEMENT AUDIO & METADONN√âES\n",
            "------------------------------\n",
            "‚ÑπÔ∏è  R√©cup√©ration des m√©tadonn√©es de la vid√©o...\n",
            "‚úÖ M√©tadonn√©es r√©cup√©r√©es.\n",
            "üîÑ T√©l√©chargement/V√©rification audio vers /content/audio_output/youtube_audio.mp3...\n",
            "‚ÑπÔ∏è Fichier audio '/content/audio_output/youtube_audio.mp3' existe d√©j√† et n'est pas vide. Utilisation du fichier existant.\n",
            "\n",
            "--- Informations Vid√©o R√©cup√©r√©es ---\n",
            "{\n",
            "  \"video_id\": \"7q88I_hs3Uw\",\n",
            "  \"channel_name\": \"May Ho\",\n",
            "  \"channel_url\": \"https://www.youtube.com/@MayHo\",\n",
            "  \"title\": \"„Äê‰∏≠Âúã ÈáçÂ∫Ü Chongqing Vlog Part 2„ÄëÁ¨¨‰∏ÄÊ¨°Âá∫ÂúãË∑®Âπ¥ üéâ ÂêÉÈ≠öÂêÉÂà∞Áòã‰∫ÜÊàëÂÄë 4Ê¢ùÊì∫Êì∫Âïä üêü ÈæîÁÅòÂè§ÈéÆÊúâ1700Â§öÂπ¥ÁöÑÊ≠∑Âè≤‰∫Ü!! ‚û°Ô∏èÂåóÂÄâÊñáÂâµË°óÂçÄ,ËßÇÈü≥Ê°•Ë°óÈÅìÂ°îÂù™,Â±±ÂüéÂ∑∑,ÈáçÊÖ∂È≠ÅÊòüÊ®ì\",\n",
            "  \"description\": \"‚ú¶ ‰∏äÈõÜÂΩ±ÁâáÔºöhttps://youtu.be/mrLZdfZC724?si=KFthVhiNW5kzC476\\n‚ú¶ Ë®ÇÈñ±ÊàëÂêß üòù https://bit.ly/mayhosubscribe\\n‚ú¶ Â∑•‰ΩúÂêà‰ΩúÈÇÄÁ¥Ñ mayho0110@hotmail.com\\n\\n- - -\\n\\nÁ•ùÂ§ßÂÆ∂2025Âπ¥Âø´Ê®Ç‚ú® Êñ∞ÁöÑ‰∏ÄÂπ¥Â∏åÊúõÂ§ßÂÆ∂Ë∫´È´îÂÅ•Â∫∑ Âø´Âø´Ê®ÇÊ®Ç Èå¢Ë∂äË≥∫Ë∂äÂ§öüí∞‚ù§Ô∏è\\nÂú®60Ê®ì KÊàøË∑®Âπ¥ ÁàΩÂïäÔºÅ ‰∏ÄÈÇäÂêÉÁÅ´Èçã ‰∏ÄÈÇäÂî±K ‰∏ÄÈÇäÂÄíÊï∏ !!!!!!!\\n\\nËæ£Ê§íÂíñÂï°  Â§†ÁâπÂà•üå∂Ô∏è Â§ßÂÆ∂ÂèØ‰ª•Ë©¶‰∏Ä‰∏ã ü§£\\n\\n- - -‚úàÔ∏èüõ≥Ô∏èüöó- - -\\n\\nÂ™ΩÂ™ΩÊ°ëÁöÑÊúãÂèã Travel agent\\nÊâæÂåÖËªäÔºåÈÅäËº™ÔºåË©¢ÂïèÊóÖÈÅäË©≥ÊÉÖ ÂèØ‰ª•Êâæ‰ªñÂÄëÂî∑~\\nIG Ôºögrand.holidays\\nhttps://bit.ly/4axsgmC\\n\\n- - -\\n\\n‚ú¶ Day 5  00:00 \\n\\nüìçÈæîÁÅòÂè§ÈéÆ\\nüìçÊ¢µÈ´òÁï´ÂªäÂíñÂï°È§®\\n\\n‚ú¶ Day 6 19:44 \\n\\nüìçÂçÉÂêàÊô∫ÈÅ∏È´òÁ©∫ÈÖíÂ∫ó\\nüìçÂêõ‰∫≠Ë®≠Ë®àÈÖíÂ∫ó ÔºàÂî±KÔºâ\\n\\n‚ú¶  Day 7 36:02 \\n\\nüìçËÄÅË°óÁ≥ØÁ±≥Âõ¢(Ê¥™Â¥ñÊ¥ûÂ∫ó) \\nüìçÂåóÂÄâÊñáÂâµË°óÂçÄ\\nüìçËßÇÈü≥Ê°•Ë°óÈÅìÂ°îÂù™\\nüìçË¢ÅÂ™ΩÁ¥ÖË±ÜÊπØ\\nüìçÁÅ∂Êù±ÂÆ∂\\n\\n‚ú¶  Day 8 56:44 \\n\\nüìçÂ±±ÂüéÂ∑∑\\nüìçÂçÅÂÖ´Ê¢Ø ÔºàÊ≤íÂéªÂà∞ü•π Êúâ‰∫∫ÊãçÊà≤Ôºâ\\nüìçÈáçÊÖ∂È≠ÅÊòüÊ®ì\\nüìçÂåóÊ≠•ÂúíÁÅ´Èçã\\n\\n- - -\\n\\nhttps://youtu.be/7fUQ50-SllU?si=3quhCczRHFYwVgM0\\nhttps://youtu.be/lYE176cDqek?si=h90FsgxQAsm8HgDq\\nhttps://youtu.be/PHQNL99q58s?si=Q3br2KPEv0cf9nuN\\n\\n- - -\\n\\n‚úß Instagram  https://www.instagram.com/mayho10/\\n‚úß Facebook  http://www.facebook.com/msmayho/\\n‚úß TikTok https://www.tiktok.com/@mayho10\\n‚úß Â∞èÁ¥ÖÊõ∏ https://bit.ly/MayhoXhs\\n\\n‚ñ† ÊîùÂΩ±Âô®Êùê\\nSONY ZV-1 II & ECM-G1\\niPhone 16 Pro Max \\n\\n‚ñ† Music  \\nTrackTribe\\n\\n#MayHo‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã #ÁæéÂ•ΩÁöÑ‰∏ÄÂ§© #ÊóÖË°åvlog  #ÈáçÊÖ∂\",\n",
            "  \"duration\": 4253,\n",
            "  \"upload_date\": \"20250129\",\n",
            "  \"thumbnail\": \"https://i.ytimg.com/vi/7q88I_hs3Uw/maxresdefault.jpg\"\n",
            "}\n",
            "-----------------------------------\n",
            "\n",
            "------------------------------\n",
            "√âTAPE 2 & 3: TRANSCRIPTION & TRADUCTION ASYNCHRONE\n",
            "  Mod√®le Whisper: base, Compute: default, Device: cuda\n",
            "  Chunk Size: 50, Max Concurrent API: 5, Rate Limit: 15/min\n",
            "------------------------------\n",
            "\n",
            "üöÄ D√©marrage du pipeline asynchrone...\n",
            "üîÑ Chargement du mod√®le faster-whisper 'base' (compute_type=float16) sur 'cuda'...\n",
            "‚ùå ERREUR CRITIQUE (NameError): name 'AutoModel' is not defined. 'AutoModel' n'est toujours pas d√©fini.\n",
            "   Cela indique un probl√®me persistant avec l'importation de faster_whisper.\n",
            "   V√©rifiez l'installation manuelle et l'environnement.\n",
            "\n",
            "------------------------------\n",
            "√âTAPE 4: SAUVEGARDE DES R√âSULTATS\n",
            "------------------------------\n",
            "‚ö†Ô∏è Aucune donn√©e traduite √† sauvegarder (pipeline √©chou√© ou aucun segment g√©n√©r√©/traduit).\n",
            "\n",
            "========================================\n",
            "üèÅ Pipeline Global Termin√© en 5.09 secondes.\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp requests torch faster_whisper ctranslate2 aiohttp aiolimiter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXX_A2vdZftR",
        "outputId": "e2c9812e-a9fd-456f-cac9-5612ace434c7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.3.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting faster_whisper\n",
            "  Using cached faster_whisper-1.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting ctranslate2\n",
            "  Using cached ctranslate2-4.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.15)\n",
            "Collecting aiolimiter\n",
            "  Using cached aiolimiter-1.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster_whisper) (0.30.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster_whisper) (0.21.1)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster_whisper)\n",
            "  Using cached onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting av>=11 (from faster_whisper)\n",
            "  Using cached av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from faster_whisper) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ctranslate2) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ctranslate2) (2.0.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.11/dist-packages (from ctranslate2) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.19.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster_whisper) (24.2)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster_whisper)\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster_whisper) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster_whisper) (5.29.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster_whisper)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading faster_whisper-1.1.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiolimiter-1.2.1-py3-none-any.whl (6.7 kB)\n",
            "Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, ctranslate2, av, aiolimiter, coloredlogs, onnxruntime, faster_whisper\n",
            "Successfully installed aiolimiter-1.2.1 av-14.3.0 coloredlogs-15.0.1 ctranslate2-4.6.0 faster_whisper-1.1.1 humanfriendly-10.0 onnxruntime-1.21.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "collapsed": true,
        "id": "T-lK75Wtt26K",
        "outputId": "93d86721-7b17-4262-88b0-c2a0a99fe2fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h1 style=\"color: green;\">Done! ‚úÖ</h1></br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the URL to access the App: https://journal-congo-currently-pointer.trycloudflare.com                                 |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:09] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:10] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:13] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:13] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:13] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:13] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:13] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:50:27] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:50:28] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:50:57] \"GET / HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# @title # **Start App**\n",
        "# @markdown <-- Start the cell.\n",
        "\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  clear_output()\n",
        "  display(HTML(f'<strong style=\"color: blue;\">Trying to launch the App on the Web(if it gets stuck here cloudflared is having issues)</strong>'))\n",
        "  p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  for line in p.stderr:\n",
        "    l = line.decode()\n",
        "    if \"trycloudflare.com \" in l:\n",
        "      clear_output()\n",
        "      display(HTML('<h1 style=\"color: green;\">Done! ‚úÖ</h1></br>'))\n",
        "      print(\"This is the URL to access the App:\", l[l.find(\"http\"):], end='')\n",
        "\n",
        "clear_output()\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(5051,)).start()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=False, port=5051)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d48129ecb82e4def940ab46552119416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce22a87a00ad4eadbda7a9aaa7db0aa5",
              "IPY_MODEL_acdd87c665284f9b9db3f1fe94425f75",
              "IPY_MODEL_7526d70cd55344bd91428697091bb15c"
            ],
            "layout": "IPY_MODEL_1469cfba43a343e4aa8c8a9089846678"
          }
        },
        "ce22a87a00ad4eadbda7a9aaa7db0aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc4ecb0c285a467d8626dd91f8543ccf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_62e4251d8b9d42c194e2e60ca81b621b",
            "value": "config.json:‚Äá100%"
          }
        },
        "acdd87c665284f9b9db3f1fe94425f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24a261c7925f41b2ac97bfac8e307054",
            "max": 2394,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8a050d075ba4f9696e3d61fa8e34766",
            "value": 2394
          }
        },
        "7526d70cd55344bd91428697091bb15c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff96605356fa4b918fd400a188539eb0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8fed5d7e2321467490d2b803e1db1708",
            "value": "‚Äá2.39k/2.39k‚Äá[00:00&lt;00:00,‚Äá216kB/s]"
          }
        },
        "1469cfba43a343e4aa8c8a9089846678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc4ecb0c285a467d8626dd91f8543ccf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62e4251d8b9d42c194e2e60ca81b621b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24a261c7925f41b2ac97bfac8e307054": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8a050d075ba4f9696e3d61fa8e34766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff96605356fa4b918fd400a188539eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fed5d7e2321467490d2b803e1db1708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2f73b55932a4ccab1b9ea4251c74691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9225d6af04494c54b921b856b8aef838",
              "IPY_MODEL_08a3d7afce5841928d4ee049b0138768",
              "IPY_MODEL_5c3e92d4db8548578792c357341bf6cf"
            ],
            "layout": "IPY_MODEL_8ba5c9795e6a49b4b31a3b450a69c2cc"
          }
        },
        "9225d6af04494c54b921b856b8aef838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26bcfdbf9cbc4d4996644bececf55ea7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_24159900f732477ea4f43d6ae5b77b4a",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "08a3d7afce5841928d4ee049b0138768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28091c5ea0794fb1afe15799e2d3a0f7",
            "max": 340,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7497d7c9fb8f4821981ef6e69e18b0f8",
            "value": 340
          }
        },
        "5c3e92d4db8548578792c357341bf6cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d9703f174bb4578a87e149082439912",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4cd3bdec33a24dd0bda7b91c1278ef36",
            "value": "‚Äá340/340‚Äá[00:00&lt;00:00,‚Äá13.3kB/s]"
          }
        },
        "8ba5c9795e6a49b4b31a3b450a69c2cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26bcfdbf9cbc4d4996644bececf55ea7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24159900f732477ea4f43d6ae5b77b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28091c5ea0794fb1afe15799e2d3a0f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7497d7c9fb8f4821981ef6e69e18b0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d9703f174bb4578a87e149082439912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cd3bdec33a24dd0bda7b91c1278ef36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e3f0af0cb2b4e8aa6d067bff07ef6c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06b6e47664d142d487800d35e36e57b6",
              "IPY_MODEL_76b5534fc817401896a81e1ce800a14b",
              "IPY_MODEL_93f1f4f37d534141958c32602e67ff56"
            ],
            "layout": "IPY_MODEL_c82c25ef01ce4c2d97633ed68b8d417d"
          }
        },
        "06b6e47664d142d487800d35e36e57b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d0454f62e37455ca05d8c5efdd2f1ca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6f8df4fff387417caf6b83bb664a1b40",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "76b5534fc817401896a81e1ce800a14b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef14a0a096b34ec9b143edc0c7ab2ff6",
            "max": 2480617,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4fd4db0014d341a98b39eddb9a274c41",
            "value": 2480617
          }
        },
        "93f1f4f37d534141958c32602e67ff56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f27022e86b841d892ba2ee1dfd415ce",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_65a6cd188c374d5bbb66a9e21d59c85d",
            "value": "‚Äá2.48M/2.48M‚Äá[00:00&lt;00:00,‚Äá8.49MB/s]"
          }
        },
        "c82c25ef01ce4c2d97633ed68b8d417d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d0454f62e37455ca05d8c5efdd2f1ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8df4fff387417caf6b83bb664a1b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef14a0a096b34ec9b143edc0c7ab2ff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fd4db0014d341a98b39eddb9a274c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f27022e86b841d892ba2ee1dfd415ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65a6cd188c374d5bbb66a9e21d59c85d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d2ef692b95d44a4aa1bbf54822518ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c32f0ba880a64b46a81a35c67eeca347",
              "IPY_MODEL_65b9b14be52741899899da0e14a78690",
              "IPY_MODEL_d29275c8d5ab41339b516e4e67c1bb35"
            ],
            "layout": "IPY_MODEL_b9c46ffd5a3b41029508664d5a28491b"
          }
        },
        "c32f0ba880a64b46a81a35c67eeca347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e95b62c25cb54e99a8ab442393910ad7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_400896d7239240b380ca6f1048365368",
            "value": "model.bin:‚Äá100%"
          }
        },
        "65b9b14be52741899899da0e14a78690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f94a8299d7264de2b87cba11c7410070",
            "max": 3087284237,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08d364b65a8a4f2989c9889cf3ce1e4c",
            "value": 3087284237
          }
        },
        "d29275c8d5ab41339b516e4e67c1bb35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1830a152ab054383ac8a1e55785cd02b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_621db47e2e8e444c8b2baa36055dda26",
            "value": "‚Äá3.09G/3.09G‚Äá[00:16&lt;00:00,‚Äá227MB/s]"
          }
        },
        "b9c46ffd5a3b41029508664d5a28491b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e95b62c25cb54e99a8ab442393910ad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "400896d7239240b380ca6f1048365368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f94a8299d7264de2b87cba11c7410070": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08d364b65a8a4f2989c9889cf3ce1e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1830a152ab054383ac8a1e55785cd02b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "621db47e2e8e444c8b2baa36055dda26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a32c8415b4348099f7f878aa0810bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd77afc989e144d3a1a74d1ac9e3c622",
              "IPY_MODEL_d52e687fd3ec45a984478737f24f5748",
              "IPY_MODEL_e62356eee274499cbfdc726991e471fd"
            ],
            "layout": "IPY_MODEL_a8cd999c6daf474db8c06e37b18c8214"
          }
        },
        "dd77afc989e144d3a1a74d1ac9e3c622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d6a1badd4214a289c981d3b2bfa3983",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c493e4b16a9f42fdb71e62551e6e185d",
            "value": "vocabulary.json:‚Äá100%"
          }
        },
        "d52e687fd3ec45a984478737f24f5748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18751380a2e644958863c0b9e98b47b5",
            "max": 1068114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3c8628bb02b4f02abded625ab3905c3",
            "value": 1068114
          }
        },
        "e62356eee274499cbfdc726991e471fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6be2e376cd924a7caec2e6ed71b08635",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ea8471c6aeff43dbb772df06724f7422",
            "value": "‚Äá1.07M/1.07M‚Äá[00:00&lt;00:00,‚Äá7.19MB/s]"
          }
        },
        "a8cd999c6daf474db8c06e37b18c8214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d6a1badd4214a289c981d3b2bfa3983": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c493e4b16a9f42fdb71e62551e6e185d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18751380a2e644958863c0b9e98b47b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3c8628bb02b4f02abded625ab3905c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6be2e376cd924a7caec2e6ed71b08635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea8471c6aeff43dbb772df06724f7422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}