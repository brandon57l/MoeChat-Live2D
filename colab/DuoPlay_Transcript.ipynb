{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/My-LinkedIn-red?&style=flat)](https://www.linkedin.com/in/brandon-laroche-3b73691b7/) [![GitHub](https://img.shields.io/badge/GitHub-red?logo=github)](https://github.com/brandon57l/) [![huggingface](https://img.shields.io/badge/HuggingFace-red)\n",
        "](https://huggingface.co/brandon57)\n",
        "<!-- [![GitHub](https://img.shields.io/badge/huggingface-red?logo=huggingface&logoColor=white&style=flat)\n",
        "](https://www.linkedin.com/in/brandon-laroche-3b73691b7/) -->\n"
      ],
      "metadata": {
        "id": "hWRr6WWjPE0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "X8jkshBMrxsq",
        "outputId": "a48d9d4c-406d-4e6b-a50d-f51bc01ae1ec",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1 style=\"color: green;\">‚úÖ Done !</h1>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.5.1+cu121\n",
            "CUDA available: True\n",
            "CUDA version PyTorch built with: 12.1\n",
            "cuDNN version PyTorch built with: 90100\n",
            "Device Name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# @title # 1Ô∏è‚É£ **Installation**\n",
        "from IPython.display import clear_output, HTML\n",
        "\n",
        "clear_output()\n",
        "display(HTML('<h2 style=\"color: blue;\">Installation des d√©pendances... üöÄ</h2>'))\n",
        "\n",
        "\n",
        "!wget -P ~ https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i ~/cloudflared-linux-amd64.deb\n",
        "clear_output()\n",
        "display(HTML('<h3 style=\"color: black;\">Cloudflared - OK</h3>'))\n",
        "\n",
        "!pip install flask flask-cors\n",
        "clear_output()\n",
        "display(HTML('<h3 style=\"color: black;\">Flask/Cors - OK</h3>'))\n",
        "\n",
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "clear_output()\n",
        "display(HTML('<h3 style=\"color: black;\">Torch/Cuda - OK</h3>'))\n",
        "\n",
        "!pip uninstall faster-whisper ctranslate2 -y\n",
        "!pip install --no-cache-dir ctranslate2 faster-whisper\n",
        "clear_output()\n",
        "display(HTML('<h3 style=\"color: black;\">Whisper - OK</h3>'))\n",
        "\n",
        "# !pip install -q openai-whisper\n",
        "# clear_output()\n",
        "# display(HTML('<h3 style=\"color: black;\">Whisper - OK</h3>'))\n",
        "\n",
        "!pip install yt-dlp pydub==0.25.1\n",
        "clear_output()\n",
        "display(HTML('<h3 style=\"color: black;\">Youtube dep - OK</h3>'))\n",
        "\n",
        "!apt-get update && apt-get install -y ffmpeg\n",
        "clear_output()\n",
        "display(HTML('<h3 style=\"color: black;\">ffmpeg - OK</h3>'))\n",
        "\n",
        "\n",
        "clear_output()\n",
        "display(HTML('<h1 style=\"color: green;\">‚úÖ Done !</h1>'))\n",
        "\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version PyTorch built with: {torch.version.cuda}\")\n",
        "    print(f\"cuDNN version PyTorch built with: {torch.backends.cudnn.version()}\")\n",
        "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # **Transcription & Traduction (Upload Incr√©mental)**\n",
        "\n",
        "# --- Initial Cleanup ---\n",
        "print(\"--- Initial Cleanup ---\")\n",
        "# Keep OUTPUT_DIR_V2 cleanup, but remove others as they might conflict if script reruns quickly\n",
        "# !rm -f /content/audio_output/* # Keep if you used this dir before\n",
        "# !rm -f /content/audio_output_optimized/* # Keep if you used this dir before\n",
        "# !rm -f /content/audio_output_optimized_v2/* # Keep V2 cleanup\n",
        "# !rm -rf /content/audio_chunks/* # Keep chunk cleanup\n",
        "\n",
        "# Safer cleanup targeting specific potential outputs if they exist\n",
        "OUTPUT_DIR_V2 = \"/content/audio_output_optimized_v2\"\n",
        "CHUNK_DIR = \"/content/audio_chunks\"\n",
        "TEMP_AUDIO_FILE = \"/content/temp_downloaded_audio.mp3\" # Define a temp file path\n",
        "\n",
        "if os.path.exists(OUTPUT_DIR_V2):\n",
        "    for f in os.listdir(OUTPUT_DIR_V2):\n",
        "        if f.endswith(\".json\") or f.endswith(\".mp3\"): # Target specific files\n",
        "             try: os.remove(os.path.join(OUTPUT_DIR_V2, f))\n",
        "             except OSError as e: print(f\"Warn: Could not remove {f}: {e}\")\n",
        "else:\n",
        "    os.makedirs(OUTPUT_DIR_V2, exist_ok=True)\n",
        "\n",
        "if os.path.exists(CHUNK_DIR):\n",
        "    try: shutil.rmtree(CHUNK_DIR)\n",
        "    except OSError as e: print(f\"Warn: Could not remove {CHUNK_DIR}: {e}\")\n",
        "os.makedirs(CHUNK_DIR, exist_ok=True)\n",
        "\n",
        "if os.path.exists(TEMP_AUDIO_FILE):\n",
        "    try: os.remove(TEMP_AUDIO_FILE)\n",
        "    except OSError as e: print(f\"Warn: Could not remove temp audio {TEMP_AUDIO_FILE}: {e}\")\n",
        "\n",
        "\n",
        "print(\"Cleanup done.\")\n",
        "\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "import math\n",
        "import shutil\n",
        "import requests\n",
        "import sys\n",
        "import random\n",
        "import concurrent.futures\n",
        "import glob # <-- Ajout pour lister les fichiers g√©n√©r√©s par ffmpeg\n",
        "\n",
        "# --- Library Imports & Checks ---\n",
        "print(\"\\n--- Library Imports & Checks ---\")\n",
        "# ... (v√©rifications inchang√©es) ...\n",
        "try:\n",
        "    import torch\n",
        "    print(\"‚úÖ PyTorch install√©.\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå PyTorch non install√©. Veuillez l'installer.\")\n",
        "    exit()\n",
        "try:\n",
        "    from faster_whisper import WhisperModel\n",
        "    print(\"‚úÖ faster-whisper install√©.\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå faster-whisper non install√©. Veuillez l'installer.\")\n",
        "    exit()\n",
        "# pydub n'est plus n√©cessaire pour le d√©coupage, mais peut √™tre utile ailleurs ?\n",
        "# Gardons-le pour l'instant au cas o√π, mais on pourrait l'enlever si non utilis√©.\n",
        "try:\n",
        "    from pydub import AudioSegment\n",
        "    print(\"‚úÖ pydub install√© (peut ne plus √™tre utilis√© pour le d√©coupage).\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå pydub non install√©. Veuillez l'installer si n√©cessaire pour d'autres t√¢ches.\")\n",
        "    # exit() # Ne pas quitter si pydub manque, car on utilise ffmpeg pour d√©couper\n",
        "\n",
        "# --- GPU Check ---\n",
        "print(\"\\n--- GPU Check ---\")\n",
        "# ... (inchang√©) ...\n",
        "IS_GPU_AVAILABLE = torch.cuda.is_available()\n",
        "print(f\"‚úÖ GPU d√©tect√©: {IS_GPU_AVAILABLE}\")\n",
        "if IS_GPU_AVAILABLE:\n",
        "    try:\n",
        "        print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "        major, minor = torch.cuda.get_device_capability(0)\n",
        "        print(f\"   Compute Capability: {major}.{minor}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Impossible de r√©cup√©rer les d√©tails du GPU: {e}\")\n",
        "\n",
        "\n",
        "# --- Directories ---\n",
        "print(\"\\n--- Directories ---\")\n",
        "# CHUNK_DIR et OUTPUT_DIR_V2 d√©finis dans le cleanup\n",
        "print(f\"‚úÖ Dossier chunks pr√™t: {CHUNK_DIR}\")\n",
        "print(f\"‚úÖ Dossier sortie pr√™t: {OUTPUT_DIR_V2}\")\n",
        "\n",
        "# ==============================================================\n",
        "# --- FONCTIONS TRANSCRIPTION (INCHANG√âES) ---\n",
        "# ==============================================================\n",
        "print(\"\\n--- D√©finition Fonctions Utilitaires (Pr√©-traitement) ---\")\n",
        "\n",
        "# --- download_youtube_audio_improved (MODIFI√âE) ---\n",
        "# --- download_youtube_audio_improved (MODIFI√âE POUR FORCER MP3) ---\n",
        "def download_youtube_audio_improved(youtube_url, output_path):\n",
        "    \"\"\"\n",
        "    Downloads audio from YouTube using yt-dlp, FORCES conversion to MP3,\n",
        "    and retrieves metadata. Saves audio to the specified output_path.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- T√©l√©chargement et Conversion FORC√âE en MP3 depuis YouTube ---\")\n",
        "    print(f\"URL: {youtube_url}\")\n",
        "    print(f\"Destination MP3: {output_path}\") # Emphasize it will be MP3\n",
        "\n",
        "    video_info = None\n",
        "    audio_file_path = None\n",
        "    output_dir = os.path.dirname(output_path) # Dir for the audio file\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        try:\n",
        "            os.makedirs(output_dir)\n",
        "            print(f\"üìÅ Cr√©ation r√©pertoire: {output_dir}\")\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Erreur cr√©ation r√©pertoire {output_dir}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    print(\"‚ÑπÔ∏è R√©cup√©ration m√©tadonn√©es...\")\n",
        "    # ... (metadata fetching code remains the same) ...\n",
        "    try:\n",
        "        cmd = [\"yt-dlp\", \"--dump-json\", \"--encoding\", \"utf-8\", \"--socket-timeout\", \"30\", youtube_url]\n",
        "        metadata_result = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', timeout=60)\n",
        "        m = json.loads(metadata_result.stdout)\n",
        "        video_id_match = re.search(r\"v=([a-zA-Z0-9_-]+)\", youtube_url)\n",
        "        v_id = m.get(\"id\") or (video_id_match.group(1) if video_id_match else None) or \"UNKNOWN_ID\"\n",
        "\n",
        "        video_info = {\n",
        "            \"video_id\": v_id, \"channel_name\": m.get(\"uploader\", \"N/A\"),\n",
        "            \"channel_url\": m.get(\"uploader_url\", \"N/A\"), \"title\": m.get(\"title\", \"N/A\"),\n",
        "            \"description\": m.get(\"description\", \"N/A\"), \"original_url\": youtube_url,\n",
        "            \"duration\": m.get(\"duration\"), \"upload_date\": m.get(\"upload_date\"),\n",
        "        }\n",
        "        print(\"‚úÖ M√©tadonn√©es r√©cup√©r√©es.\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "         print(f\"‚ùå Timeout r√©cup√©ration m√©tadonn√©es.\")\n",
        "         return None, None\n",
        "    # ... (rest of metadata error handling) ...\n",
        "    except subprocess.CalledProcessError as e:\n",
        "         print(f\"‚ùå Erreur yt-dlp (metadata): {e}\\n{e.stderr}\")\n",
        "         return None, None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Erreur d√©codage JSON m√©tadonn√©es: {e}\")\n",
        "        return None, {\"original_url\": youtube_url, \"video_id\": \"UNKNOWN_ERROR_JSON\"}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue (m√©tadonn√©es): {e}\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"üîÑ V√©rif/T√©l√©chargement & Conversion en MP3 -> {os.path.basename(output_path)}...\")\n",
        "    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "        # Add a check to see if it's actually MP3? Maybe too complex for now. Assume correct if exists.\n",
        "        print(f\"‚úÖ Fichier MP3 existant trouv√© et non vide. Utilisation de '{os.path.basename(output_path)}'.\")\n",
        "        audio_file_path = output_path\n",
        "    else:\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"‚ÑπÔ∏è Fichier existant '{os.path.basename(output_path)}' est vide ou potentiellement pas MP3. Re-t√©l√©chargement/conversion...\")\n",
        "        else:\n",
        "            print(f\"‚ÑπÔ∏è Fichier '{os.path.basename(output_path)}' absent. T√©l√©chargement et conversion...\")\n",
        "        try:\n",
        "            # MODIFIED COMMAND: Use -x and --audio-format mp3\n",
        "            cmd = [\n",
        "                \"yt-dlp\",\n",
        "                \"-x\",                             # Extract audio track\n",
        "                \"--audio-format\", \"mp3\",          # Convert the audio to mp3\n",
        "                \"--audio-quality\", \"0\",           # Best quality for the conversion (usually ignored for -x mp3, but doesn't hurt)\n",
        "                \"--force-overwrites\",             # Overwrite if exists\n",
        "                \"-o\", output_path,                # Output path (ensure it ends with .mp3)\n",
        "                \"--encoding\", \"utf-8\",\n",
        "                \"--socket-timeout\", \"60\",         # Slightly longer timeout maybe needed for conversion\n",
        "                \"--prefer-ffmpeg\",                # Explicitly prefer ffmpeg for conversion\n",
        "                 youtube_url\n",
        "            ]\n",
        "            # Note: -f bestaudio is removed; -x implies getting the best audio available for extraction.\n",
        "            print(f\"   Commande yt-dlp (avec conversion): {' '.join(cmd)}\") # Log command\n",
        "\n",
        "            dl_res = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', timeout=1800) # 30 min timeout\n",
        "\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f\"‚úÖ Audio t√©l√©charg√© et converti en MP3 avec succ√®s.\")\n",
        "                audio_file_path = output_path\n",
        "                 # Optional: Verify it's actually MP3 using ffprobe if reliability is paramount\n",
        "                # try:\n",
        "                #    probe_cmd = [\"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"a:0\", \"-show_entries\", \"stream=codec_name\", \"-of\", \"default=noprint_wrappers=1:nokey=1\", output_path]\n",
        "                #    probe_res = subprocess.run(probe_cmd, check=True, capture_output=True, text=True)\n",
        "                #    if \"mp3\" not in probe_res.stdout.lower():\n",
        "                #        print(f\"‚ö†Ô∏è WARNING: yt-dlp finished but output file doesn't seem to be MP3! Codec found: {probe_res.stdout.strip()}\")\n",
        "                #        # Decide how to handle this? Error out or continue?\n",
        "                # except Exception as probe_e:\n",
        "                #    print(f\"‚ö†Ô∏è Could not verify audio codec with ffprobe: {probe_e}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è yt-dlp a termin√© sans erreur, mais le fichier MP3 est absent ou vide.\")\n",
        "                print(f\"   Sortie yt-dlp:\\n{dl_res.stdout}\\n{dl_res.stderr}\")\n",
        "                return None, video_info # Return metadata even if download fails here\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"‚ùå Timeout durant le t√©l√©chargement/conversion audio.\")\n",
        "            return None, video_info\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Erreur yt-dlp (t√©l√©chargement/conversion audio): {e}\")\n",
        "            print(f\"   Commande √©chou√©e: {' '.join(e.cmd)}\")\n",
        "            print(f\"   Stderr yt-dlp:\\n{e.stderr}\")\n",
        "            return None, video_info\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur inattendue (t√©l√©chargement/conversion audio): {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None, video_info\n",
        "\n",
        "    if audio_file_path and (not os.path.exists(audio_file_path) or os.path.getsize(audio_file_path) == 0):\n",
        "        print(f\"‚ùå ERREUR FINALE: Le chemin du fichier MP3 est d√©fini mais le fichier est absent ou vide.\")\n",
        "        return None, video_info\n",
        "\n",
        "    return audio_file_path, video_info\n",
        "\n",
        "\n",
        "# --- NOUVELLE FONCTION: split_audio_ffmpeg ---\n",
        "def split_audio_ffmpeg(input_audio_path, output_chunk_dir, chunk_duration_s):\n",
        "    \"\"\"\n",
        "    Splits an audio file into fixed duration chunks using ffmpeg.\n",
        "    Returns a list of chunk file paths and their corresponding start offsets in seconds.\n",
        "    Uses '-c copy' for speed, which might fail for some rare input codecs.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîä D√©coupage audio RAPIDE avec ffmpeg '{os.path.basename(input_audio_path)}' en chunks de {chunk_duration_s}s...\")\n",
        "    if not os.path.exists(input_audio_path) or os.path.getsize(input_audio_path) == 0:\n",
        "        print(f\"‚ùå Fichier audio d'entr√©e absent ou vide: {input_audio_path}\")\n",
        "        return [], []\n",
        "    if chunk_duration_s <= 0:\n",
        "        print(f\"‚ùå Dur√©e de chunk invalide ({chunk_duration_s}s). Doit √™tre positive.\")\n",
        "        return [], []\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_chunk_dir, exist_ok=True)\n",
        "\n",
        "    # Define the output pattern for ffmpeg segments\n",
        "    # Using %04d for up to 10000 chunks, adjust if more are expected\n",
        "    output_pattern = os.path.join(output_chunk_dir, \"chunk_%04d.mp3\")\n",
        "\n",
        "    # Construct the ffmpeg command\n",
        "    # -i : input file\n",
        "    # -f segment : use the segment muxer\n",
        "    # -segment_time : duration of each segment in seconds\n",
        "    # -c copy : copy the stream without re-encoding (FASTEST)\n",
        "    # -reset_timestamps 1 : start timestamps from 0 for each segment\n",
        "    # -an : No audio (useful if input accidentally had video) - NO, we want audio! Remove this.\n",
        "    # output_pattern : naming scheme for output files\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\n",
        "        \"-i\", input_audio_path,\n",
        "        \"-f\", \"segment\",\n",
        "        \"-segment_time\", str(chunk_duration_s),\n",
        "        \"-c\", \"copy\",           # Fast, but assumes input codec is compatible with mp3 container\n",
        "        \"-reset_timestamps\", \"1\",\n",
        "        \"-map\", \"0:a\",          # Ensure only audio stream is mapped\n",
        "        output_pattern\n",
        "    ]\n",
        "\n",
        "    # Alternative command if '-c copy' fails (re-encodes, slower):\n",
        "    # cmd_recode = [\n",
        "    #     \"ffmpeg\", \"-i\", input_audio_path, \"-f\", \"segment\",\n",
        "    #     \"-segment_time\", str(chunk_duration_s),\n",
        "    #     \"-reset_timestamps\", \"1\",\n",
        "    #     \"-map\", \"0:a\",\n",
        "    #     # Specify audio codec like libmp3lame, and bitrate if needed\n",
        "    #     \"-c:a\", \"libmp3lame\", \"-b:a\", \"192k\", # Example re-encode settings\n",
        "    #     output_pattern\n",
        "    # ]\n",
        "\n",
        "    print(f\"   Ex√©cution ffmpeg: {' '.join(cmd)}\")\n",
        "    start_time = time.time()\n",
        "    paths = []\n",
        "    offsets = []\n",
        "\n",
        "    try:\n",
        "        # Execute the command\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8')\n",
        "        # If check=True and it fails, CalledProcessError is raised\n",
        "\n",
        "        ffmpeg_time = time.time() - start_time\n",
        "        print(f\"   ‚úÖ Commande ffmpeg termin√©e avec succ√®s en {ffmpeg_time:.2f}s.\")\n",
        "        # print(f\"   Sortie ffmpeg (stdout):\\n{result.stdout}\") # Often empty for segment muxer\n",
        "        # print(f\"   Sortie ffmpeg (stderr):\\n{result.stderr}\") # Contains progress/details\n",
        "\n",
        "        # Find the generated chunk files\n",
        "        search_pattern = os.path.join(output_chunk_dir, \"chunk_*.mp3\")\n",
        "        generated_files = sorted(glob.glob(search_pattern))\n",
        "\n",
        "        if not generated_files:\n",
        "            print(\"‚ùå Aucun chunk n'a √©t√© g√©n√©r√© par ffmpeg (v√©rifier la sortie stderr ci-dessus).\")\n",
        "            return [], []\n",
        "\n",
        "        # Calculate offsets and validate files\n",
        "        for i, fpath in enumerate(generated_files):\n",
        "            if os.path.exists(fpath) and os.path.getsize(fpath) > 0:\n",
        "                paths.append(fpath)\n",
        "                # Offset is simply the chunk index * duration\n",
        "                offset = round(i * chunk_duration_s, 3)\n",
        "                offsets.append(offset)\n",
        "            else:\n",
        "                print(f\"     ‚ö†Ô∏è Fichier chunk g√©n√©r√© est vide ou absent: {fpath}\")\n",
        "\n",
        "        if not paths:\n",
        "            print(\"‚ùå Aucun chunk valide trouv√© apr√®s ex√©cution ffmpeg.\")\n",
        "        else:\n",
        "            print(f\"‚úÖ D√©coupage ffmpeg termin√©: {len(paths)} chunks valides cr√©√©s dans {output_chunk_dir}\")\n",
        "\n",
        "        return paths, offsets\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Erreur durant l'ex√©cution de ffmpeg:\")\n",
        "        print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "        print(f\"   Code retour: {e.returncode}\")\n",
        "        print(f\"   Stderr:\\n{e.stderr}\")\n",
        "        # Consider trying the re-encoding command here as a fallback?\n",
        "        # For now, just report failure.\n",
        "        print(\"   -> Le d√©coupage a √©chou√©. Cause possible: codec audio incompatible avec '-c copy'.\")\n",
        "        return [], []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue durant le d√©coupage ffmpeg: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return [], []\n",
        "\n",
        "\n",
        "# --- transcribe_audio_faster (INCHANG√âE) ---\n",
        "# ... (code inchang√©) ...\n",
        "def transcribe_audio_faster(file_path, model, chunk_offset_s, beam_size, vad_filter, vad_min_silence_ms):\n",
        "    \"\"\"Transcribes a single audio file chunk using the preloaded faster-whisper model.\"\"\"\n",
        "    # ... (code inchang√©) ...\n",
        "    start_time_transcribe = time.time()\n",
        "    print(f\"\\nüéôÔ∏è Transcription: {os.path.basename(file_path)} (Offset Global: {chunk_offset_s:.3f}s)\")\n",
        "\n",
        "    segments_data = []\n",
        "    total_duration = 0\n",
        "    last_prog = -1\n",
        "\n",
        "    if model is None:\n",
        "        print(\"‚ùå Mod√®le Whisper non charg√©! Impossible de transcrire.\")\n",
        "        return {\"segments\": []}\n",
        "\n",
        "    try:\n",
        "        vad_params = {\"min_silence_duration_ms\": vad_min_silence_ms} if vad_filter else None\n",
        "\n",
        "        segments_generator, info = model.transcribe(\n",
        "            file_path,\n",
        "            beam_size=beam_size,\n",
        "            vad_filter=vad_filter,\n",
        "            vad_parameters=vad_params\n",
        "        )\n",
        "\n",
        "        lang, prob, total_duration = info.language, info.language_probability, info.duration\n",
        "        print(f\"   Infos chunk d√©tect√©es: Lang='{lang}' (Conf: {prob:.2f}), Dur√©e: {total_duration:.2f}s\")\n",
        "\n",
        "        if total_duration <= 0:\n",
        "            print(\"   ‚ö†Ô∏è Dur√©e du chunk audio nulle ou n√©gative selon Whisper.\")\n",
        "            return {\"segments\": []}\n",
        "\n",
        "        print(\"   Traitement des segments...\")\n",
        "        seg_count = 0\n",
        "        for segment in segments_generator:\n",
        "            seg_count += 1\n",
        "            start_local, end_local = segment.start, segment.end\n",
        "            duration_local = max(0, end_local - start_local)\n",
        "            text = segment.text.strip() if segment.text else \"\"\n",
        "            # IMPORTANT: Ensure start_global calculation is correct\n",
        "            start_global = round(start_local + chunk_offset_s, 3) # Offset global du *chunk* + offset *local* du segment\n",
        "            duration_rounded = round(duration_local, 3)\n",
        "\n",
        "            prog = min(100.0, (end_local / total_duration) * 100) if total_duration > 0 else 0\n",
        "            rounded_prog = math.floor(prog)\n",
        "            if rounded_prog > last_prog and (rounded_prog % 10 == 0 or rounded_prog >= 99):\n",
        "                bar_len = 20\n",
        "                filled_len = int(bar_len * prog / 100)\n",
        "                bar = '‚ñà' * filled_len + '-' * (bar_len - filled_len)\n",
        "                sys.stdout.write(f\"\\r   Progression: [{bar}] {prog:.0f}% \")\n",
        "                sys.stdout.flush()\n",
        "                last_prog = rounded_prog\n",
        "\n",
        "            segments_data.append({\n",
        "                \"text\": text,\n",
        "                \"start\": start_global,\n",
        "                \"duration\": duration_rounded\n",
        "            })\n",
        "        sys.stdout.write(\"\\n\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        transcription_time = time.time() - start_time_transcribe\n",
        "        print(f\"   üïí Transcription du chunk termin√©e en {transcription_time:.2f}s. {seg_count} segments trouv√©s.\")\n",
        "        if seg_count == 0:\n",
        "            print(\"   ‚ö†Ô∏è Aucun segment de parole trouv√© dans ce chunk.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erreur durant la transcription du chunk: {e}\")\n",
        "        traceback.print_exc()\n",
        "        sys.stdout.write(\"\\n\")\n",
        "        sys.stdout.flush()\n",
        "        return {\"segments\": []}\n",
        "\n",
        "    return {\"segments\": segments_data}\n",
        "\n",
        "# ==============================================================\n",
        "# --- FONCTIONS TRADUCTION (INCHANG√âES EN ELLES-M√äMES) ---\n",
        "# ==============================================================\n",
        "print(\"\\n--- D√©finition Fonctions Traduction ---\")\n",
        "# --- chunk_list (INCHANG√âE) ---\n",
        "# ... (code inchang√©) ...\n",
        "def chunk_list(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    if not isinstance(lst, list):\n",
        "        raise TypeError(\"Input must be a list.\")\n",
        "    if n <= 0:\n",
        "        raise ValueError(\"Chunk size must be positive.\")\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "# --- extract_json_from_response (INCHANG√âE) ---\n",
        "# ... (code inchang√©) ...\n",
        "def extract_json_from_response(text_response):\n",
        "    # ... (code inchang√©) ...\n",
        "    match_fence = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', text_response, re.DOTALL)\n",
        "    if match_fence:\n",
        "        return match_fence.group(1).strip()\n",
        "    match_list = re.search(r'(\\[[^\\]]*\\])', text_response, re.DOTALL)\n",
        "    match_obj = re.search(r'(\\{[\\s\\S]*\\})', text_response, re.DOTALL)\n",
        "    json_string = None\n",
        "    first_match_pos = float('inf')\n",
        "    if match_list and match_list.start() < first_match_pos:\n",
        "        json_string = match_list.group(1)\n",
        "        first_match_pos = match_list.start()\n",
        "    if match_obj and match_obj.start() < first_match_pos:\n",
        "        json_string = match_obj.group(1)\n",
        "    if json_string:\n",
        "        return json_string.strip()\n",
        "    return text_response.strip()\n",
        "\n",
        "# --- translate_audio_chunk_segments (INCHANG√âE EN ELLE-M√äME) ---\n",
        "# ... (code inchang√©) ...\n",
        "def translate_audio_chunk_segments(transcript_segments, api_key, audio_chunk_index, total_audio_chunks, segment_chunk_index=None, total_segment_chunks=None):\n",
        "    \"\"\"\n",
        "    Translates a list of transcript segments using Google Gemini API.\n",
        "    Logs added to indicate segment chunk index if provided.\n",
        "    \"\"\"\n",
        "    # ... (code inchang√© - la fonction elle-m√™me reste la m√™me) ...\n",
        "    if not transcript_segments:\n",
        "        log_prefix = f\"   >> [Audio Chunk {audio_chunk_index + 1}/{total_audio_chunks}]\"\n",
        "        # print(f\"{log_prefix} Aucun segment √† traduire (peut-√™tre un sous-chunk vide?).\", flush=True) # Log plus discret\n",
        "        return []\n",
        "\n",
        "    start_time = time.time()\n",
        "    num_segments = len(transcript_segments)\n",
        "\n",
        "    log_prefix = f\"   >> [Audio Chunk {audio_chunk_index + 1}/{total_audio_chunks}\"\n",
        "    if segment_chunk_index is not None and total_segment_chunks is not None:\n",
        "        log_prefix += f\" | Segment Chunk {segment_chunk_index + 1}/{total_segment_chunks}\"\n",
        "    log_prefix += \"]\"\n",
        "\n",
        "    # Rendre le log initial plus discret car il y en aura potentiellement beaucoup en parall√®le\n",
        "    # print(f\"{log_prefix} üîÑ Traduction de {num_segments} segments...\", flush=True)\n",
        "\n",
        "    if not api_key or not api_key.startswith(\"AIzaSy\"):\n",
        "         print(f\"{log_prefix} ‚ùå Cl√© API Gemini invalide ou manquante.\", flush=True)\n",
        "         return None # Retourne None explicitement en cas d'erreur de cl√©\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}\"\n",
        "\n",
        "    try:\n",
        "        transcript_str = json.dumps(transcript_segments, ensure_ascii=False, separators=(',', ':'))\n",
        "    except TypeError as e:\n",
        "        print(f\"{log_prefix} ‚ùå Erreur pr√©paration JSON pour API: {e}\", flush=True)\n",
        "        return None # Retourne None en cas d'erreur\n",
        "\n",
        "    prompt = (\n",
        "        \"You are an expert multilingual transcriber and translator.\\n\"\n",
        "        \"INPUT: A JSON array. Each object in the array represents a transcript segment and has keys 'text', 'start', and 'duration'.\\n\"\n",
        "        \"TASK: Process EACH segment object in the input JSON array:\\n\"\n",
        "        \"1. Identify the original language of the 'text' field.\\n\"\n",
        "        \"2. Add a new key 'text_english' containing the English translation of the original 'text'.\\n\"\n",
        "        \"3. Add a new key 'text_french' containing the French translation of the original 'text'.\\n\"\n",
        "        \"4. IMPORTANT: Preserve ALL original keys ('text', 'start', 'duration') and their values.\\n\"\n",
        "        \"OUTPUT: Return ONLY the modified JSON array containing all processed segments. Ensure the output is a single, valid JSON array. Do NOT include any extra text, explanations, or markdown formatting (like ```json ... ```) outside the JSON array itself.\\n\\n\"\n",
        "        \"INPUT JSON:\\n\"\n",
        "        f\"{transcript_str}\"\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": 0.2,\n",
        "            \"maxOutputTokens\": 8192,\n",
        "            \"response_mime_type\": \"application/json\" # Keep requesting JSON directly\n",
        "        },\n",
        "        \"safetySettings\": [\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    max_retries = 3\n",
        "    base_delay = 3\n",
        "    raw_text_response = \"\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        if attempt > 0:\n",
        "            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
        "            # print(f\"{log_prefix} ‚è≥ Tentative Traduction {attempt + 1}/{max_retries} apr√®s {delay:.1f}s d'attente...\", flush=True)\n",
        "            time.sleep(delay)\n",
        "\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=payload, timeout=240)\n",
        "\n",
        "            if response.status_code == 429:\n",
        "                print(f\"{log_prefix} ‚ö†Ô∏è Erreur 429 (Rate Limit API). Tentative {attempt + 1}/{max_retries}. Re-essai...\", flush=True)\n",
        "                wait_time = 20 + random.uniform(0, 10) # Increased wait for 429\n",
        "                print(f\"{log_prefix}    Attente de {wait_time:.1f}s...\")\n",
        "                time.sleep(wait_time)\n",
        "                if attempt == max_retries - 1:\n",
        "                     print(f\"{log_prefix} ‚ùå Rate Limit persiste apr√®s {max_retries} tentatives.\", flush=True)\n",
        "                     return None # √âchec final apr√®s retries pour rate limit\n",
        "                continue # Passe √† la tentative suivante\n",
        "\n",
        "            response.raise_for_status() # L√®ve une exception pour les autres erreurs HTTP (4xx, 5xx)\n",
        "            response_data = response.json()\n",
        "\n",
        "            # --- Validation R√©ponse Gemini ---\n",
        "            if not response_data.get(\"candidates\"):\n",
        "                prompt_feedback = response_data.get(\"promptFeedback\", {})\n",
        "                block_reason = prompt_feedback.get(\"blockReason\")\n",
        "                safety_ratings = prompt_feedback.get(\"safetyRatings\", [])\n",
        "                error_message = f\"Aucun 'candidates' dans la r√©ponse API.\"\n",
        "                if block_reason: error_message += f\" Raison blocage: {block_reason}.\"\n",
        "                if safety_ratings: error_message += f\" Safety Ratings: {safety_ratings}\"\n",
        "                print(f\"{log_prefix} ‚ùå Erreur API Gemini: {error_message}\", flush=True)\n",
        "                # Log the full response if blocked or error for debugging\n",
        "                print(f\"{log_prefix}    R√©ponse compl√®te: {response_data}\")\n",
        "                if block_reason == \"SAFETY\": return None # Ne pas retenter pour SAFETY block\n",
        "                if attempt == max_retries - 1: return None # √âchec final apr√®s retries\n",
        "                continue # Retenter pour autres erreurs\n",
        "\n",
        "            candidate = response_data[\"candidates\"][0]\n",
        "            finish_reason = candidate.get(\"finishReason\")\n",
        "\n",
        "            if finish_reason not in [\"STOP\", \"MAX_TOKENS\"]:\n",
        "                safety_ratings = candidate.get(\"safetyRatings\", [])\n",
        "                print(f\"{log_prefix} ‚ùå Fin anormale API: {finish_reason}.\", flush=True)\n",
        "                if safety_ratings: print(f\"      -> Safety Ratings: {safety_ratings}\", flush=True)\n",
        "                # Log candidate details\n",
        "                print(f\"{log_prefix}    D√©tails candidat: {candidate}\")\n",
        "                if finish_reason == \"SAFETY\": return None # Ne pas retenter pour SAFETY finish reason\n",
        "                if attempt == max_retries - 1: return None # √âchec final\n",
        "                continue # Retenter pour autres fins anormales\n",
        "\n",
        "            if not (\"content\" in candidate and \"parts\" in candidate[\"content\"] and\n",
        "                    candidate[\"content\"][\"parts\"] and \"text\" in candidate[\"content\"][\"parts\"][0]):\n",
        "                print(f\"{log_prefix} ‚ùå Structure de contenu de r√©ponse API invalide.\", flush=True)\n",
        "                print(f\"{log_prefix}    R√©ponse candidat: {candidate}\")\n",
        "                if attempt == max_retries - 1: return None # √âchec final\n",
        "                continue # Retenter\n",
        "\n",
        "            # --- Extraction et Parsing JSON ---\n",
        "            # Avec response_mime_type=\"application/json\", le text devrait √™tre directement du JSON\n",
        "            raw_text_response = candidate[\"content\"][\"parts\"][0][\"text\"]\n",
        "            json_string = raw_text_response # Assuming it's directly JSON\n",
        "            try:\n",
        "                result_json = json.loads(json_string)\n",
        "            except json.JSONDecodeError as e:\n",
        "                 # Maybe it's wrapped in ```json ... ``` despite the mime type? Try extracting.\n",
        "                 print(f\"{log_prefix} ‚ö†Ô∏è D√©codage JSON direct √©chou√©, tentative d'extraction...\")\n",
        "                 extracted_json_string = extract_json_from_response(raw_text_response)\n",
        "                 try:\n",
        "                      result_json = json.loads(extracted_json_string)\n",
        "                      print(f\"{log_prefix} ‚úÖ Extraction et d√©codage r√©ussis apr√®s √©chec initial.\")\n",
        "                 except json.JSONDecodeError as e2:\n",
        "                     print(f\"{log_prefix} ‚ùå Erreur d√©codage JSON m√™me apr√®s extraction: {e2}\", flush=True)\n",
        "                     print(f\"      R√©ponse brute re√ßue:\\n{raw_text_response[:500]}...\")\n",
        "                     if attempt == max_retries - 1: return None # √âchec final\n",
        "                     continue # Retenter\n",
        "\n",
        "            # --- Validation R√©sultat Pars√© ---\n",
        "            if not isinstance(result_json, list):\n",
        "                print(f\"{log_prefix} ‚ùå Le JSON d√©cod√© n'est pas une liste.\", flush=True)\n",
        "                print(f\"      JSON D√©coded: {result_json}\")\n",
        "                if attempt == max_retries - 1: return None # √âchec final\n",
        "                continue # Retenter\n",
        "\n",
        "            if len(result_json) != num_segments:\n",
        "                print(f\"{log_prefix} ‚ö†Ô∏è Nombre de segments retourn√©s ({len(result_json)}) != entr√©e ({num_segments}). Possiblement tronqu√© par MAX_TOKENS ou autre?\", flush=True)\n",
        "                # Ne pas retenter juste pour √ßa, mais le signaler\n",
        "\n",
        "            # Check first segment structure (if list not empty)\n",
        "            if result_json and isinstance(result_json[0], dict):\n",
        "                 missing_keys = []\n",
        "                 if 'text_english' not in result_json[0]: missing_keys.append('text_english')\n",
        "                 if 'text_french' not in result_json[0]: missing_keys.append('text_french')\n",
        "                 if 'text' not in result_json[0]: missing_keys.append('text') # Check original keys too\n",
        "                 if 'start' not in result_json[0]: missing_keys.append('start')\n",
        "                 if 'duration' not in result_json[0]: missing_keys.append('duration')\n",
        "\n",
        "                 if missing_keys:\n",
        "                     print(f\"{log_prefix} ‚ö†Ô∏è Cl√©s manquantes dans le premier segment retourn√©: {', '.join(missing_keys)}. V√©rifier le prompt et la r√©ponse API.\", flush=True)\n",
        "                     # Don't retry automatically for this, but it's a serious warning\n",
        "\n",
        "            # --- Succ√®s ---\n",
        "            elapsed_time = time.time() - start_time\n",
        "            # print(f\"{log_prefix} ‚úÖ Traduction r√©ussie en {elapsed_time:.2f}s.\", flush=True) # Log discret\n",
        "            if finish_reason == \"MAX_TOKENS\":\n",
        "                 print(f\"{log_prefix} ‚ö†Ô∏è Attention: MAX_TOKENS atteint. Traduction pourrait √™tre incompl√®te.\", flush=True)\n",
        "            return result_json # Retourne la liste des segments traduits\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"{log_prefix} ‚ö†Ô∏è Timeout API (Tentative {attempt + 1}/{max_retries}).\", flush=True)\n",
        "            if attempt == max_retries - 1: return None # √âchec final apr√®s retries\n",
        "            # Wait before retrying timeout\n",
        "            time.sleep(base_delay * (2 ** attempt) + random.uniform(0, 1))\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            # Handle non-429 HTTP errors here\n",
        "            status_code = e.response.status_code if e.response is not None else \"N/A\"\n",
        "            print(f\"{log_prefix} ‚ùå Erreur R√©seau/HTTP API ({status_code}) (Tentative {attempt + 1}/{max_retries}): {e}\", flush=True)\n",
        "            if e.response is not None:\n",
        "                 print(f\"      R√©ponse serveur: {e.response.text[:300]}...\") # Log server response\n",
        "            if attempt == max_retries - 1: return None # √âchec final apr√®s retries\n",
        "            # Wait before retry for generic errors\n",
        "            time.sleep(base_delay * (2 ** attempt) + random.uniform(0, 1))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{log_prefix} ‚ùå Erreur inattendue durant traduction (Tentative {attempt + 1}/{max_retries}): {e}\", flush=True)\n",
        "            traceback.print_exc() # Imprime la trace pour le d√©bogage\n",
        "            if attempt == max_retries - 1: return None # √âchec final apr√®s retries\n",
        "            # Wait before retry for unexpected errors\n",
        "            time.sleep(base_delay * (2 ** attempt) + random.uniform(0, 1))\n",
        "\n",
        "    # Si la boucle se termine sans succ√®s apr√®s toutes les tentatives\n",
        "    print(f\"{log_prefix} ‚ùå Traduction √âCHOU√âE apr√®s {max_retries} tentatives.\", flush=True)\n",
        "    return None # Retourne None pour indiquer l'√©chec final\n",
        "\n",
        "# =======================================================================\n",
        "# --- Configuration et Ex√©cution Principale (AVEC FFMPEG SPLIT) ---\n",
        "# =======================================================================\n",
        "print(\"\\n\\n\" + \"=\"*40 + \"\\n--- Configuration Principale ---\\n\" + \"=\"*40)\n",
        "\n",
        "# --- Param√®tres de G√©n√©ral ---\n",
        "youtube_url = \"https://www.youtube.com/watch?v=v7RRTGdTquc\" #@param {\"type\":\"string\"}\n",
        "model_size = \"large-v3\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v3\"]\n",
        "gemini_api_key = \"AIzaSyAI2CLDtikFeKi5P6UxgXi9D9bMwYA6l8w\" #@param {\"type\":\"string\"}\n",
        "\n",
        "\n",
        "# --- Param√®tres de Pr√©-traitement Audio (MODIFI√â) ---\n",
        "# output_filename_base = \"youtube_audio.mp3\" # Base name for final json, not the temp audio\n",
        "temp_audio_path = TEMP_AUDIO_FILE # Utilise le chemin d√©fini globalement\n",
        "output_chunk_dir = CHUNK_DIR # Garde le m√™me nom pour le dossier des chunks\n",
        "\n",
        "#@markdown _____\n",
        "#@markdown ### **üöÄ Options avanc√©es**\n",
        "#@markdown *Les param√®tres suivent sont configur√©s pour un fonctionnement optimal. Si vous ne savez pas ce qu'ils font, il est g√©n√©ralement judicieux de ne pas les modifier.*\n",
        "#@markdown _____\n",
        "\n",
        "#@markdown ### Param√®tres de D√©coupage\n",
        "#@markdown *Activer pour t√©l√©charger et d√©couper l'audio en flux continu (n√©cessite `ffmpeg` et `yt-dlp`). Potentiellement plus rapide, moins d'espace disque temporaire.*\n",
        "enable_pre_splitting = True #@param {type:\"boolean\"}\n",
        "#@markdown *Utilis√© par le t√©l√©chargement stream√© OU le d√©coupage pydub classique si le streaming est d√©sactiv√©.*\n",
        "split_fixed_duration_minutes = 5 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "#@markdown _____\n",
        "#@markdown ### Param√®tres de Transcription (Whisper)\n",
        "#@markdown *Si activ√©, Whisper essaie de filtrer les silences. Utile si `split_duration_minutes` est grand.*\n",
        "use_vad_during_transcription = False #@param {type:\"boolean\"}\n",
        "vad_silence_duration_ms = 500 #@param {type:\"slider\", min:100, max:2000, step:50}\n",
        "beam_search_size = 5\n",
        "\n",
        "\n",
        "#@markdown _____\n",
        "#@markdown ### Param√®tres de Traduction (Gemini)\n",
        "#@markdown *D√©couper la liste des segments transcrits en plus petits groupes pour l'API Gemini (√©vite les erreurs de taille de prompt/r√©ponse).*\n",
        "enable_segment_chunking = True #@param {type:\"boolean\"}\n",
        "max_segments_per_translation_chunk = 30 #@param {type:\"integer\"}\n",
        "#@markdown *Nombre d'appels API Gemini simultan√©s. Augmenter acc√©l√®re la traduction mais peut atteindre les limites de taux de l'API.*\n",
        "max_concurrent_translation_tasks = 14 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown _____\n",
        "#@markdown ### Param√®tres d'Upload Incr√©mental\n",
        "upload_chunk_url = \"default\"  #@param {\"type\":\"string\"}\n",
        "if upload_chunk_url == \"default\":\n",
        "    upload_chunk_url = \"https://qingplay.pythonanywhere.com/update_transcript_chunk\"\n",
        "enable_incremental_upload = True #@param {type:\"boolean\"}\n",
        "\n",
        "# --- Variables Globales ---\n",
        "downloaded_full_audio_path = None # Renamed from original_audio_file_path for clarity\n",
        "video_info = None\n",
        "final_output_data_local = None\n",
        "json_output_filename_final = None\n",
        "loaded_whisper_model = None\n",
        "\n",
        "# --- Validation Configuration ---\n",
        "print(\"--- Validation Configuration ---\")\n",
        "# ... (inchang√©e, sauf qu'on ne valide plus output_path_original) ...\n",
        "valid_config = True\n",
        "if not youtube_url or not youtube_url.startswith(\"http\"):\n",
        "    print(\"‚ùå URL YouTube invalide.\")\n",
        "    valid_config = False\n",
        "if not gemini_api_key or gemini_api_key == \"YOUR_GEMINI_API_KEY\":\n",
        "     if enable_incremental_upload or enable_segment_chunking: # Translation needed for upload\n",
        "         print(\"‚ùå Cl√© API Gemini manquante ou non remplac√©e. La traduction et/ou l'upload √©choueront.\")\n",
        "         valid_config = False\n",
        "     else:\n",
        "         print(\"‚ö†Ô∏è Cl√© API Gemini manquante ou non remplac√©e. Traduction et upload seront ignor√©s.\")\n",
        "     gemini_api_key = None # Set to None if missing/default\n",
        "elif not gemini_api_key.startswith(\"AIzaSy\"):\n",
        "     print(f\"‚ö†Ô∏è Cl√© API Gemini ne semble pas valide (ne commence pas par AIzaSy...). Cl√© utilis√©e: '{gemini_api_key[:10]}...'\")\n",
        "     # Ne pas bloquer, mais avertir\n",
        "if enable_incremental_upload and (not upload_chunk_url or not upload_chunk_url.startswith(\"http\")):\n",
        "    print(\"‚ùå URL d'upload invalide.\")\n",
        "    valid_config = False\n",
        "if enable_segment_chunking and max_segments_per_translation_chunk <= 0:\n",
        "    print(\"‚ùå max_segments_per_translation_chunk doit √™tre positif.\")\n",
        "    valid_config = False\n",
        "if enable_segment_chunking and max_concurrent_translation_tasks <= 0:\n",
        "    print(\"‚ùå max_concurrent_translation_tasks doit √™tre positif.\")\n",
        "    valid_config = False\n",
        "\n",
        "\n",
        "if not valid_config:\n",
        "    print(\"\\nüö´ Erreurs de configuration d√©tect√©es. Arr√™t du script.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"‚úÖ Configuration valid√©e.\")\n",
        "    print(f\"   Chunking Audio (FFmpeg): {'Activ√©' if enable_pre_splitting else 'D√©sactiv√©'} (Dur√©e: {split_fixed_duration_minutes} min)\")\n",
        "    print(f\"   Mod√®le Whisper: {model_size}, VAD: {'Activ√©' if use_vad_during_transcription else 'D√©sactiv√©'}\")\n",
        "    print(f\"   Traduction : {'Activ√©e' if gemini_api_key else 'D√©sactiv√©e'}\")\n",
        "    if gemini_api_key:\n",
        "        print(f\"     Chunking Segments: {'Activ√©' if enable_segment_chunking else 'D√©sactiv√©'} (Max Segments/Chunk: {max_segments_per_translation_chunk if enable_segment_chunking else 'N/A'})\")\n",
        "        if enable_segment_chunking:\n",
        "             print(f\"     T√¢ches traduction concurrentes max: {max_concurrent_translation_tasks}\")\n",
        "    print(f\"   Upload Incr√©mental: {'Activ√©' if enable_incremental_upload else 'D√©sactiv√©'}\")\n",
        "\n",
        "\n",
        "# --- Ex√©cution ---\n",
        "try:\n",
        "    # ========================================\n",
        "    # √âTAPE 1: Pr√©paration (Download + Split) & Chargement Mod√®le\n",
        "    # ========================================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 1: PR√âPARATION & CHARGEMENT MOD√àLE\\n\" + \"=\"*40)\n",
        "    start_step1 = time.time()\n",
        "\n",
        "    # 1. Download best audio only (to temp file) and get metadata\n",
        "    downloaded_full_audio_path, video_info = download_youtube_audio_improved(youtube_url, temp_audio_path)\n",
        "\n",
        "    if not downloaded_full_audio_path or not video_info:\n",
        "        raise ValueError(\"√âchec du t√©l√©chargement audio (best quality) ou de la r√©cup√©ration des m√©tadonn√©es.\")\n",
        "\n",
        "    safe_video_id = video_info.get(\"video_id\", \"UNKNOWN_ID\").replace(\"-\", \"_\")\n",
        "\n",
        "    print(\"\\n--- Infos Vid√©o R√©cup√©r√©es ---\")\n",
        "    print(f\"   ID: {video_info.get('video_id', 'N/A')}\")\n",
        "    print(f\"   Titre: {video_info.get('title', 'N/A')}\")\n",
        "    print(f\"   Cha√Æne: {video_info.get('channel_name', 'N/A')}\")\n",
        "    print(f\"   Dur√©e: {video_info.get('duration', 'N/A')}s\")\n",
        "    print(\"----------------------------\\n\")\n",
        "\n",
        "    audio_files_to_process = []\n",
        "    chunk_offsets = []\n",
        "\n",
        "    # 2. Split the downloaded audio using FFmpeg if enabled\n",
        "    if enable_pre_splitting:\n",
        "        split_secs = split_fixed_duration_minutes * 60\n",
        "        # Utilise la nouvelle fonction de d√©coupage ffmpeg\n",
        "        paths, offs = split_audio_ffmpeg(downloaded_full_audio_path, output_chunk_dir, split_secs)\n",
        "        if paths:\n",
        "            audio_files_to_process = paths\n",
        "            chunk_offsets = offs\n",
        "            print(f\"‚úÖ Pr√©-d√©coupage (FFmpeg) activ√©. {len(paths)} chunks audio √† traiter.\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Le d√©coupage FFmpeg a √©chou√© ou n'a produit aucun chunk. Traitement du fichier audio entier t√©l√©charg√©.\")\n",
        "            audio_files_to_process = [downloaded_full_audio_path] # Fallback to full file\n",
        "            chunk_offsets = [0.0]\n",
        "            enable_pre_splitting = False # Mark as disabled for filename suffix logic\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Pr√©-d√©coupage d√©sactiv√©. Traitement du fichier audio entier t√©l√©charg√©.\")\n",
        "        audio_files_to_process = [downloaded_full_audio_path]\n",
        "        chunk_offsets = [0.0]\n",
        "\n",
        "    total_audio_chunks = len(audio_files_to_process)\n",
        "    print(f\"Nombre total de fichiers/chunks audio √† traiter: {total_audio_chunks}\")\n",
        "\n",
        "    # 3. Load Whisper Model (unchanged)\n",
        "    print(\"\\n--- Chargement Mod√®le Whisper ---\")\n",
        "    print(f\"Mod√®le demand√©: {model_size}\")\n",
        "    start_load = time.time()\n",
        "    device = \"cuda\" if IS_GPU_AVAILABLE else \"cpu\"\n",
        "    compute_type = \"default\" # Default, will be adjusted based on GPU\n",
        "\n",
        "    if IS_GPU_AVAILABLE:\n",
        "        try:\n",
        "            major, minor = torch.cuda.get_device_capability(0)\n",
        "            if major >= 8: compute_type = \"bfloat16\"; print(\"   Utilisation compute_type: bfloat16 (Ampere+)\")\n",
        "            else: compute_type = \"float16\"; print(\"   Utilisation compute_type: float16 (GPU Pre-Ampere)\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Impossible de d√©terminer la compute capability: {e}. Utilisation de float16 par d√©faut pour GPU.\")\n",
        "            compute_type = \"float16\"\n",
        "    else: compute_type = \"int8\"; print(\"   Utilisation compute_type: int8 (CPU)\")\n",
        "\n",
        "    try:\n",
        "        loaded_whisper_model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "        load_time = time.time() - start_load\n",
        "        print(f\"‚úÖ Mod√®le '{model_size}' charg√© sur {device} ({compute_type}) en {load_time:.2f}s.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå √âchec chargement mod√®le Whisper: {e}\")\n",
        "        # Try fallback compute type if on GPU? e.g., float16 if bfloat16 failed\n",
        "        if IS_GPU_AVAILABLE and compute_type == \"bfloat16\":\n",
        "            print(\"   Tentative de fallback vers float16...\")\n",
        "            compute_type = \"float16\"\n",
        "            try:\n",
        "                 loaded_whisper_model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "                 load_time = time.time() - start_load\n",
        "                 print(f\"‚úÖ Mod√®le '{model_size}' charg√© sur {device} (float16 fallback) en {load_time:.2f}s.\")\n",
        "            except Exception as e2:\n",
        "                 raise ValueError(f\"√âchec chargement mod√®le Whisper m√™me avec float16 fallback: {e2}\")\n",
        "        else:\n",
        "            raise ValueError(f\"√âchec chargement mod√®le Whisper: {e}\")\n",
        "\n",
        "\n",
        "    step1_time = time.time() - start_step1\n",
        "    print(f\"‚è±Ô∏è Temps √âtape 1 (Pr√©paration + Chargement Mod√®le): {step1_time:.2f}s\")\n",
        "\n",
        "\n",
        "    # ================================================\n",
        "    # √âTAPE 2: Boucle de Traitement & Upload Intercal√© (Logique interne inchang√©e, utilise chunks ffmpeg)\n",
        "    # ================================================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 2: TRAITEMENT & UPLOAD INTERCAL√âS\\n\" + \"=\"*40)\n",
        "    all_final_segments_local = []\n",
        "    total_transcribed_segments = 0\n",
        "    total_translated_segments_ok = 0\n",
        "    successful_uploads = 0\n",
        "    failed_uploads = 0\n",
        "    failed_translation_audio_chunks = [] # Stores indices (1-based) of audio chunks where translation failed\n",
        "    global_start_proc = time.time()\n",
        "\n",
        "    if not audio_files_to_process:\n",
        "        print(\"üö´ Aucun fichier audio √† traiter (√âchec t√©l√©chargement ou d√©coupage).\")\n",
        "    else:\n",
        "        for i, file_path in enumerate(audio_files_to_process):\n",
        "            chunk_start_time = time.time()\n",
        "            current_offset = chunk_offsets[i] # Get the pre-calculated offset for this chunk\n",
        "            print(f\"\\n--- Traitement Chunk Audio {i + 1}/{total_audio_chunks}: {os.path.basename(file_path)} (Offset: {current_offset:.3f}s) ---\")\n",
        "\n",
        "            # --- 2.1 Transcription (utilise le chunk et son offset) ---\n",
        "            transcript_result = transcribe_audio_faster(\n",
        "                file_path, loaded_whisper_model, current_offset, # Passe l'offset du chunk\n",
        "                beam_search_size, use_vad_during_transcription, vad_silence_duration_ms\n",
        "            )\n",
        "            current_chunk_segments_transcribed = transcript_result.get(\"segments\", [])\n",
        "\n",
        "            if not current_chunk_segments_transcribed:\n",
        "                print(f\"   -> Aucun segment transcrit pour ce chunk audio.\")\n",
        "                chunk_end_time = time.time()\n",
        "                print(f\"   ‚è±Ô∏è Temps traitement Chunk Audio {i+1}: {chunk_end_time - chunk_start_time:.2f}s\")\n",
        "                continue # Passe au chunk audio suivant\n",
        "\n",
        "            num_transcribed = len(current_chunk_segments_transcribed)\n",
        "            total_transcribed_segments += num_transcribed\n",
        "            print(f\"   -> Transcrit {num_transcribed} segments.\")\n",
        "\n",
        "            # --- 2.2 Traduction (avec gestion concurrence si activ√©e) ---\n",
        "            aggregated_translated_segments_for_audio_chunk = [] # Segments for THIS audio chunk AFTER potential translation\n",
        "            translation_failed_for_this_audio_chunk = False\n",
        "            translation_api_calls = 0\n",
        "            translation_start_time = time.time()\n",
        "\n",
        "            if not gemini_api_key:\n",
        "                print(\"   >> ‚ö†Ô∏è Cl√© API Gemini non fournie. Traduction ignor√©e.\")\n",
        "                # Use transcribed segments directly for local aggregation and potential later upload if logic changes\n",
        "                aggregated_translated_segments_for_audio_chunk = current_chunk_segments_transcribed\n",
        "            else:\n",
        "                # --- 2.2.1 Segment Chunking & Concurrent Execution ---\n",
        "                if enable_segment_chunking:\n",
        "                    segment_sub_chunks = list(chunk_list(current_chunk_segments_transcribed, max_segments_per_translation_chunk))\n",
        "                    total_segment_sub_chunks = len(segment_sub_chunks)\n",
        "                    print(f\"   >> Pr√©paration traduction pour {num_transcribed} segments en {total_segment_sub_chunks} sous-chunk(s) API.\")\n",
        "                    print(f\"      (Utilisation de max {max_concurrent_translation_tasks} workers concurrents)\")\n",
        "\n",
        "                    futures_results = {} # Key: sub-chunk index (j), Value: list of segments or None\n",
        "                    futures_map = {} # Key: Future, Value: sub-chunk index (j)\n",
        "\n",
        "                    with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_translation_tasks, thread_name_prefix=f'TranslateChunk_{i+1}') as executor:\n",
        "                        # Submit translation tasks\n",
        "                        for j, segment_sub_chunk in enumerate(segment_sub_chunks):\n",
        "                             if segment_sub_chunk: # Ensure sub-chunk is not empty\n",
        "                                 future = executor.submit(\n",
        "                                     translate_audio_chunk_segments,\n",
        "                                     segment_sub_chunk, gemini_api_key,\n",
        "                                     i, total_audio_chunks, # Audio chunk indices\n",
        "                                     j, total_segment_sub_chunks # Segment sub-chunk indices\n",
        "                                 )\n",
        "                                 futures_map[future] = j\n",
        "                             else:\n",
        "                                 futures_results[j] = [] # Empty input -> empty output\n",
        "\n",
        "                        translation_api_calls = len(futures_map) # Actual API calls submitted\n",
        "                        print(f\"      >> {translation_api_calls} t√¢ches de traduction soumises...\")\n",
        "\n",
        "                        # Process results as they complete\n",
        "                        processed_count = 0\n",
        "                        for future in concurrent.futures.as_completed(futures_map):\n",
        "                             j_index = futures_map[future]\n",
        "                             try:\n",
        "                                 result = future.result() # Get result (list or None)\n",
        "                                 futures_results[j_index] = result\n",
        "                                 if result is None:\n",
        "                                      translation_failed_for_this_audio_chunk = True\n",
        "                                      # Error logged within the function\n",
        "                                 processed_count += 1\n",
        "                                 # Progress indicator\n",
        "                                 sys.stdout.write(f\"\\r      >> Progression traduction (t√¢ches termin√©es): {processed_count}/{translation_api_calls} \")\n",
        "                                 sys.stdout.flush()\n",
        "\n",
        "                             except Exception as exc:\n",
        "                                 print(f'\\n      >> ‚ùå Erreur non intercept√©e lors de la traduction du sous-chunk {j_index + 1}: {exc}')\n",
        "                                 # traceback.print_exc() # Optionally print full traceback\n",
        "                                 futures_results[j_index] = None # Mark as failed\n",
        "                                 translation_failed_for_this_audio_chunk = True\n",
        "                                 processed_count += 1\n",
        "                                 sys.stdout.write(f\"\\r      >> Progression traduction (t√¢ches termin√©es): {processed_count}/{translation_api_calls} \")\n",
        "                                 sys.stdout.flush()\n",
        "\n",
        "                        sys.stdout.write(\"\\n\") # Newline after progress\n",
        "                        sys.stdout.flush()\n",
        "\n",
        "                    # Aggregate results in the original order\n",
        "                    for j in range(total_segment_sub_chunks):\n",
        "                         result_sub_chunk = futures_results.get(j) # Get result for sub-chunk j\n",
        "                         if result_sub_chunk is not None: # If translation succeeded for this sub-chunk\n",
        "                              aggregated_translated_segments_for_audio_chunk.extend(result_sub_chunk)\n",
        "                         # If None, failure already marked, don't add anything for this sub-chunk\n",
        "\n",
        "                # --- 2.2.2 No Segment Chunking (Sequential) ---\n",
        "                else:\n",
        "                     print(f\"   >> Traduction s√©quentielle de {num_transcribed} segments (Chunking Segment d√©sactiv√©).\")\n",
        "                     translation_api_calls = 1\n",
        "                     translated_segments = translate_audio_chunk_segments(\n",
        "                         current_chunk_segments_transcribed, gemini_api_key,\n",
        "                         i, total_audio_chunks\n",
        "                     )\n",
        "                     if translated_segments is not None:\n",
        "                         aggregated_translated_segments_for_audio_chunk = translated_segments\n",
        "                     else:\n",
        "                         translation_failed_for_this_audio_chunk = True\n",
        "                         # Error logged within the function\n",
        "\n",
        "                translation_end_time = time.time()\n",
        "                translation_duration = translation_end_time - translation_start_time\n",
        "                status_msg = \"√âCHEC\" if translation_failed_for_this_audio_chunk else \"Termin√©e\"\n",
        "                print(f\"   >> Traduction {status_msg} pour Audio Chunk {i+1} en {translation_duration:.2f}s ({translation_api_calls} appel(s) API).\")\n",
        "\n",
        "\n",
        "            # --- Post-Processing (Aggregation for Local Save, Stats) ---\n",
        "            if translation_failed_for_this_audio_chunk:\n",
        "                failed_translation_audio_chunks.append(i + 1) # Record failed audio chunk index (1-based)\n",
        "                print(f\"   >> ‚ùóÔ∏è Traduction √©chou√©e pour Audio Chunk {i+1}. Sauvegarde locale utilisera les segments transcrits originaux.\")\n",
        "                # Aggregate the *original* transcribed segments for local save fallback\n",
        "                all_final_segments_local.extend(current_chunk_segments_transcribed)\n",
        "            elif gemini_api_key: # Translation was attempted and succeeded\n",
        "                 num_translated_in_chunk = len(aggregated_translated_segments_for_audio_chunk)\n",
        "                 total_translated_segments_ok += num_translated_in_chunk\n",
        "                 all_final_segments_local.extend(aggregated_translated_segments_for_audio_chunk)\n",
        "                 if num_translated_in_chunk != num_transcribed:\n",
        "                     print(f\"   >> ‚ö†Ô∏è {num_translated_in_chunk} segments dans le r√©sultat traduit vs {num_transcribed} transcrits (possible perte/troncature API).\")\n",
        "                 else:\n",
        "                     print(f\"   >> {num_translated_in_chunk} segments traduits agr√©g√©s pour Audio Chunk {i+1}.\")\n",
        "            else: # No translation attempted (no API key)\n",
        "                 all_final_segments_local.extend(aggregated_translated_segments_for_audio_chunk) # Which are the transcribed ones\n",
        "                 print(\"   >> Segments transcrits ajout√©s √† la sauvegarde locale (traduction d√©sactiv√©e).\")\n",
        "\n",
        "\n",
        "            # --- 2.3 Upload Incr√©mental (uses the aggregated_translated_segments_for_audio_chunk) ---\n",
        "            # Upload only if enabled, API key exists, and translation for THIS chunk succeeded (or wasn't needed)\n",
        "            can_upload = enable_incremental_upload and gemini_api_key and not translation_failed_for_this_audio_chunk\n",
        "\n",
        "            if can_upload and aggregated_translated_segments_for_audio_chunk:\n",
        "                print(f\"      >> ‚¨ÜÔ∏è Tentative d'upload pour Chunk Audio {i + 1} ({len(aggregated_translated_segments_for_audio_chunk)} segments)...\", flush=True)\n",
        "                upload_payload = {\n",
        "                    \"video_id\": video_info.get(\"video_id\", \"N/A\"), \"description\": video_info.get(\"description\", \"N/A\"),\n",
        "                    \"channel_name\": video_info.get(\"channel_name\", \"N/A\"), \"channel_url\": video_info.get(\"channel_url\", \"N/A\"),\n",
        "                    \"segments\": aggregated_translated_segments_for_audio_chunk,\n",
        "                    \"chunk_index\": i, \"total_chunks\": total_audio_chunks,\n",
        "                    # Send title only with the first chunk\n",
        "                    \"title\": video_info.get(\"title\", \"N/A\") if i == 0 else None,\n",
        "                    # Add duration only with the first chunk? Or maybe always? Send always for now.\n",
        "                    \"duration\": video_info.get(\"duration\")\n",
        "                }\n",
        "                upload_success = False\n",
        "                upload_retries = 2\n",
        "                for upload_attempt in range(upload_retries):\n",
        "                    try:\n",
        "                        response = requests.post(upload_chunk_url, json=upload_payload, timeout=90) # Increased timeout\n",
        "                        response.raise_for_status() # Check for HTTP errors\n",
        "                        try: server_message = response.json().get(\"message\", response.text[:100])\n",
        "                        except json.JSONDecodeError: server_message = response.text[:100]\n",
        "                        print(f\"      >> ‚úÖ Upload Chunk Audio {i + 1} r√©ussi (Tentative {upload_attempt+1}, Status: {response.status_code}). Serveur: {server_message}\", flush=True)\n",
        "                        successful_uploads += 1\n",
        "                        upload_success = True\n",
        "                        break # Exit retry loop on success\n",
        "                    except requests.exceptions.Timeout:\n",
        "                        print(f\"      >> ‚ö†Ô∏è Upload Chunk Audio {i + 1} TIMEOUT (Tentative {upload_attempt+1}/{upload_retries}).\", flush=True)\n",
        "                        if upload_attempt == upload_retries - 1: failed_uploads += 1\n",
        "                        else: time.sleep(5) # Wait before retry\n",
        "                    except requests.exceptions.RequestException as e:\n",
        "                        status_code = e.response.status_code if e.response is not None else \"N/A\"\n",
        "                        print(f\"      >> ‚ùå Upload Chunk Audio {i + 1} √©chou√© (Erreur HTTP/R√©seau {status_code}, Tentative {upload_attempt+1}/{upload_retries}): {e}\", flush=True)\n",
        "                        if e.response is not None: print(f\"         R√©ponse serveur: {e.response.text[:200]}...\")\n",
        "                        if upload_attempt == upload_retries - 1: failed_uploads += 1\n",
        "                        else: time.sleep(5 + random.uniform(0, 5)) # Wait before retry\n",
        "                    except Exception as e:\n",
        "                        print(f\"      >> ‚ùå Erreur inattendue pendant upload Chunk Audio {i + 1} (Tentative {upload_attempt+1}): {e}\", flush=True); traceback.print_exc()\n",
        "                        if upload_attempt == upload_retries - 1: failed_uploads += 1\n",
        "                        break # Don't retry unknown errors usually\n",
        "            elif not enable_incremental_upload: print(f\"      >> ‚ÑπÔ∏è Upload incr√©mental d√©sactiv√©.\")\n",
        "            elif not gemini_api_key: print(f\"      >> ‚ÑπÔ∏è Upload ignor√© (traduction d√©sactiv√©e/cl√© API manquante).\")\n",
        "            elif translation_failed_for_this_audio_chunk: print(f\"      >> ‚ÑπÔ∏è Upload ignor√© pour ce chunk (√©chec traduction).\")\n",
        "            elif not aggregated_translated_segments_for_audio_chunk: print(f\"      >> ‚ÑπÔ∏è Aucun segment √† uploader pour ce chunk.\")\n",
        "\n",
        "            chunk_end_time = time.time()\n",
        "            print(f\"   ‚è±Ô∏è Temps total traitement Chunk Audio {i+1}: {chunk_end_time - chunk_start_time:.2f}s\")\n",
        "\n",
        "    global_end_proc = time.time()\n",
        "\n",
        "    # --- R√©sum√© Traitement ---\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 2: R√âSUM√â DU TRAITEMENT\\n\" + \"=\"*40)\n",
        "    total_proc_time = global_end_proc - global_start_proc\n",
        "    print(f\"‚è±Ô∏è Temps total Traitement & Upload (√âtape 2): {total_proc_time:.2f}s.\")\n",
        "    print(f\"üìä Total Chunks Audio Trait√©s: {total_audio_chunks}\")\n",
        "    print(f\"üìä Total Segments Transcrits: {total_transcribed_segments}\")\n",
        "    if gemini_api_key:\n",
        "        print(f\"üìä Total Segments agr√©g√©s apr√®s traduction r√©ussie: {total_translated_segments_ok}\")\n",
        "        failed_chunks_list = sorted(list(set(failed_translation_audio_chunks)))\n",
        "        if failed_chunks_list:\n",
        "             print(f\"‚ùå Traduction √©chou√©e (partiellement ou totalement) pour {len(failed_chunks_list)} chunk(s) audio: {', '.join(map(str, failed_chunks_list))}\")\n",
        "             print(f\"   (Sauvegarde locale utilise fallback transcrit pour eux)\")\n",
        "        elif total_transcribed_segments > 0 : print(f\"‚úÖ Traduction r√©ussie pour tous les chunks audio avec segments.\")\n",
        "        else: print(f\"‚ÑπÔ∏è Aucune traduction n√©cessaire (pas de segments transcrits).\")\n",
        "    else: print(\"‚ÑπÔ∏è Traduction ignor√©e (pas de cl√© API).\")\n",
        "    print(f\"üíæ Total Segments agr√©g√©s pour sauvegarde locale: {len(all_final_segments_local)}\")\n",
        "    if enable_incremental_upload:\n",
        "        print(f\"‚òÅÔ∏è Uploads Incr√©mentaux R√©ussis (par chunk audio): {successful_uploads}\")\n",
        "        print(f\"‚òÅÔ∏è Uploads Incr√©mentaux √âchou√©s (par chunk audio): {failed_uploads}\")\n",
        "    else: print(\"‚òÅÔ∏è Upload Incr√©mental d√©sactiv√©.\")\n",
        "\n",
        "\n",
        "    # ========================================\n",
        "    # √âTAPE 3: SAUVEGARDE LOCALE FINALE (Logique inchang√©e, utilise les nouveaux suffixes)\n",
        "    # ========================================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 3: SAUVEGARDE LOCALE FINALE\\n\" + \"=\"*40)\n",
        "    start_step3 = time.time()\n",
        "    if all_final_segments_local:\n",
        "        final_output_data_local = {\"segments\": all_final_segments_local}\n",
        "        if video_info:\n",
        "             # Add relevant metadata to the final JSON\n",
        "             metadata_to_include = [\"video_id\", \"title\", \"channel_name\", \"channel_url\", \"duration\", \"original_url\", \"upload_date\"]\n",
        "             final_output_data_local[\"metadata\"] = {k: v for k, v in video_info.items() if k in metadata_to_include and v is not None}\n",
        "\n",
        "        # --- Build Filename Suffixes ---\n",
        "        # Splitting Suffix\n",
        "        split_suffix = f\"_splitFFMPEG{split_fixed_duration_minutes}min\" if enable_pre_splitting else \"_noSplit\"\n",
        "        # VAD Suffix\n",
        "        vad_suffix = \"_VAD\" if use_vad_during_transcription else \"_noVAD\"\n",
        "        # Translation Config Suffix\n",
        "        segchunk_suffix = \"\"\n",
        "        if gemini_api_key: # Only relevant if translation was attempted\n",
        "            if enable_segment_chunking:\n",
        "                segchunk_suffix = f\"_segChunk{max_segments_per_translation_chunk}_conc{max_concurrent_translation_tasks}\"\n",
        "            else:\n",
        "                segchunk_suffix = \"_segFull\" # Translation attempted on full segment list per audio chunk\n",
        "        # Translation Status Suffix\n",
        "        failed_chunks_list = sorted(list(set(failed_translation_audio_chunks)))\n",
        "        if not gemini_api_key: status_suffix = \"_transcribed_only\"\n",
        "        elif not failed_chunks_list and total_transcribed_segments > 0: status_suffix = \"_fully_translated\"\n",
        "        elif total_transcribed_segments == 0: status_suffix = \"_no_segments_found\" # No segments to transcribe/translate\n",
        "        elif failed_chunks_list and len(all_final_segments_local) == total_transcribed_segments : status_suffix = \"_translation_failed_fallback\" # Translation failed for all chunks with segments\n",
        "        elif failed_chunks_list: status_suffix = \"_partially_translated_fallback\" # Some failed, some succeeded\n",
        "        else: status_suffix = \"_unknown_state\" # Should not happen\n",
        "\n",
        "        # Final Filename\n",
        "        json_output_filename_final = os.path.join(\n",
        "            OUTPUT_DIR_V2,\n",
        "            f\"{safe_video_id}__{model_size}{split_suffix}{vad_suffix}{segchunk_suffix}{status_suffix}.json\"\n",
        "        )\n",
        "\n",
        "        print(f\"Tentative de sauvegarde du r√©sultat local agr√©g√© vers:\")\n",
        "        print(f\"   {json_output_filename_final}\")\n",
        "        try:\n",
        "            with open(json_output_filename_final, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(final_output_data_local, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"‚úÖ R√©sultat local complet sauvegard√© avec succ√®s.\")\n",
        "            if failed_uploads > 0: print(f\"   -> ‚ö†Ô∏è Attention: {failed_uploads} upload(s) incr√©mentaux ont √©chou√©.\")\n",
        "            if failed_chunks_list: print(f\"   -> ‚ÑπÔ∏è Ce fichier contient les segments transcrits originaux pour les chunks audio ({', '.join(map(str, failed_chunks_list))}) dont la traduction a √©chou√©.\")\n",
        "        except IOError as e: print(f\"‚ùå Erreur d'√©criture lors de la sauvegarde JSON locale: {e}\")\n",
        "        except Exception as e: print(f\"‚ùå Erreur inattendue lors de la sauvegarde JSON locale: {e}\"); traceback.print_exc()\n",
        "    else:\n",
        "        print(\"‚ùå Aucune donn√©e de segment n'a √©t√© collect√©e (√©chec transcription/traitement?). Pas de fichier JSON local sauvegard√©.\")\n",
        "\n",
        "    step3_time = time.time() - start_step3\n",
        "    print(f\"‚è±Ô∏è Temps √âtape 3 (Sauvegarde): {step3_time:.2f}s\")\n",
        "\n",
        "\n",
        "except ValueError as ve: print(f\"\\n‚ùå ERREUR DE CONFIGURATION OU DE PROCESSUS: {ve}\"); traceback.print_exc()\n",
        "except Exception as e: print(f\"\\n‚ùå ERREUR GLOBALE INATTENDUE: {e}\"); traceback.print_exc()\n",
        "finally:\n",
        "    # ===========================\n",
        "    # √âTAPE 4: NETTOYAGE (Optionnel)\n",
        "    # ===========================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 4: NETTOYAGE (OPTIONNEL)\\n\" + \"=\"*40)\n",
        "    cleanup_intermediate = False #@param {type:\"boolean\"} # Set to True to delete temp audio and chunks\n",
        "    if cleanup_intermediate:\n",
        "        print(\"--- Nettoyage des fichiers interm√©diaires ---\")\n",
        "        # Delete the downloaded full audio file\n",
        "        if downloaded_full_audio_path and os.path.exists(downloaded_full_audio_path):\n",
        "            try: os.remove(downloaded_full_audio_path); print(f\"üóëÔ∏è Fichier audio temporaire complet supprim√©: {downloaded_full_audio_path}\")\n",
        "            except Exception as e: print(f\"‚ùå Erreur suppression fichier audio temporaire: {e}\")\n",
        "        # Delete the chunk directory\n",
        "        if os.path.exists(output_chunk_dir):\n",
        "             try: shutil.rmtree(output_chunk_dir); print(f\"üóëÔ∏è Dossier des chunks audio supprim√©: {output_chunk_dir}\")\n",
        "             except Exception as e: print(f\"‚ùå Erreur suppression dossier chunks: {e}\")\n",
        "        print(\"--- Nettoyage interm√©diaire termin√© ---\")\n",
        "    else:\n",
        "        print(\"--- Nettoyage interm√©diaire d√©sactiv√© ---\")\n",
        "        if downloaded_full_audio_path and os.path.exists(downloaded_full_audio_path): print(f\"‚ÑπÔ∏è Fichier audio temporaire complet conserv√©: {downloaded_full_audio_path}\")\n",
        "        if os.path.exists(output_chunk_dir) and os.listdir(output_chunk_dir): print(f\"‚ÑπÔ∏è Chunks audio conserv√©s dans: {output_chunk_dir}\")\n",
        "\n",
        "    # Final JSON file is kept regardless of the cleanup_intermediate flag\n",
        "    if json_output_filename_final and os.path.exists(json_output_filename_final): print(f\"\\n‚ÑπÔ∏è Fichier JSON final conserv√©: {json_output_filename_final}\")\n",
        "    elif 'all_final_segments_local' in locals() and all_final_segments_local: print(\"\\n‚ö†Ô∏è Fichier JSON final n'a pas pu √™tre sauvegard√©.\")\n",
        "\n",
        "\n",
        "    print(\"\\nüèÅ Script complet termin√©.\")\n",
        "    total_runtime = 0\n",
        "    if 'start_step1' in locals():\n",
        "        total_runtime = time.time() - start_step1\n",
        "        print(f\"‚è±Ô∏è Dur√©e totale d'ex√©cution du script (toutes √©tapes): {total_runtime:.2f}s\")\n",
        "    elif 'global_start_proc' in locals():\n",
        "         total_runtime = time.time() - global_start_proc\n",
        "         print(f\"‚è±Ô∏è Dur√©e d'ex√©cution (√âtape 2 et suivantes): {total_runtime:.2f}s\")\n",
        "\n",
        "    if video_info and video_info.get(\"video_id\") != \"UNKNOWN_ID\":\n",
        "        vid_id = video_info['video_id']\n",
        "        if enable_incremental_upload and successful_uploads > 0:\n",
        "            print(f\"\\nüîó Lien potentiel pour consulter le r√©sultat sur le serveur:\")\n",
        "            print(f\"   https://qingplay.pythonanywhere.com/vid/{vid_id}\")\n",
        "            if failed_uploads > 0:\n",
        "                 print(f\"   ‚ö†Ô∏è Attention: {failed_uploads} upload(s) ont √©chou√©, le r√©sultat sur le serveur peut √™tre incomplet.\")\n",
        "        elif enable_incremental_upload and failed_uploads > 0:\n",
        "            print(f\"\\n‚òÅÔ∏è Tous les uploads ont √©chou√©. V√©rifiez les logs d'upload et la connexion.\")\n",
        "        elif enable_incremental_upload and successful_uploads == 0 and total_transcribed_segments > 0 and gemini_api_key and not failed_translation_audio_chunks:\n",
        "             print(f\"\\n‚òÅÔ∏è Aucun upload r√©ussi malgr√© des traductions r√©ussies. V√©rifiez les logs d'upload.\")\n",
        "        elif enable_incremental_upload and not gemini_api_key:\n",
        "            print(f\"\\n‚òÅÔ∏è Upload incr√©mental activ√© mais aucune cl√© API Gemini fournie (upload ignor√©).\")"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "jxsqd_YMyana",
        "outputId": "fce9e140-0899-4253-f1e1-2cd9a3f25759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Cleanup ---\n",
            "Cleanup done.\n",
            "\n",
            "--- Library Imports & Checks ---\n",
            "‚úÖ PyTorch install√©.\n",
            "‚úÖ faster-whisper install√©.\n",
            "‚úÖ pydub install√© (peut ne plus √™tre utilis√© pour le d√©coupage).\n",
            "\n",
            "--- GPU Check ---\n",
            "‚úÖ GPU d√©tect√©: True\n",
            "   GPU Name: Tesla T4\n",
            "   Compute Capability: 7.5\n",
            "\n",
            "--- Directories ---\n",
            "‚úÖ Dossier chunks pr√™t: /content/audio_chunks\n",
            "‚úÖ Dossier sortie pr√™t: /content/audio_output_optimized_v2\n",
            "\n",
            "--- D√©finition Fonctions Utilitaires (Pr√©-traitement) ---\n",
            "\n",
            "--- D√©finition Fonctions Traduction ---\n",
            "\n",
            "\n",
            "========================================\n",
            "--- Configuration Principale ---\n",
            "========================================\n",
            "--- Validation Configuration ---\n",
            "‚úÖ Configuration valid√©e.\n",
            "   Chunking Audio (FFmpeg): Activ√© (Dur√©e: 10 min)\n",
            "   Mod√®le Whisper: large-v3, VAD: D√©sactiv√©\n",
            "   Traduction : Activ√©e\n",
            "     Chunking Segments: Activ√© (Max Segments/Chunk: 30)\n",
            "     T√¢ches traduction concurrentes max: 14\n",
            "   Upload Incr√©mental: Activ√©\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 1: PR√âPARATION & CHARGEMENT MOD√àLE\n",
            "========================================\n",
            "\n",
            "--- T√©l√©chargement et Conversion FORC√âE en MP3 depuis YouTube ---\n",
            "URL: https://www.youtube.com/watch?v=v7RRTGdTquc\n",
            "Destination MP3: /content/temp_downloaded_audio.mp3\n",
            "‚ÑπÔ∏è R√©cup√©ration m√©tadonn√©es...\n",
            "‚úÖ M√©tadonn√©es r√©cup√©r√©es.\n",
            "üîÑ V√©rif/T√©l√©chargement & Conversion en MP3 -> temp_downloaded_audio.mp3...\n",
            "‚ÑπÔ∏è Fichier 'temp_downloaded_audio.mp3' absent. T√©l√©chargement et conversion...\n",
            "   Commande yt-dlp (avec conversion): yt-dlp -x --audio-format mp3 --audio-quality 0 --force-overwrites -o /content/temp_downloaded_audio.mp3 --encoding utf-8 --socket-timeout 60 --prefer-ffmpeg https://www.youtube.com/watch?v=v7RRTGdTquc\n",
            "‚úÖ Audio t√©l√©charg√© et converti en MP3 avec succ√®s.\n",
            "\n",
            "--- Infos Vid√©o R√©cup√©r√©es ---\n",
            "   ID: v7RRTGdTquc\n",
            "   Titre: Our FIRST 24 hours in China‚Äôs futuristic megacity‚Ä¶ üá®üá≥ | Suzhou, China\n",
            "   Cha√Æne: Pat & Giulia\n",
            "   Dur√©e: 1786s\n",
            "----------------------------\n",
            "\n",
            "\n",
            "üîä D√©coupage audio RAPIDE avec ffmpeg 'temp_downloaded_audio.mp3' en chunks de 600s...\n",
            "   Ex√©cution ffmpeg: ffmpeg -i /content/temp_downloaded_audio.mp3 -f segment -segment_time 600 -c copy -reset_timestamps 1 -map 0:a /content/audio_chunks/chunk_%04d.mp3\n",
            "   ‚úÖ Commande ffmpeg termin√©e avec succ√®s en 0.73s.\n",
            "‚úÖ D√©coupage ffmpeg termin√©: 3 chunks valides cr√©√©s dans /content/audio_chunks\n",
            "‚úÖ Pr√©-d√©coupage (FFmpeg) activ√©. 3 chunks audio √† traiter.\n",
            "Nombre total de fichiers/chunks audio √† traiter: 3\n",
            "\n",
            "--- Chargement Mod√®le Whisper ---\n",
            "Mod√®le demand√©: large-v3\n",
            "   Utilisation compute_type: float16 (GPU Pre-Ampere)\n",
            "‚úÖ Mod√®le 'large-v3' charg√© sur cuda (float16) en 2.60s.\n",
            "‚è±Ô∏è Temps √âtape 1 (Pr√©paration + Chargement Mod√®le): 53.74s\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 2: TRAITEMENT & UPLOAD INTERCAL√âS\n",
            "========================================\n",
            "\n",
            "--- Traitement Chunk Audio 1/3: chunk_0000.mp3 (Offset: 0.000s) ---\n",
            "\n",
            "üéôÔ∏è Transcription: chunk_0000.mp3 (Offset Global: 0.000s)\n",
            "   Infos chunk d√©tect√©es: Lang='en' (Conf: 0.98), Dur√©e: 599.98s\n",
            "   Traitement des segments...\n",
            "   Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà-] 99% \n",
            "   üïí Transcription du chunk termin√©e en 128.49s. 104 segments trouv√©s.\n",
            "   -> Transcrit 104 segments.\n",
            "   >> Pr√©paration traduction pour 104 segments en 4 sous-chunk(s) API.\n",
            "      (Utilisation de max 14 workers concurrents)\n",
            "      >> 4 t√¢ches de traduction soumises...\n",
            "      >> Progression traduction (t√¢ches termin√©es): 4/4 \n",
            "   >> Traduction Termin√©e pour Audio Chunk 1 en 20.17s (4 appel(s) API).\n",
            "   >> 104 segments traduits agr√©g√©s pour Audio Chunk 1.\n",
            "      >> ‚¨ÜÔ∏è Tentative d'upload pour Chunk Audio 1 (104 segments)...\n",
            "      >> ‚úÖ Upload Chunk Audio 1 r√©ussi (Tentative 1, Status: 201). Serveur: New transcript created successfully\n",
            "   ‚è±Ô∏è Temps total traitement Chunk Audio 1: 149.21s\n",
            "\n",
            "--- Traitement Chunk Audio 2/3: chunk_0001.mp3 (Offset: 600.000s) ---\n",
            "\n",
            "üéôÔ∏è Transcription: chunk_0001.mp3 (Offset Global: 600.000s)\n",
            "   Infos chunk d√©tect√©es: Lang='en' (Conf: 0.98), Dur√©e: 599.99s\n",
            "   Traitement des segments...\n",
            "   Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% \n",
            "   üïí Transcription du chunk termin√©e en 142.98s. 193 segments trouv√©s.\n",
            "   -> Transcrit 193 segments.\n",
            "   >> Pr√©paration traduction pour 193 segments en 7 sous-chunk(s) API.\n",
            "      (Utilisation de max 14 workers concurrents)\n",
            "      >> 7 t√¢ches de traduction soumises...\n",
            "      >> Progression traduction (t√¢ches termin√©es): 7/7 \n",
            "   >> Traduction Termin√©e pour Audio Chunk 2 en 14.90s (7 appel(s) API).\n",
            "   >> 193 segments traduits agr√©g√©s pour Audio Chunk 2.\n",
            "      >> ‚¨ÜÔ∏è Tentative d'upload pour Chunk Audio 2 (193 segments)...\n",
            "      >> ‚úÖ Upload Chunk Audio 2 r√©ussi (Tentative 1, Status: 200). Serveur: Segments appended successfully to video_id v7RRTGdTquc\n",
            "   ‚è±Ô∏è Temps total traitement Chunk Audio 2: 158.28s\n",
            "\n",
            "--- Traitement Chunk Audio 3/3: chunk_0002.mp3 (Offset: 1200.000s) ---\n",
            "\n",
            "üéôÔ∏è Transcription: chunk_0002.mp3 (Offset Global: 1200.000s)\n",
            "   Infos chunk d√©tect√©es: Lang='en' (Conf: 0.52), Dur√©e: 585.53s\n",
            "   Traitement des segments...\n",
            "   Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% \n",
            "   üïí Transcription du chunk termin√©e en 70.16s. 229 segments trouv√©s.\n",
            "   -> Transcrit 229 segments.\n",
            "   >> Pr√©paration traduction pour 229 segments en 8 sous-chunk(s) API.\n",
            "      (Utilisation de max 14 workers concurrents)\n",
            "      >> 8 t√¢ches de traduction soumises...\n",
            "      >> Progression traduction (t√¢ches termin√©es): 8/8 \n",
            "   >> Traduction Termin√©e pour Audio Chunk 3 en 11.63s (8 appel(s) API).\n",
            "   >> 229 segments traduits agr√©g√©s pour Audio Chunk 3.\n",
            "      >> ‚¨ÜÔ∏è Tentative d'upload pour Chunk Audio 3 (229 segments)...\n",
            "      >> ‚úÖ Upload Chunk Audio 3 r√©ussi (Tentative 1, Status: 200). Serveur: Segments appended successfully to video_id v7RRTGdTquc\n",
            "   ‚è±Ô∏è Temps total traitement Chunk Audio 3: 82.12s\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 2: R√âSUM√â DU TRAITEMENT\n",
            "========================================\n",
            "‚è±Ô∏è Temps total Traitement & Upload (√âtape 2): 389.61s.\n",
            "üìä Total Chunks Audio Trait√©s: 3\n",
            "üìä Total Segments Transcrits: 526\n",
            "üìä Total Segments agr√©g√©s apr√®s traduction r√©ussie: 526\n",
            "‚úÖ Traduction r√©ussie pour tous les chunks audio avec segments.\n",
            "üíæ Total Segments agr√©g√©s pour sauvegarde locale: 526\n",
            "‚òÅÔ∏è Uploads Incr√©mentaux R√©ussis (par chunk audio): 3\n",
            "‚òÅÔ∏è Uploads Incr√©mentaux √âchou√©s (par chunk audio): 0\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 3: SAUVEGARDE LOCALE FINALE\n",
            "========================================\n",
            "Tentative de sauvegarde du r√©sultat local agr√©g√© vers:\n",
            "   /content/audio_output_optimized_v2/v7RRTGdTquc__large-v3_splitFFMPEG10min_noVAD_segChunk30_conc14_fully_translated.json\n",
            "‚úÖ R√©sultat local complet sauvegard√© avec succ√®s.\n",
            "‚è±Ô∏è Temps √âtape 3 (Sauvegarde): 0.01s\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 4: NETTOYAGE (OPTIONNEL)\n",
            "========================================\n",
            "--- Nettoyage interm√©diaire d√©sactiv√© ---\n",
            "‚ÑπÔ∏è Fichier audio temporaire complet conserv√©: /content/temp_downloaded_audio.mp3\n",
            "‚ÑπÔ∏è Chunks audio conserv√©s dans: /content/audio_chunks\n",
            "\n",
            "‚ÑπÔ∏è Fichier JSON final conserv√©: /content/audio_output_optimized_v2/v7RRTGdTquc__large-v3_splitFFMPEG10min_noVAD_segChunk30_conc14_fully_translated.json\n",
            "\n",
            "üèÅ Script complet termin√©.\n",
            "‚è±Ô∏è Dur√©e totale d'ex√©cution du script (toutes √©tapes): 443.36s\n",
            "\n",
            "üîó Lien potentiel pour consulter le r√©sultat sur le serveur:\n",
            "   https://qingplay.pythonanywhere.com/vid/v7RRTGdTquc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-----------------------------------------------------------------------------**\n",
        "# **Developper testing cells**"
      ],
      "metadata": {
        "id": "EB7bfxLpBhj_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dTbIWD890ZwZ",
        "outputId": "c17ed8f2-94f7-4287-db0d-d9dddda4b5df",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/audio_output/*': No such file or directory\n",
            "rm: cannot remove '/content/audio_output_optimized/*': No such file or directory\n",
            "‚úÖ GPU d√©tect√©. PyTorch utilisera CUDA.\n",
            "‚ÑπÔ∏è  R√©cup√©ration des m√©tadonn√©es de la vid√©o...\n",
            "‚úÖ M√©tadonn√©es r√©cup√©r√©es.\n",
            "üîÑ V√©rification/T√©l√©chargement de l'audio vers /content/audio_output_optimized_v2/youtube_audio_opt.mp3...\n",
            "‚ÑπÔ∏è Fichier audio non trouv√©. Tentative de t√©l√©chargement...\n",
            "‚úÖ Fichier audio t√©l√©charg√© avec succ√®s : /content/audio_output_optimized_v2/youtube_audio_opt.mp3\n",
            "\n",
            "--- Informations Vid√©o R√©cup√©r√©es ---\n",
            "{\n",
            "  \"video_id\": \"GRLdsdBDjE4\",\n",
            "  \"channel_name\": \"May Ho\",\n",
            "  \"channel_url\": \"https://www.youtube.com/@MayHo\",\n",
            "  \"title\": \"„Äê Ê∑±Âú≥ VLOG „Äë ‰∏≠ÂúãÂÖçÁ∞ΩÁ´ãÈ¶¨È£õÂéªÊ∑±Âú≥ÂêÉÂñùÁé©Ê®Ç üòù Â∏∂Ëëó‰Ω†ÂÄëÁöÑÁñëÂïèÊâæ Ulike ÂïèÊ∏ÖÊ•ö üò§ÔΩúMAYHO\",\n",
            "  \"description\": \"ÊØè‰∏ÄÊ®£È£üÁâ©ÈÉΩÂ•ΩÂ•ΩÂêÉÂïäÂïäÂïä ü•∫\\n‚ñ∏  Ë®ÇÈñ±ÊàëÂêß üòâ  http://bit.ly/37IiWLu\\n‚ñ∏ ‰∏äÈõÜÂΩ±ÁâáÔºöhttps://youtu.be/XQbXKy2jViQ?si=hUFWW0tuBf87PdCw\\n\\n- - -\\n\\nÂâõÂÆ£‰Ωà‰∏≠ÂúãÂÖçÁ∞ΩÔºåÁ´ãÈ¶¨Ë≤∑Ê©üÁ•®È£õÂéªÊ∑±Âú≥Áé©ÂòçÔºÅü§£\\nÈùûÂ∏∏Ë¨ùË¨ù Ulike ÁöÑÈÇÄË´ãÔºåÂèÉËßÄ‰ªñÂÄëÁöÑÂ∫óÈù¢ÂèäÂÖ¨Âè∏\\nÊàëÈÇÑÂπ´‰Ω†ÂÄëË´á‰∫ÜË∂ÖÊ£íÁöÑÂÑ™ÊÉ†ÔºåÂ§ßÂÆ∂ÂçÉËê¨Âà•ÈåØÈÅéÂñîÔºÅüòé\\n\\n‰πüË¨ùË¨ù Kris Âíå Â§ßË°õÂ∏∂ÊàëÂÄëÂêÉ‰∫ÜÂæàÂ§öÈÅìÂú∞ÁæéÈ£ü\\nÈÄôÂÄãÊóÖÁ®ãÂæàÁÖßÈ°ßÊàëÂÄëÔºåÊÑõ‰Ω†ÂÄëÊ∑±Ê∑± üíï\\nÊ∑±Âú≥‰πãÊóÖÁµêÊùü‰∫ÜÔºå‰ΩÜÂª£Â∑û‰πãÊóÖÂâõË¶ÅÈñãÂßã\\nÂ§ßÂÆ∂ÊúüÂæÖ‰∏ãÂÄãÂΩ±ÁâáÂêßÔºåÊàëÂÄëÊòéÂπ¥Ë¶ã ü•∞\\n\\n- - -\\n\\n‚ú¶ JmoonÁæéÂÆπ‰ª™\\nShopee Shop Voucher„Äê ULIKEMAY „ÄëÂÖ®Â§ßÂ≠óÊØç\\n\\nÊäòÊâ£ÔºöÈ¶¨Âπ£ 40 / Êñ∞Âπ£20 / Âè∞Âπ£ 600\\nËµ†ÈÄÅ2ÊîØÂáùËÉ∂\\n‰∏ãÂçïÊó∂Â§áÊ≥®„Äê MayHo „Äë\\nÂèØ‰ª•ÂÜçËé∑Âæó‰∏ÄÁõíÈù¢ËÜú*5Áâá ( ‰π∞1Ëµ†3 ! )\\n\\nJmoon MY ShopeeÔºöhttps://bit.ly/3peDOYw\\nJmoon MY LazadaÔºöhttps://bit.ly/3vnkDhR\\nJmoon SG shopeeÔºöhttps://bit.ly/3JIk1bh\\nULIKE TWÂÆòÁΩëÔºöhttps://bit.ly/3XPpum7\\n\\n- - -\\n\\n‚ú¶ UlikeËÑ±ÊØõ‰ª™\\nShopee Shop Voucher„Äê ULIKEMAY „Äë ÂÖ®Â§ßÂ≠óÊØç\\n\\nÊäòÊâ£ÔºöÈ¶¨Âπ£ 40 / Êñ∞Âπ£20 / Âè∞Âπ£ 600\\nËµ†ÈÄÅËòÜËñàËÜ†*1ÔºåÈõªÂãïÁâôÂà∑ÔºåÂàÆÊØõÂàÄÔºåÂ¢®Èè°\\n‰∏ãÂçïÊó∂ËÆ∞ÂæóÂ§áÊ≥® „Äê MayHo „Äë\\nÂèØ‰ª•È°çÂ§ñÂÜçËé∑ÂæóÊåâÊë©Êûï‰∏ÄÂÄã ( ‰π∞1Ëµ†5 !! Ôºâ\\n\\nULIKE MY ShopeeÔºöhttp://bit.ly/3V05tX0\\nULIKE MY LazadaÔºöhttps://bit.ly/3vnkDhR\\nULIKE SG ShopeeÔºöhttp://bit.ly/3ByqUaY\\nULIKE TW ÂÆòÁ∂≤Ôºöhttps://bit.ly/3PVoco0\\n\\nÊúâ‰ªÄÈ∫ºÂïèÈ°åÁöÑË©±ÔºåÂèØ‰ª•DM Shopee / Lazada ÂÆ¢ÊúçÂì¶ ‚ù§Ô∏è\\n\\n- - -\\n\\n‚ú¶ 00:00 Day 1 \\n\\nUlike Ê∑±Âú≥ÁæÖÊπñËê¨Ë±°Âüé‰∏âÊúü\\n\\nÊù±ÈñÄËÄÅË°ó\\n\\nÂ§ßËâØÊµ∑Ë®ò Á≤•Â∫ïÁÅ´Èçã\\n\\nÈÖíÂ∫óÔºöÁÅ£ÁßëÊäÄÂúíÂçÄÈ∫óÁàæÊü•ÈõÖÈ†ìÈ£ØÂ∫ó\\n\\n‚ú¶ 16:50 Day 2 \\n\\nÂçóÂ±±ÂçÄ ÈáëÁôæÂë≥\\n\\n‰∏ÄÊ®πËäôËìâ\\n\\n‚ú¶ 35:54 Day 3 \\n\\nËèØÊΩ§Ëê¨Ë±°Â§©Âú∞\\n\\nÂñúËå∂( Ëê¨Ë±°Â§©Âú∞Â∫ó Ôºâ\\n\\nËî°ÁÄæÊ∏ØÂºèÈªûÂøÉÔºà ËèØÊΩ§Ëê¨Ë±°Â§©Âú∞Â∫ó Ôºâ\\n\\nÊ≠°Ê®ÇÊ∏ØÁÅ£ \\n\\nÈçæÊõ∏Èñ£\\n\\nÈä´Â≠êÁÖ®Ëóï\\n\\n‚ú¶ 48:20 Day 4 \\n\\nÊ∑±Âú≥ÁÅ£Ëê¨Ë±°Âüé\\n\\nÊ∑±Âú≥‰∫∫ÊâçÂÖ¨Âúí\\n\\n- - -\\n\\n‚úß Instagram  http://bit.ly/2OFuK8H\\n‚úß Facebook  http://bit.ly/2rhJjr9\\n‚úß TikTok  https://bit.ly/34FwcQd\\n‚úß Â∞èÁ¥ÖÊõ∏ https://bit.ly/3qJBQvk\\n\\n‚úß @allenkhor  http://bit.ly/377lPF1\\n‚úß ÁëæËê± https://bit.ly/3FOvZMM\\n‚úß ‰ªüÂΩ± https://bit.ly/3lW0CWB\\n\\n‚ñ† ÊîùÂΩ±Âô®Êùê\\n @sonymalaysia  ZV-1 II & ECM-G1\\n @Apple  iPhone 15 Pro Max \\n\\n‚ñ† Â∑•‰ΩúÂêà‰ΩúÈÇÄÁ¥Ñ\\nmayho0110@hotmail.com \\n\\n‚ñ† Music  \\nE's Jammy Jams,TrackTribe,Otis McDonald\\n\\n#MayHo‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã #AllenKhor #Ê∑±Âú≥Vlog #ÁæéÂ•ΩÁöÑ‰∏ÄÂ§© #Ulike #Jmoon\"\n",
            "}\n",
            "-----------------------------------\n",
            "\n",
            "üöÄ Lancement de la transcription optimis√©e pour : /content/audio_output_optimized_v2/youtube_audio_opt.mp3\n",
            "\n",
            "üîÑ Chargement du mod√®le faster-whisper 'large-v3'...\n",
            "‚ÑπÔ∏è GPU d√©tect√©, utilisation de float16.\n",
            "‚úÖ Mod√®le charg√© sur 'cuda' avec compute_type='float16' en 3.09s.\n",
            "\n",
            "üéôÔ∏è D√©but de la transcription (faster-whisper) pour youtube_audio_opt.mp3...\n",
            "   Options: beam_size=5, vad_filter=False\n",
            "‚úÖ Infos d√©tect√©es: Langue='zh' (Prob: 0.95), Dur√©e Totale: 3211.98s\n",
            "--> D√©but du traitement des segments...\n",
            "   Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà-] 99.1% (Segment ~1918, Temps: 3182.24s / 3211.98s)\n",
            "üïí Transcription (faster-whisper) termin√©e en 629.34 secondes.\n",
            "   Nombre total de segments g√©n√©r√©s : 1933\n",
            "\n",
            "--- V√©rification des Timestamps ---\n",
            "‚ÑπÔ∏è VAD √©tait d√©sactiv√©. Les temps proviennent directement du mod√®le. Si incorrects:\n",
            "   - Envisagez un mod√®le plus grand ('large-v3' -> 'large-v3'?) pour une meilleure pr√©cision potentielle.\n",
            "   - V√©rifiez la qualit√© de l'audio source.\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "------------------------------------\n",
            "‚úÖ Transcription sauvegard√©e dans : /content/audio_output_optimized_v2/youtube_audio_opt_transcript_large-v3_noVAD.json\n",
            "\n",
            "üèÅ Script termin√©.\n"
          ]
        }
      ],
      "source": [
        "# @title # **Transcription (Optimized with faster-whisper & GPU support)**\n",
        "\n",
        "!rm -r /content/audio_output/*\n",
        "!rm -r /content/audio_output_optimized/*\n",
        "!rm -r /content/audio_output_optimized_v2/*\n",
        "\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# --- Try importing necessary libraries and provide guidance if missing ---\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    print(\"‚ùå PyTorch n'est pas install√©. Veuillez l'installer.\")\n",
        "    print(\"   - CPU: pip install torch torchvision torchaudio\")\n",
        "    print(\"   - GPU: Voir https://pytorch.org/ pour la commande CUDA appropri√©e.\")\n",
        "    exit()\n",
        "\n",
        "try:\n",
        "    from faster_whisper import WhisperModel\n",
        "except ImportError:\n",
        "    print(\"‚ùå faster-whisper n'est pas install√©. Veuillez ex√©cuter : pip install faster-whisper\")\n",
        "    exit()\n",
        "\n",
        "# --- Check for GPU availability ---\n",
        "IS_GPU_AVAILABLE = torch.cuda.is_available()\n",
        "if IS_GPU_AVAILABLE:\n",
        "    print(\"‚úÖ GPU d√©tect√©. PyTorch utilisera CUDA.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Aucun GPU compatible CUDA d√©tect√©. PyTorch utilisera le CPU.\")\n",
        "    print(\"   La transcription sera plus lente. faster-whisper avec 'int8' peut aider.\")\n",
        "\n",
        "\n",
        "# --- Fonction download_youtube_audio_improved (PAS DE CHANGEMENT ICI) ---\n",
        "# ... (votre fonction download_youtube_audio_improved reste identique) ...\n",
        "def download_youtube_audio_improved(youtube_url, output_path):\n",
        "    \"\"\"\n",
        "    Downloads YouTube audio and reliably extracts metadata using separate yt-dlp calls.\n",
        "    Checks if the audio file already exists before attempting download.\n",
        "\n",
        "    Args:\n",
        "        youtube_url (str): The URL of the YouTube video.\n",
        "        output_path (str): The desired path to save the MP3 audio file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (str, dict) containing the audio file path and video info dictionary,\n",
        "               or (None, None) if a critical error occurs (metadata failure).\n",
        "               If audio download fails but metadata is retrieved, returns (None, video_info).\n",
        "    \"\"\"\n",
        "    video_info = None\n",
        "    audio_file_path = None\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    if not os.path.exists(output_dir):\n",
        "        try:\n",
        "            os.makedirs(output_dir)\n",
        "            print(f\"üìÅ Cr√©ation du r√©pertoire de sortie : {output_dir}\")\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Erreur lors de la cr√©ation du r√©pertoire {output_dir}: {e}\")\n",
        "            return None, None # Cannot proceed without output directory\n",
        "\n",
        "    # --- √âtape 1: R√©cup√©rer les m√©tadonn√©es avec --dump-json ---\n",
        "    print(\"‚ÑπÔ∏è  R√©cup√©ration des m√©tadonn√©es de la vid√©o...\")\n",
        "    metadata_result = None # Initialize\n",
        "    try:\n",
        "        metadata_command = [\n",
        "            \"yt-dlp\",\n",
        "            \"--dump-json\", # Sortir les m√©tadonn√©es en JSON sur stdout\n",
        "            \"--encoding\", \"utf-8\", # Assurer l'encodage correct\n",
        "            youtube_url,\n",
        "        ]\n",
        "        metadata_result = subprocess.run(metadata_command, check=True, capture_output=True, text=True, encoding='utf-8')\n",
        "        metadata = json.loads(metadata_result.stdout)\n",
        "\n",
        "        # Extraire les informations n√©cessaires\n",
        "        channel_name = metadata.get(\"uploader\", \"N/A\")\n",
        "        channel_url = metadata.get(\"uploader_url\", \"N/A\")\n",
        "        title = metadata.get(\"title\", \"N/A\")\n",
        "        description = metadata.get(\"description\", \"N/A\")\n",
        "        video_id = metadata.get(\"id\", None) # ID YouTube r√©el\n",
        "\n",
        "        if not video_id:\n",
        "             print(\"‚ö†Ô∏è Impossible d'extraire l'ID de la vid√©o via yt-dlp.\")\n",
        "             # Essayer avec regex en secours\n",
        "             video_id_match = re.search(r\"v=([a-zA-Z0-9_-]+)\", youtube_url)\n",
        "             video_id = video_id_match.group(1) if video_id_match else \"UNKNOWN\"\n",
        "\n",
        "        video_info = {\n",
        "            \"video_id\": video_id, # ID YouTube r√©el\n",
        "            \"channel_name\": channel_name,\n",
        "            \"channel_url\": channel_url,\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "        }\n",
        "        print(\"‚úÖ M√©tadonn√©es r√©cup√©r√©es.\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Erreur critique lors de la r√©cup√©ration des m√©tadonn√©es avec yt-dlp (Code: {e.returncode}).\")\n",
        "        print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "        stderr_output = e.stderr.strip() if e.stderr else \"N/A\"\n",
        "        print(f\"   Erreur: {stderr_output}\")\n",
        "        return None, None # Erreur critique, impossible de continuer sans m√©tadonn√©es\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Erreur critique lors de l'analyse des m√©tadonn√©es JSON : {e}\")\n",
        "        if metadata_result and metadata_result.stdout:\n",
        "             print(f\"--- Sortie brute de yt-dlp --- \\n{metadata_result.stdout[:500]}...\\n--------------------------\")\n",
        "        return None, None # Erreur critique\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå yt-dlp n'est pas install√© ou introuvable dans le PATH.\")\n",
        "        print(\"   Veuillez l'installer (par exemple avec : pip install yt-dlp)\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue critique lors de la r√©cup√©ration des m√©tadonn√©es : {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None, None # Erreur critique\n",
        "\n",
        "    # --- √âtape 2: T√©l√©charger (ou v√©rifier) l'audio ---\n",
        "    print(f\"üîÑ V√©rification/T√©l√©chargement de l'audio vers {output_path}...\")\n",
        "\n",
        "    # V√©rification pr√©alable de l'existence du fichier\n",
        "    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "        print(f\"‚úÖ Fichier audio '{output_path}' existe d√©j√† et n'est pas vide. Utilisation du fichier existant.\")\n",
        "        audio_file_path = output_path\n",
        "    else:\n",
        "        if os.path.exists(output_path):\n",
        "             print(f\"‚ÑπÔ∏è Fichier audio '{output_path}' existe mais est vide. Tentative de re-t√©l√©chargement...\")\n",
        "        else:\n",
        "             print(f\"‚ÑπÔ∏è Fichier audio non trouv√©. Tentative de t√©l√©chargement...\")\n",
        "\n",
        "        download_result = None # Initialize\n",
        "        try:\n",
        "            download_command = [\n",
        "                \"yt-dlp\",\n",
        "                \"-x\", # Extraire l'audio\n",
        "                \"--audio-format\", \"mp3\",\n",
        "                # '--audio-quality', '0', # Optionnel: Meilleure qualit√© audio (peut √™tre plus lent)\n",
        "                # '--no-overwrites', # On g√®re la v√©rification avant, mais laisser en s√©curit√©\n",
        "                \"--force-overwrites\", # Forcer l'√©crasement si le fichier existant √©tait vide/corrompu\n",
        "                \"-o\", output_path, # Sp√©cifier le chemin de sortie complet\n",
        "                \"--encoding\", \"utf-8\",\n",
        "                # \"-v\", # D√©commenter pour une sortie tr√®s d√©taill√©e (debug)\n",
        "                youtube_url,\n",
        "            ]\n",
        "            download_result = subprocess.run(download_command, check=True, capture_output=True, text=True, encoding='utf-8')\n",
        "\n",
        "            # V√©rifier si le fichier existe et n'est pas vide APRES l'ex√©cution\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f\"‚úÖ Fichier audio t√©l√©charg√© avec succ√®s : {output_path}\")\n",
        "                audio_file_path = output_path\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è yt-dlp a termin√© sans erreur mais le fichier audio '{output_path}' est introuvable ou vide apr√®s la tentative.\")\n",
        "                if download_result:\n",
        "                    print(f\"--- Sortie yt-dlp (stdout) ---\\n{download_result.stdout.strip()}\")\n",
        "                    print(f\"--- Sortie yt-dlp (stderr) ---\\n{download_result.stderr.strip()}\")\n",
        "                # Retourner les m√©tadonn√©es m√™me si le t√©l√©chargement √©choue ici\n",
        "                return None, video_info\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Erreur lors du t√©l√©chargement de l'audio avec yt-dlp (Code: {e.returncode}).\")\n",
        "            print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "            stderr_output = e.stderr.strip() if e.stderr else \"N/A\"\n",
        "            print(f\"   Erreur: {stderr_output}\")\n",
        "            # V√©rifier si le fichier existe quand m√™me (parfois yt-dlp √©choue mais laisse un fichier partiel)\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                 print(f\"‚ÑπÔ∏è Un fichier audio existe √† '{output_path}' malgr√© l'erreur. Il est peut-√™tre incomplet.\")\n",
        "                 audio_file_path = output_path # On le retourne quand m√™me, l'utilisateur verra l'erreur\n",
        "            else:\n",
        "                 # Retourner les m√©tadonn√©es, mais pas de chemin audio\n",
        "                 return None, video_info\n",
        "        except FileNotFoundError:\n",
        "            # Devrait avoir √©t√© captur√© √† l'√©tape 1, mais redondance\n",
        "            print(\"‚ùå yt-dlp n'est pas install√© ou introuvable dans le PATH.\")\n",
        "            return None, video_info # On a peut-√™tre les m√©tadonn√©es\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur inattendue lors du t√©l√©chargement de l'audio : {e}\")\n",
        "            traceback.print_exc()\n",
        "            # V√©rifier si le fichier existe malgr√© l'erreur\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f\"‚ÑπÔ∏è Utilisation du fichier audio existant '{output_path}' malgr√© l'erreur inattendue.\")\n",
        "                audio_file_path = output_path\n",
        "            else:\n",
        "                return None, video_info # Retourner m√©tadonn√©es, mais pas de chemin audio\n",
        "\n",
        "    # Derni√®re v√©rification de l'existence et de la taille du fichier avant de retourner\n",
        "    if audio_file_path and (not os.path.exists(audio_file_path) or os.path.getsize(audio_file_path) == 0):\n",
        "        print(f\"‚ùå ERREUR FINALE: Le chemin audio '{audio_file_path}' indiqu√© n'existe pas ou est vide.\")\n",
        "        return None, video_info\n",
        "\n",
        "    return audio_file_path, video_info\n",
        "\n",
        "\n",
        "# --- Fonction transcribe_audio (MISE √Ä JOUR avec progression et conseils VAD) ---\n",
        "def transcribe_audio_faster(file_path, model_size=\"base\", video_info=None, beam_size=5, vad_filter=True, vad_min_silence_ms=700):\n",
        "    \"\"\"\n",
        "    Transcrit un fichier audio en utilisant faster-whisper optimis√©, affiche la progression\n",
        "    et retourne une liste de segments. Utilise le GPU si disponible.\n",
        "\n",
        "    Args:\n",
        "      file_path (str): Chemin vers le fichier audio (ex : \"audio.mp3\").\n",
        "      model_size (str): Taille du mod√®le (\"tiny\", \"base\", \"small\", \"medium\", \"large-v3\").\n",
        "      video_info (dict, optional): Informations sur la vid√©o. Defaults to None.\n",
        "      beam_size (int): Taille du faisceau pour le d√©codage.\n",
        "      vad_filter (bool): Activer le filtre VAD (Voice Activity Detection).\n",
        "                         Si True, utilise vad_min_silence_ms.\n",
        "                         Si False, d√©sactive VAD (peut √™tre plus lent mais utile pour d√©boguer les temps).\n",
        "      vad_min_silence_ms (int): Dur√©e minimale de silence en ms pour couper un segment avec VAD.\n",
        "                                Uniquement utilis√© si vad_filter=True.\n",
        "                                Augmenter (ex: 1000) peut cr√©er des segments plus longs.\n",
        "                                Diminuer (ex: 500) peut cr√©er des segments plus courts.\n",
        "\n",
        "    Returns:\n",
        "      dict: Un dictionnaire contenant 'segments' (liste de segments)\n",
        "            et potentiellement des informations sur la vid√©o si fournies.\n",
        "            Retourne {'segments': []} en cas d'erreur majeure.\n",
        "    \"\"\"\n",
        "    start_time_load = time.time()\n",
        "    print(f\"\\nüîÑ Chargement du mod√®le faster-whisper '{model_size}'...\")\n",
        "\n",
        "    device = \"cuda\" if IS_GPU_AVAILABLE else \"cpu\"\n",
        "    # Utiliser float16 pour GPU r√©cent, bfloat16 pour Ampere+, float32/int8 pour CPU\n",
        "    if IS_GPU_AVAILABLE:\n",
        "        # V√©rifier la capacit√© de calcul pour bfloat16 (Ampere et plus r√©cent)\n",
        "        if torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "            compute_type = \"bfloat16\"\n",
        "            print(\"‚ÑπÔ∏è GPU compatible Ampere+ d√©tect√©, utilisation de bfloat16.\")\n",
        "        else:\n",
        "            compute_type = \"float16\"\n",
        "            print(\"‚ÑπÔ∏è GPU d√©tect√©, utilisation de float16.\")\n",
        "    else:\n",
        "        compute_type = \"int8\" # int8 est g√©n√©ralement un bon compromis pour CPU\n",
        "\n",
        "    model = None\n",
        "    try:\n",
        "        model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "        load_time = time.time() - start_time_load\n",
        "        print(f\"‚úÖ Mod√®le charg√© sur '{device}' avec compute_type='{compute_type}' en {load_time:.2f}s.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors du chargement du mod√®le faster-whisper ({model_size}, {device}, {compute_type}): {e}\")\n",
        "        # Essayer un fallback plus s√ªr si le premier choix a √©chou√©\n",
        "        fallback_compute_type = None\n",
        "        if IS_GPU_AVAILABLE and compute_type != \"float32\":\n",
        "             fallback_compute_type = \"float32\" # Moins performant mais plus compatible sur GPU\n",
        "        elif not IS_GPU_AVAILABLE and compute_type != \"int8\": # S'il y avait une autre option CPU test√©e\n",
        "             fallback_compute_type = \"int8\"\n",
        "\n",
        "        if fallback_compute_type:\n",
        "            print(f\"‚ÑπÔ∏è Tentative de fallback avec compute_type='{fallback_compute_type}'...\")\n",
        "            try:\n",
        "                model = WhisperModel(model_size, device=device, compute_type=fallback_compute_type)\n",
        "                load_time = time.time() - start_time_load\n",
        "                print(f\"‚úÖ Mod√®le charg√© sur '{device}' avec compute_type='{fallback_compute_type}' en {load_time:.2f}s.\")\n",
        "            except Exception as e2:\n",
        "                print(f\"‚ùå √âchec du chargement m√™me avec fallback '{fallback_compute_type}': {e2}\")\n",
        "                traceback.print_exc()\n",
        "                return {\"segments\": []}\n",
        "        else:\n",
        "            traceback.print_exc()\n",
        "            return {\"segments\": []} # √âchec initial sans option de fallback √©vidente\n",
        "\n",
        "    print(f\"\\nüéôÔ∏è D√©but de la transcription (faster-whisper) pour {os.path.basename(file_path)}...\")\n",
        "    print(f\"   Options: beam_size={beam_size}, vad_filter={vad_filter}\" + (f\", vad_min_silence_ms={vad_min_silence_ms}\" if vad_filter else \"\"))\n",
        "    start_time_transcribe = time.time()\n",
        "    transcript_segments_data = []\n",
        "    total_audio_duration = 0\n",
        "    last_printed_progress = -1 # Pour √©viter d'imprimer 0% plusieurs fois\n",
        "\n",
        "    try:\n",
        "        # Configurer les param√®tres VAD si activ√©\n",
        "        vad_parameters = None\n",
        "        if vad_filter:\n",
        "            vad_parameters = dict(min_silence_duration_ms=vad_min_silence_ms)\n",
        "            # Vous pouvez ajouter d'autres param√®tres VAD ici si n√©cessaire, ex:\n",
        "            # vad_parameters[\"threshold\"] = 0.5 # Seuil de d√©tection vocale (0 √† 1)\n",
        "\n",
        "        # Lancer la transcription - segments_generator est un it√©rateur !\n",
        "        segments_generator, info = model.transcribe(\n",
        "            file_path,\n",
        "            beam_size=beam_size,\n",
        "            vad_filter=vad_filter,\n",
        "            vad_parameters=vad_parameters\n",
        "            # word_timestamps=False # Mettre √† True pour des temps au niveau du mot (change la structure)\n",
        "        )\n",
        "\n",
        "        detected_lang = info.language\n",
        "        lang_prob = info.language_probability\n",
        "        total_audio_duration = info.duration # Dur√©e totale d√©tect√©e par faster-whisper\n",
        "\n",
        "        print(f\"‚úÖ Infos d√©tect√©es: Langue='{detected_lang}' (Prob: {lang_prob:.2f}), Dur√©e Totale: {total_audio_duration:.2f}s\")\n",
        "        if total_audio_duration <= 0:\n",
        "             print(\"‚ö†Ô∏è Dur√©e audio d√©tect√©e nulle ou n√©gative. Le calcul de progression sera impr√©cis.\")\n",
        "\n",
        "        print(\"--> D√©but du traitement des segments...\")\n",
        "\n",
        "        # --- Boucle principale avec progression ---\n",
        "        segment_count = 0\n",
        "        for segment in segments_generator:\n",
        "            segment_count += 1\n",
        "            seg_start = segment.start\n",
        "            seg_end = segment.end\n",
        "            duration = max(0, seg_end - seg_start)\n",
        "            text = segment.text.strip() if segment.text else \"\"\n",
        "\n",
        "            # Calculer le pourcentage de progression bas√© sur la FIN du segment actuel\n",
        "            current_progress = 0\n",
        "            if total_audio_duration > 0:\n",
        "                current_progress = min(100.0, (seg_end / total_audio_duration) * 100)\n",
        "\n",
        "            # Afficher la progression (avec une barre simple et moins fr√©quente)\n",
        "            # Imprime tous les 5% ou pour le dernier segment\n",
        "            rounded_progress = int(current_progress)\n",
        "            if rounded_progress > last_printed_progress and (rounded_progress % 5 == 0 or rounded_progress >= 99):\n",
        "                progress_bar_length = 20\n",
        "                filled_length = int(progress_bar_length * current_progress / 100)\n",
        "                bar = '‚ñà' * filled_length + '-' * (progress_bar_length - filled_length)\n",
        "                # Utiliser \\r pour revenir au d√©but de la ligne et √©craser le pr√©c√©dent message\n",
        "                print(f\"\\r   Progression: [{bar}] {current_progress:.1f}% (Segment ~{segment_count}, Temps: {seg_end:.2f}s / {total_audio_duration:.2f}s)\", end=\"\")\n",
        "                last_printed_progress = rounded_progress\n",
        "\n",
        "            transcript_segments_data.append({\n",
        "                \"text\": text,\n",
        "                \"start\": round(seg_start, 3),\n",
        "                \"duration\": round(duration, 3),\n",
        "                # On peut garder le % par segment si utile, sinon on l'a d√©j√† affich√©\n",
        "                \"progress_percentage_at_segment_end\": round(current_progress, 2)\n",
        "            })\n",
        "\n",
        "        # Assurer un saut de ligne apr√®s la barre de progression finale\n",
        "        print() # New line after the loop finishes\n",
        "\n",
        "        transcription_time = time.time() - start_time_transcribe\n",
        "        print(f\"üïí Transcription (faster-whisper) termin√©e en {transcription_time:.2f} secondes.\")\n",
        "        print(f\"   Nombre total de segments g√©n√©r√©s : {segment_count}\")\n",
        "\n",
        "        if segment_count == 0:\n",
        "            print(\"‚ö†Ô∏è Aucun segment transcrit trouv√©.\")\n",
        "            # Si aucun segment, v√©rifier si VAD √©tait actif.\n",
        "            if vad_filter:\n",
        "                print(\"   -> Conseil: Essayez avec 'vad_filter=False' ou ajustez 'vad_min_silence_ms'.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Erreur: Fichier audio introuvable √† '{file_path}'\")\n",
        "        return {\"segments\": []}\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erreur majeure lors de la transcription avec faster-whisper : {e}\")\n",
        "        print(\"--- Traceback de l'erreur ---\")\n",
        "        traceback.print_exc()\n",
        "        print(\"---------------------------\")\n",
        "        return {\"segments\": []}\n",
        "\n",
        "    # --- Conseils si les temps semblent incorrects ---\n",
        "    print(\"\\n--- V√©rification des Timestamps ---\")\n",
        "    if vad_filter:\n",
        "        print(\"‚ÑπÔ∏è VAD √©tait activ√©. Si les temps des segments semblent incorrects (trop longs, d√©cal√©s) :\")\n",
        "        print(f\"   - Essayez d'ajuster 'vad_min_silence_ms' (actuellement: {vad_min_silence_ms}). Augmenter (ex: 1000, 1500) regroupe plus, diminuer (ex: 500, 300) segmente plus.\")\n",
        "        print(f\"   - Essayez de relancer avec 'vad_filter=False' pour voir les segments bruts du mod√®le (peut √™tre plus lent).\")\n",
        "    else:\n",
        "         print(\"‚ÑπÔ∏è VAD √©tait d√©sactiv√©. Les temps proviennent directement du mod√®le. Si incorrects:\")\n",
        "         print(f\"   - Envisagez un mod√®le plus grand ('{model_size}' -> 'large-v3'?) pour une meilleure pr√©cision potentielle.\")\n",
        "         print(f\"   - V√©rifiez la qualit√© de l'audio source.\")\n",
        "    print(\"------------------------------------\\n\")\n",
        "\n",
        "\n",
        "    # Construire la sortie finale\n",
        "    output = {\"segments\": transcript_segments_data}\n",
        "    if video_info:\n",
        "        output_video_info = {k: v for k, v in video_info.items() if v is not None}\n",
        "        output.update(output_video_info)\n",
        "\n",
        "    return output\n",
        "\n",
        "# --- Exemple d'utilisation (Adapt√©) ---\n",
        "youtube_url = \"https://www.youtube.com/watch?v=GRLdsdBDjE4&pp=ygUObWF5aG8gc2hlbnpoZW4%3D\" #@param {\"type\":\"string\"}\n",
        "output_dir = \"/content/audio_output_optimized_v2\" # Nouveau dossier pour √©viter conflits\n",
        "output_filename = \"youtube_audio_opt.mp3\"\n",
        "output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "# --- Param√®tres de Transcription ---\n",
        "model_size = \"large-v3\" #@param [\"tiny\",\"base\",\"small\",\"medium\",\"large-v3\"]\n",
        "\n",
        "# --- NOUVEAUX param√®tres pour le contr√¥le VAD et timing ---\n",
        "use_vad = False # @param {type:\"boolean\"}\n",
        "# Cette valeur est IGNOR√âE si use_vad est False\n",
        "vad_silence_duration_ms = 200 # @param {type:\"slider\", min:100, max:2000, step:50}\n",
        "\n",
        "beam_search_size = 5 # Taille standard pour un bon √©quilibre vitesse/qualit√©\n",
        "\n",
        "# --- Ex√©cution Principale ---\n",
        "file_path = None\n",
        "video_info = None\n",
        "json_output_filename = None # Initialiser pour le bloc finally\n",
        "\n",
        "try:\n",
        "    # 1. T√©l√©charger l'audio et r√©cup√©rer les infos vid√©o\n",
        "    file_path, video_info = download_youtube_audio_improved(youtube_url, output_path)\n",
        "\n",
        "    # 2. V√©rifier si le t√©l√©chargement et les m√©tadonn√©es ont r√©ussi\n",
        "    if file_path and video_info:\n",
        "        print(\"\\n--- Informations Vid√©o R√©cup√©r√©es ---\")\n",
        "        print(json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "        print(\"-----------------------------------\\n\")\n",
        "\n",
        "        print(f\"üöÄ Lancement de la transcription optimis√©e pour : {file_path}\")\n",
        "        # 3. Lancer la transcription avec faster-whisper ET les nouveaux param√®tres\n",
        "        transcript = transcribe_audio_faster(\n",
        "            file_path,\n",
        "            model_size=model_size,\n",
        "            video_info=video_info,\n",
        "            beam_size=beam_search_size,\n",
        "            vad_filter=use_vad, # Utiliser le param√®tre d√©fini ci-dessus\n",
        "            vad_min_silence_ms=vad_silence_duration_ms # Utiliser le param√®tre d√©fini ci-dessus\n",
        "        )\n",
        "\n",
        "        # (Le reste du code pour sauvegarder le JSON est inchang√©)\n",
        "        print(\"\\n------------------------------------\")\n",
        "\n",
        "        # 4. Sauvegarder la transcription si elle n'est pas vide\n",
        "        if transcript and transcript.get(\"segments\"): # V√©rifier que des segments existent\n",
        "            # Ajouter les param√®tres utilis√©s au nom de fichier pour r√©f√©rence\n",
        "            vad_suffix = f\"vad{vad_silence_duration_ms}\" if use_vad else \"noVAD\"\n",
        "            json_output_filename = os.path.splitext(output_path)[0] + f\"_transcript_{model_size}_{vad_suffix}.json\"\n",
        "            try:\n",
        "                with open(json_output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(transcript, f, indent=2, ensure_ascii=False)\n",
        "                print(f\"‚úÖ Transcription sauvegard√©e dans : {json_output_filename}\")\n",
        "            except IOError as e:\n",
        "                print(f\"‚ùå Erreur lors de la sauvegarde du fichier JSON : {e}\")\n",
        "            except Exception as e:\n",
        "                 print(f\"‚ùå Erreur inattendue lors de la sauvegarde JSON : {e}\")\n",
        "                 traceback.print_exc() # Voir l'erreur\n",
        "        elif transcript: # Si transcript existe mais pas de segments\n",
        "             print(\"‚ÑπÔ∏è La transcription a √©t√© ex√©cut√©e mais n'a produit aucun segment. Aucun fichier JSON sauvegard√©.\")\n",
        "        else: # Si transcript est None ou vide\n",
        "             print(\"‚ùå La transcription a √©chou√© ou a retourn√© un r√©sultat vide. Aucun fichier JSON sauvegard√©.\")\n",
        "\n",
        "\n",
        "    elif video_info:\n",
        "        print(\"\\n‚ö†Ô∏è Le t√©l√©chargement/acc√®s au fichier audio a √©chou√©, mais les m√©tadonn√©es ont √©t√© r√©cup√©r√©es.\")\n",
        "        print(\"--- Informations Vid√©o ---\")\n",
        "        print(json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "        print(\"------------------------\")\n",
        "        print(\"üö´ Transcription annul√©e car le fichier audio est manquant ou inaccessible.\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Impossible de r√©cup√©rer les m√©tadonn√©es et/ou de t√©l√©charger/trouver le fichier audio.\")\n",
        "        print(\"   V√©rifiez l'URL YouTube, votre connexion internet et l'installation de yt-dlp.\")\n",
        "        print(\"üö´ Transcription annul√©e.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Une erreur globale inattendue s'est produite : {e}\")\n",
        "    print(\"--- Traceback de l'erreur ---\")\n",
        "    traceback.print_exc()\n",
        "    print(\"---------------------------\")\n",
        "\n",
        "finally:\n",
        "    # --- Nettoyage (Optionnel - Inchang√©) ---\n",
        "    # D√©commentez pour supprimer l'audio apr√®s traitement\n",
        "    # print(\"\\n--- Nettoyage ---\")\n",
        "    # if file_path and os.path.exists(file_path):\n",
        "    #     try:\n",
        "    #         # Utilisez 'rm -f' pour forcer la suppression sous Linux/Colab si n√©cessaire\n",
        "    #         # subprocess.run(['rm', '-f', file_path], check=True)\n",
        "    #         os.remove(file_path) # Essayez d'abord os.remove\n",
        "    #         print(f\"üóëÔ∏è Fichier audio supprim√© : {file_path}\")\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"‚ùå Erreur lors de la suppression du fichier audio {file_path}: {e}\")\n",
        "    # elif file_path:\n",
        "    #     print(f\"‚ÑπÔ∏è Fichier audio {file_path} non trouv√© pour suppression.\")\n",
        "    # else:\n",
        "    #      print(f\"‚ÑπÔ∏è Aucun fichier audio √† supprimer.\")\n",
        "    pass # Ne rien faire par d√©faut\n",
        "\n",
        "print(\"\\nüèÅ Script termin√©.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # **Traduction Parall√©lis√©e**\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import math # Pour calculer le nombre total de chunks\n",
        "import concurrent.futures # Pour la parall√©lisation\n",
        "import sys # Pour sys.stdout.flush() si besoin dans certains environnements\n",
        "\n",
        "# --- Fonction Utilitaires ---\n",
        "def chunk_list(lst, n):\n",
        "    \"\"\"Divise une liste en sous-listes de taille n.\"\"\"\n",
        "    if not isinstance(lst, list):\n",
        "        raise TypeError(\"L'entr√©e doit √™tre une liste.\")\n",
        "    if n <= 0:\n",
        "        raise ValueError(\"La taille du chunk doit √™tre positive.\")\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i+n]\n",
        "\n",
        "def extract_json_from_response(text_response):\n",
        "    \"\"\"\n",
        "    Tente d'extraire une cha√Æne JSON valide √† partir d'une r√©ponse textuelle,\n",
        "    en g√©rant les blocs de code markdown potentiels (```json ... ```).\n",
        "    \"\"\"\n",
        "    # 1. Essayer de trouver un bloc de code JSON d√©marqu√©\n",
        "    match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', text_response, re.DOTALL)\n",
        "    if match:\n",
        "        # print(\"‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\") # Moins de logs en parall√®le\n",
        "        return match.group(1).strip()\n",
        "\n",
        "    # 2. Si pas de bloc, chercher un JSON qui commence par '[' et finit par ']' (liste)\n",
        "    #    Ou commence par '{' et finit par '}' (objet - moins probable ici mais par s√©curit√©)\n",
        "    match_list = re.search(r'(\\[[\\s\\S]*\\])', text_response, re.DOTALL)\n",
        "    match_obj = re.search(r'(\\{[\\s\\S]*\\})', text_response, re.DOTALL)\n",
        "\n",
        "    if match_list and match_obj:\n",
        "        json_string = match_list.group(1) if match_list.start() < match_obj.start() else match_obj.group(1)\n",
        "        # print(\"‚ÑπÔ∏è JSON d√©tect√© via d√©limiteurs [...] ou {...}.\")\n",
        "        return json_string.strip()\n",
        "    elif match_list:\n",
        "        # print(\"‚ÑπÔ∏è JSON d√©tect√© via d√©limiteurs [...].\")\n",
        "        return match_list.group(1).strip()\n",
        "    elif match_obj:\n",
        "         # print(\"‚ÑπÔ∏è JSON d√©tect√© via d√©limiteurs {...}.\")\n",
        "         return match_obj.group(1).strip()\n",
        "\n",
        "    # 3. Si rien ne fonctionne, retourner le texte original\n",
        "    # print(\"‚ö†Ô∏è Impossible d'isoler un bloc JSON sp√©cifique. Tentative d'analyse du texte brut.\")\n",
        "    return text_response.strip()\n",
        "\n",
        "\n",
        "# --- Fonction de Traduction (pour un seul chunk - reste inchang√©e) ---\n",
        "def translate_chunk_with_gemini(transcript_chunk, api_key, chunk_index, total_chunks):\n",
        "    \"\"\"\n",
        "    Envoie un chunk de transcription √† l'API Gemini pour traduction.\n",
        "    (Cette fonction est appel√©e par les threads)\n",
        "\n",
        "    Args:\n",
        "        transcript_chunk (list): Un morceau (chunk) de la transcription (liste de segments).\n",
        "        api_key (str): Cl√© API Gemini.\n",
        "        chunk_index (int): Index global du chunk (pour l'affichage).\n",
        "        total_chunks (int): Nombre total de chunks (pour l'affichage).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (chunk_index, list | None) - Retourne l'index du chunk et le r√©sultat (liste traduite ou None)\n",
        "               pour pouvoir r√©assembler dans le bon ordre.\n",
        "    \"\"\"\n",
        "    if not transcript_chunk:\n",
        "        print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] est vide, ignor√©.\")\n",
        "        # Retourne l'index et une liste vide pour indiquer qu'il a √©t√© trait√© (mais √©tait vide)\n",
        "        return chunk_index, []\n",
        "\n",
        "    start_time = time.time()\n",
        "    # Utiliser sys.stdout.flush() peut aider si les logs n'apparaissent pas imm√©diatement en environnement multi-thread√©\n",
        "    print(f\"üîÑ [Chunk {chunk_index+1}/{total_chunks}] D√©but traduction ({len(transcript_chunk)} segments)...\", flush=True)\n",
        "\n",
        "    # Utiliser un mod√®le r√©cent et appropri√©\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}\"\n",
        "\n",
        "    try:\n",
        "        transcript_str = json.dumps(transcript_chunk, ensure_ascii=False, indent=2)\n",
        "    except TypeError as e:\n",
        "         print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur de s√©rialisation JSON du chunk : {e}\", flush=True)\n",
        "         return chunk_index, None\n",
        "\n",
        "    prompt = (\n",
        "        \"You are an expert multilingual translator. Below is a JSON array representing segments of an audio transcript.\\n\"\n",
        "        \"For EACH segment object in the array, please perform the following:\\n\"\n",
        "        \"1. Identify the original language of the 'text' field.\\n\"\n",
        "        \"2. Translate the content of the 'text' field into English and add it as a new key-value pair: 'text_english': \\\"<english_translation>\\\".\\n\"\n",
        "        \"3. Translate the content of the 'text' field into French and add it as a new key-value pair: 'text_french': \\\"<french_translation>\\\".\\n\"\n",
        "        \"4. IMPORTANT: Preserve ALL other existing keys and their values ('start', 'duration', 'progress_percentage', etc.) exactly as they are.\\n\"\n",
        "        \"5. Return ONLY the complete, modified JSON array. Do not include any explanatory text before or after the JSON array itself. Respond only with the JSON.\\n\\n\"\n",
        "        \"Here is the JSON array:\\n\"\n",
        "        f\"```json\\n{transcript_str}\\n```\"\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": 0.3,\n",
        "            \"maxOutputTokens\": 8192,\n",
        "             # Sp√©cifier explicitement le format de sortie JSON (si le mod√®le le supporte bien)\n",
        "             # NOTE : Pas tous les mod√®les ou versions supportent response_mime_type de mani√®re fiable.\n",
        "             # Si cela cause des erreurs, commentez la ligne suivante.\n",
        "             \"response_mime_type\": \"application/json\",\n",
        "        },\n",
        "         \"safetySettings\": [\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n",
        "        ]\n",
        "    }\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "    # --- Boucle de tentatives avec backoff exponentiel l√©ger ---\n",
        "    max_retries = 3\n",
        "    base_delay = 2 # secondes\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=payload, timeout=180) # Timeout long\n",
        "            response.raise_for_status() # L√®ve une exception pour 4xx/5xx\n",
        "\n",
        "            response_data = response.json()\n",
        "\n",
        "            # V√©rification des erreurs Gemini\n",
        "            if not response_data.get(\"candidates\"):\n",
        "                prompt_feedback = response_data.get(\"promptFeedback\", {})\n",
        "                block_reason = prompt_feedback.get(\"blockReason\")\n",
        "                safety_ratings = prompt_feedback.get(\"safetyRatings\")\n",
        "                error_message = f\"Aucun candidat retourn√©.\"\n",
        "                if block_reason: error_message += f\" Raison blocage: {block_reason}.\"\n",
        "                if safety_ratings: error_message += f\" Ratings: {safety_ratings}\"\n",
        "                # Si bloqu√© pour s√©curit√©, ne pas retenter\n",
        "                if block_reason == 'SAFETY':\n",
        "                     print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Blocage API Gemini (S√©curit√©). {error_message}\", flush=True)\n",
        "                     return chunk_index, None\n",
        "                # Pour d'autres erreurs sans candidat, on peut retenter\n",
        "                if attempt < max_retries - 1:\n",
        "                    delay = base_delay * (2 ** attempt)\n",
        "                    print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Erreur API Gemini: {error_message}. Tentative {attempt+2}/{max_retries} apr√®s {delay}s...\", flush=True)\n",
        "                    time.sleep(delay)\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur API Gemini: {error_message}. Echec apr√®s {max_retries} tentatives.\", flush=True)\n",
        "                    return chunk_index, None\n",
        "\n",
        "            candidate = response_data[\"candidates\"][0]\n",
        "            finish_reason = candidate.get(\"finishReason\")\n",
        "\n",
        "            if finish_reason not in [\"STOP\", \"MAX_TOKENS\"]:\n",
        "                print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Fin de g√©n√©ration anormale: {finish_reason}.\", flush=True)\n",
        "                # Retenter si ce n'est pas une erreur fatale\n",
        "                if finish_reason == 'SAFETY': # Ne pas retenter si bloqu√© pour s√©curit√©\n",
        "                     print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Blocage API Gemini (S√©curit√© - finish_reason).\", flush=True)\n",
        "                     return chunk_index, None\n",
        "                if attempt < max_retries - 1:\n",
        "                    delay = base_delay * (2 ** attempt)\n",
        "                    print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Tentative {attempt+2}/{max_retries} apr√®s {delay}s...\", flush=True)\n",
        "                    time.sleep(delay)\n",
        "                    continue\n",
        "                else:\n",
        "                     print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec apr√®s {max_retries} tentatives (finish_reason={finish_reason}).\", flush=True)\n",
        "                     return chunk_index, None # Echec final si raison anormale\n",
        "\n",
        "\n",
        "            if \"content\" not in candidate or \"parts\" not in candidate[\"content\"] or not candidate[\"content\"][\"parts\"]:\n",
        "                 print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Structure de r√©ponse inattendue (manque content/parts).\", flush=True)\n",
        "                 # Retenter pourrait aider si c'est une erreur transitoire\n",
        "                 if attempt < max_retries - 1:\n",
        "                    delay = base_delay * (2 ** attempt)\n",
        "                    print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Tentative {attempt+2}/{max_retries} apr√®s {delay}s...\", flush=True)\n",
        "                    time.sleep(delay)\n",
        "                    continue\n",
        "                 else:\n",
        "                     print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec apr√®s {max_retries} tentatives (structure r√©ponse).\", flush=True)\n",
        "                     return chunk_index, None\n",
        "\n",
        "            raw_text_response = candidate[\"content\"][\"parts\"][0][\"text\"]\n",
        "            json_string = extract_json_from_response(raw_text_response)\n",
        "            result_json = json.loads(json_string) # Peut lever JSONDecodeError\n",
        "\n",
        "            if not isinstance(result_json, list):\n",
        "                 print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Le r√©sultat d√©cod√© n'est pas une liste.\", flush=True)\n",
        "                 # Probablement une erreur du LLM, retenter ne sert √† rien ici\n",
        "                 return chunk_index, None\n",
        "            if len(result_json) != len(transcript_chunk):\n",
        "                 print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Le nombre de segments retourn√©s ({len(result_json)}) != entr√©e ({len(transcript_chunk)}). Probablement tronqu√© (MAX_TOKENS?).\", flush=True)\n",
        "                 # On accepte le r√©sultat partiel mais on logue l'avertissement\n",
        "                 # return chunk_index, None # Ou consid√©rer comme un √©chec si on veut √™tre strict\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"‚úÖ [Chunk {chunk_index+1}/{total_chunks}] Traduction r√©ussie en {elapsed_time:.2f} secondes (Tentative {attempt+1}).\", flush=True)\n",
        "            return chunk_index, result_json # Succ√®s\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Timeout lors de la requ√™te (Tentative {attempt+1}).\", flush=True)\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s timeout.\", flush=True)\n",
        "                return chunk_index, None\n",
        "            delay = base_delay * (2 ** attempt)\n",
        "            print(f\"   Retentative dans {delay}s...\", flush=True)\n",
        "            time.sleep(delay)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            # G√©rer les erreurs 429 (Too Many Requests) sp√©cifiquement si possible\n",
        "            status_code = e.response.status_code if e.response is not None else None\n",
        "            if status_code == 429:\n",
        "                print(f\"‚ö†Ô∏è [Chunk {chunk_index+1}/{total_chunks}] Erreur 429 (Too Many Requests) (Tentative {attempt+1}).\", flush=True)\n",
        "                # Attendre plus longtemps et retenter\n",
        "                delay = 10 * (attempt + 1) # Attente plus longue pour 429\n",
        "                print(f\"   Attente de {delay}s avant la prochaine tentative...\", flush=True)\n",
        "                time.sleep(delay)\n",
        "                if attempt == max_retries - 1:\n",
        "                     print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s erreur 429.\", flush=True)\n",
        "                     return chunk_index, None\n",
        "            else:\n",
        "                print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur r√©seau/HTTP (Tentative {attempt+1}): {e}\", flush=True)\n",
        "                # Pour les erreurs non-429, un backoff standard suffit\n",
        "                if attempt == max_retries - 1:\n",
        "                    print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s erreur r√©seau/HTTP.\", flush=True)\n",
        "                    return chunk_index, None\n",
        "                delay = base_delay * (2 ** attempt)\n",
        "                print(f\"   Retentative dans {delay}s...\", flush=True)\n",
        "                time.sleep(delay)\n",
        "\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur analyse JSON r√©ponse (Tentative {attempt+1}): {e}\", flush=True)\n",
        "            print(f\"--- R√©ponse textuelle brute re√ßue (Chunk {chunk_index+1}) ---\")\n",
        "            print(raw_text_response[:500] + \"...\" if len(raw_text_response) > 500 else raw_text_response)\n",
        "            print(\"--- Fin R√©ponse textuelle brute ---\", flush=True)\n",
        "            # Retenter peut aider si la r√©ponse √©tait corrompue transitoirement\n",
        "            if attempt == max_retries - 1:\n",
        "                 print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s erreur JSON.\", flush=True)\n",
        "                 return chunk_index, None\n",
        "            delay = base_delay * (2 ** attempt)\n",
        "            print(f\"   Retentative dans {delay}s...\", flush=True)\n",
        "            time.sleep(delay)\n",
        "\n",
        "        except (KeyError, IndexError) as e:\n",
        "            print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur acc√®s cl√©s r√©ponse API (Tentative {attempt+1}): {e}\", flush=True)\n",
        "             # Retenter peut aider si la r√©ponse √©tait malform√©e transitoirement\n",
        "            if attempt == max_retries - 1:\n",
        "                 print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s erreur structure r√©ponse.\", flush=True)\n",
        "                 return chunk_index, None\n",
        "            delay = base_delay * (2 ** attempt)\n",
        "            print(f\"   Retentative dans {delay}s...\", flush=True)\n",
        "            time.sleep(delay)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Erreur inattendue (Tentative {attempt+1}): {e.__class__.__name__}: {e}\", flush=True)\n",
        "            # Retenter pour erreurs g√©n√©riques\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s erreur inattendue.\", flush=True)\n",
        "                return chunk_index, None\n",
        "            delay = base_delay * (2 ** attempt)\n",
        "            print(f\"   Retentative dans {delay}s...\", flush=True)\n",
        "            time.sleep(delay)\n",
        "\n",
        "    # Si on sort de la boucle sans succ√®s\n",
        "    print(f\"‚ùå [Chunk {chunk_index+1}/{total_chunks}] Echec final apr√®s {max_retries} tentatives.\", flush=True)\n",
        "    return chunk_index, None\n",
        "\n",
        "\n",
        "# --- Fonction pour traiter un lot (batch) de chunks en parall√®le ---\n",
        "def translate_batch(batch_chunks_with_indices, api_key, total_chunks, max_workers=14):\n",
        "    \"\"\"\n",
        "    Traite un lot de chunks en parall√®le en utilisant ThreadPoolExecutor.\n",
        "\n",
        "    Args:\n",
        "        batch_chunks_with_indices (list): Liste de tuples (index_global, chunk_data).\n",
        "        api_key (str): Cl√© API Gemini.\n",
        "        total_chunks (int): Nombre total de chunks dans la transcription compl√®te.\n",
        "        max_workers (int): Nombre maximum de threads √† utiliser.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionnaire {index_global: resultat_traduction} pour ce lot.\n",
        "              resultat_traduction est soit la liste des segments traduits, soit None en cas d'√©chec.\n",
        "    \"\"\"\n",
        "    batch_results = {}\n",
        "    # Utilise un ThreadPoolExecutor pour g√©rer les threads\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Soumet chaque t√¢che de traduction de chunk au pool de threads\n",
        "        # future_to_index mappe l'objet Future (repr√©sentant l'ex√©cution) √† l'index global du chunk\n",
        "        future_to_index = {\n",
        "            executor.submit(translate_chunk_with_gemini, chunk_data, api_key, index, total_chunks): index\n",
        "            for index, chunk_data in batch_chunks_with_indices\n",
        "        }\n",
        "\n",
        "        # R√©cup√®re les r√©sultats au fur et √† mesure que les t√¢ches se terminent\n",
        "        for future in concurrent.futures.as_completed(future_to_index):\n",
        "            original_index = future_to_index[future]\n",
        "            try:\n",
        "                # Obtient le r√©sultat de la fonction (qui est un tuple: index, data)\n",
        "                index_result, translated_data = future.result()\n",
        "                # Stocke le r√©sultat (translated_data) en utilisant l'index original comme cl√©\n",
        "                batch_results[original_index] = translated_data\n",
        "            except Exception as exc:\n",
        "                # Si l'ex√©cution de la t√¢che elle-m√™me a lev√© une exception impr√©vue\n",
        "                print(f\"‚ÄºÔ∏è [Chunk {original_index+1}/{total_chunks}] a g√©n√©r√© une exception dans le thread: {exc}\", flush=True)\n",
        "                batch_results[original_index] = None # Marque comme √©chou√©\n",
        "\n",
        "    # Retourne le dictionnaire des r√©sultats pour ce lot, index√© par l'index global du chunk\n",
        "    return batch_results\n",
        "\n",
        "\n",
        "# --- Script Principal d'Ex√©cution ---\n",
        "\n",
        "# !!! IMPORTANT: Utiliser Colab Secrets ou une m√©thode s√©curis√©e pour la cl√© API. !!!\n",
        "# from google.colab import userdata\n",
        "# api_key = userdata.get('GEMINI_API_KEY')\n",
        "api_key = \"AIzaSyBiZONd6VA8y9zAd8vueZRo_IrPnn7iHlw\" #@param {type:\"string\"}\n",
        "\n",
        "# Nombre de chunks √† envoyer en parall√®le (limit√© par l'API et vos ressources)\n",
        "# Gemini free tier = 15 RPM (Requests Per Minute)\n",
        "# On prend 14 pour laisser une petite marge.\n",
        "PARALLEL_CHUNKS = 14 #@param {type:\"integer\"}\n",
        "\n",
        "# D√©lai d'attente entre les lots (en secondes) pour respecter la limite de 15 RPM\n",
        "# Si on envoie 14 requ√™tes, on attend 60s pour pouvoir en envoyer 14 autres la minute suivante.\n",
        "# Mettre un peu plus pour √™tre s√ªr (ex: 61 ou 62)\n",
        "RATE_LIMIT_DELAY = 61 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "if api_key == \"VOTRE_CLE_API_GEMINI_ICI\" or not api_key:\n",
        "     print(\"üõë Veuillez fournir votre cl√© API Gemini dans la variable 'api_key'.\")\n",
        "     # raise ValueError(\"Cl√© API Gemini manquante.\")\n",
        "else:\n",
        "    # Assurer l'existence et le format de 'transcript'\n",
        "    if 'transcript' not in locals() or not isinstance(transcript, dict) or 'segments' not in transcript or not isinstance(transcript['segments'], list):\n",
        "        print(\"‚ùå La variable 'transcript' n'est pas d√©finie ou n'a pas le format attendu.\")\n",
        "        print(\"   Assurez-vous que la cellule de transcription a √©t√© ex√©cut√©e avec succ√®s auparavant.\")\n",
        "        print(\"   Format attendu: {'segments': [{'text': ..., 'start': ..., ...}, ...], ...}\")\n",
        "        transcript = {\"segments\": []} # Cr√©er un transcript vide pour √©viter les erreurs\n",
        "\n",
        "    source_segments = transcript.get(\"segments\", [])\n",
        "    if not source_segments:\n",
        "        print(\"‚ÑπÔ∏è La transcription source ne contient aucun segment. Aucune traduction √† effectuer.\")\n",
        "        translated_transcript_data = transcript.copy()\n",
        "        translated_transcript_data['segments'] = []\n",
        "    else:\n",
        "        print(f\"\\nPr√©paration de la traduction pour {len(source_segments)} segments...\")\n",
        "\n",
        "        # Taille du chunk (nombre de segments par requ√™te API) - Ajustable\n",
        "        chunk_size = 50 # @param {type:\"integer\"}\n",
        "        if chunk_size <= 0:\n",
        "            print(\"‚ö†Ô∏è Taille de chunk invalide, utilisation de la valeur par d√©faut 50.\")\n",
        "            chunk_size = 50\n",
        "\n",
        "        # Cr√©er une liste de tous les chunks avec leur index global\n",
        "        all_chunks_with_indices = list(enumerate(chunk_list(source_segments, chunk_size)))\n",
        "        total_chunks = len(all_chunks_with_indices)\n",
        "\n",
        "        print(f\"D√©coupage en {total_chunks} chunks de {chunk_size} segments maximum.\")\n",
        "        print(f\"Traitement par lots de {PARALLEL_CHUNKS} chunks en parall√®le.\")\n",
        "\n",
        "        # Dictionnaire pour stocker tous les r√©sultats, index√©s par l'index du chunk\n",
        "        all_results_dict = {}\n",
        "        failed_chunk_indices = []\n",
        "        successful_chunk_count = 0\n",
        "\n",
        "        start_total_time = time.time()\n",
        "        batch_num = 0\n",
        "        total_batches = math.ceil(total_chunks / PARALLEL_CHUNKS)\n",
        "\n",
        "        # Boucler sur les lots (batches) de chunks\n",
        "        for i in range(0, total_chunks, PARALLEL_CHUNKS):\n",
        "            batch_num += 1\n",
        "            # S√©lectionner le lot actuel de chunks (jusqu'√† PARALLEL_CHUNKS)\n",
        "            current_batch_with_indices = all_chunks_with_indices[i : i + PARALLEL_CHUNKS]\n",
        "            batch_start_index = current_batch_with_indices[0][0] + 1 # Index du 1er chunk du lot\n",
        "            batch_end_index = current_batch_with_indices[-1][0] + 1 # Index du dernier chunk du lot\n",
        "\n",
        "            print(f\"\\n--- Traitement du Lot {batch_num}/{total_batches} (Chunks {batch_start_index} √† {batch_end_index}) ---\", flush=True)\n",
        "\n",
        "            # Traduire le lot en parall√®le\n",
        "            batch_start_time = time.time()\n",
        "            batch_results = translate_batch(current_batch_with_indices, api_key, total_chunks, PARALLEL_CHUNKS)\n",
        "            batch_end_time = time.time()\n",
        "            print(f\"--- Lot {batch_num}/{total_batches} termin√© en {batch_end_time - batch_start_time:.2f} secondes ---\", flush=True)\n",
        "\n",
        "\n",
        "            # Mettre √† jour les r√©sultats globaux et compter les succ√®s/√©checs pour ce lot\n",
        "            for index, result_data in batch_results.items():\n",
        "                all_results_dict[index] = result_data # Stocker le r√©sultat (liste ou None)\n",
        "                if result_data is not None:\n",
        "                    # V√©rification suppl√©mentaire optionnelle des cl√©s (peut ralentir un peu)\n",
        "                    if result_data and isinstance(result_data, list) and result_data[0]:\n",
        "                         if 'text_english' not in result_data[0] or 'text_french' not in result_data[0]:\n",
        "                               print(f\"‚ö†Ô∏è [Chunk {index+1}/{total_chunks}] Cl√©s 'text_english'/'text_french' manquantes dans le r√©sultat retourn√©. Format peut-√™tre incorrect.\", flush=True)\n",
        "                               # On le compte quand m√™me comme 'r√©ussi' car on a re√ßu une liste, mais avec un avertissement\n",
        "                               successful_chunk_count += 1\n",
        "                         else:\n",
        "                               successful_chunk_count += 1\n",
        "                    elif not result_data: # Cas du chunk source vide trait√© correctement\n",
        "                         successful_chunk_count += 1\n",
        "                    # else: # Cas o√π result_data est None (d√©j√† trait√© ci-dessous)\n",
        "                    #     pass\n",
        "                else:\n",
        "                    failed_chunk_indices.append(index + 1) # Ajouter l'index (base 1) du chunk √©chou√©\n",
        "\n",
        "            # --- Pause pour la limite de taux ---\n",
        "            # Si ce n'est pas le dernier lot, attendre avant de lancer le suivant\n",
        "            if i + PARALLEL_CHUNKS < total_chunks:\n",
        "                print(f\"\\n‚è∏Ô∏è Respect de la limite de taux : Attente de {RATE_LIMIT_DELAY} secondes avant le prochain lot...\", flush=True)\n",
        "                time.sleep(RATE_LIMIT_DELAY)\n",
        "\n",
        "        end_total_time = time.time()\n",
        "\n",
        "        # --- Assemblage final des r√©sultats dans le bon ordre ---\n",
        "        all_translated_segments = []\n",
        "        print(\"\\nAssemblage des r√©sultats...\", flush=True)\n",
        "        for index in range(total_chunks):\n",
        "            if index in all_results_dict and all_results_dict[index] is not None:\n",
        "                all_translated_segments.extend(all_results_dict[index])\n",
        "            # else: # Si le chunk a √©chou√© (index pas dans dict ou valeur None), on ne l'ajoute pas\n",
        "                # print(f\"‚ÑπÔ∏è Chunk {index+1} manquant (√©chec ou vide).\") # Optionnel: loguer les chunks manquants\n",
        "        print(\"Assemblage termin√©.\", flush=True)\n",
        "\n",
        "\n",
        "        # --- R√©sum√© Final ---\n",
        "        print(\"\\n--- R√©sum√© de la Traduction ---\")\n",
        "        print(f\"Temps total de traduction : {end_total_time - start_total_time:.2f} secondes.\")\n",
        "        print(f\"Chunks trait√©s avec succ√®s : {successful_chunk_count}/{total_chunks}\")\n",
        "        if failed_chunk_indices:\n",
        "            print(f\"Chunks ayant √©chou√© ({len(failed_chunk_indices)}): {', '.join(map(str, sorted(failed_chunk_indices)))}\")\n",
        "        # Comparer le nombre de segments attendus et obtenus\n",
        "        final_segment_count = len(all_translated_segments)\n",
        "        print(f\"Nombre total de segments dans la sortie finale : {final_segment_count}\")\n",
        "        if final_segment_count < len(source_segments):\n",
        "             print(f\"‚ö†Ô∏è {len(source_segments) - final_segment_count} segments sont manquants par rapport √† l'original (dus aux chunks √©chou√©s ou potentiellement tronqu√©s).\")\n",
        "        elif final_segment_count > len(source_segments):\n",
        "             print(f\"‚ö†Ô∏è Le nombre de segments finaux ({final_segment_count}) est sup√©rieur √† l'original ({len(source_segments)}). V√©rifiez les doublons potentiels ou erreurs d'assemblage.\")\n",
        "        print(\"------------------------------\\n\")\n",
        "\n",
        "\n",
        "        # Cr√©er le dictionnaire final\n",
        "        translated_transcript_data = transcript.copy()\n",
        "        translated_transcript_data[\"segments\"] = all_translated_segments\n",
        "\n",
        "        # Affichage JSON final (optionnel, peut √™tre tr√®s long)\n",
        "        # try:\n",
        "        #    print(\"--- D√©but de la Transcription JSON Traduite (Aper√ßu) ---\")\n",
        "        #    # Afficher seulement les premi√®res N segments pour √©viter un output trop massif\n",
        "        #    preview_segments = translated_transcript_data[\"segments\"][:5] # Afficher les 5 premiers\n",
        "        #    temp_preview = translated_transcript_data.copy()\n",
        "        #    temp_preview[\"segments\"] = preview_segments\n",
        "        #    print(json.dumps(temp_preview, indent=2, ensure_ascii=False))\n",
        "        #    if len(translated_transcript_data[\"segments\"]) > 5:\n",
        "        #        print(f\"\\n   ... et {len(translated_transcript_data['segments']) - 5} autres segments.\")\n",
        "        #    print(\"--- Fin de la Transcription JSON Traduite (Aper√ßu) ---\")\n",
        "        # except Exception as e:\n",
        "        #      print(f\"‚ùå Erreur lors de la g√©n√©ration de l'aper√ßu JSON : {e}\")\n",
        "\n",
        "        # Sauvegarde JSON (si n√©cessaire)\n",
        "        # json_output_filename_translated = \"transcript_translated_parallel.json\"\n",
        "        # try:\n",
        "        #     with open(json_output_filename_translated, \"w\", encoding=\"utf-8\") as f:\n",
        "        #         json.dump(translated_transcript_data, f, indent=2, ensure_ascii=False)\n",
        "        #     print(f\"‚úÖ Transcription traduite sauvegard√©e dans : {json_output_filename_translated}\")\n",
        "        # except IOError as e:\n",
        "        #     print(f\"‚ùå Erreur lors de la sauvegarde du fichier JSON traduit : {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "1r0oScNyF5g2",
        "outputId": "97a85753-62c8-49a8-e8b0-ac3b8274f102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pr√©paration de la traduction pour 1933 segments...\n",
            "D√©coupage en 39 chunks de 50 segments maximum.\n",
            "Traitement par lots de 14 chunks en parall√®le.\n",
            "\n",
            "--- Traitement du Lot 1/3 (Chunks 1 √† 14) ---\n",
            "üîÑ [Chunk 1/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 2/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 3/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 4/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 5/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 6/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 7/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 8/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 9/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 10/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 11/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 12/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 13/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 14/39] D√©but traduction (50 segments)...\n",
            "‚úÖ [Chunk 14/39] Traduction r√©ussie en 21.54 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 8/39] Traduction r√©ussie en 21.57 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 7/39] Traduction r√©ussie en 21.63 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 10/39] Traduction r√©ussie en 21.88 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 6/39] Traduction r√©ussie en 21.97 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 5/39] Traduction r√©ussie en 22.16 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 12/39] Traduction r√©ussie en 22.34 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 13/39] Traduction r√©ussie en 22.56 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 11/39] Traduction r√©ussie en 22.63 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 1/39] Traduction r√©ussie en 22.80 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 4/39] Traduction r√©ussie en 23.18 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 3/39] Traduction r√©ussie en 23.51 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 2/39] Traduction r√©ussie en 27.60 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 9/39] Traduction r√©ussie en 28.69 secondes (Tentative 1).\n",
            "--- Lot 1/3 termin√© en 28.73 secondes ---\n",
            "\n",
            "‚è∏Ô∏è Respect de la limite de taux : Attente de 61 secondes avant le prochain lot...\n",
            "\n",
            "--- Traitement du Lot 2/3 (Chunks 15 √† 28) ---\n",
            "üîÑ [Chunk 15/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 16/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 17/39] D√©but traduction (50 segments)...üîÑ [Chunk 18/39] D√©but traduction (50 segments)...\n",
            "\n",
            "üîÑ [Chunk 19/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 20/39] D√©but traduction (50 segments)...üîÑ [Chunk 21/39] D√©but traduction (50 segments)...\n",
            "\n",
            "üîÑ [Chunk 22/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 23/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 24/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 25/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 26/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 27/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 28/39] D√©but traduction (50 segments)...\n",
            "‚úÖ [Chunk 28/39] Traduction r√©ussie en 21.28 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 15/39] Traduction r√©ussie en 21.64 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 21/39] Traduction r√©ussie en 21.68 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 20/39] Traduction r√©ussie en 21.84 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 23/39] Traduction r√©ussie en 22.01 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 18/39] Traduction r√©ussie en 22.60 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 26/39] Traduction r√©ussie en 22.71 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 17/39] Traduction r√©ussie en 22.79 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 22/39] Traduction r√©ussie en 22.83 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 19/39] Traduction r√©ussie en 22.91 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 16/39] Traduction r√©ussie en 23.34 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 25/39] Traduction r√©ussie en 24.54 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 27/39] Traduction r√©ussie en 27.57 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 24/39] Traduction r√©ussie en 27.98 secondes (Tentative 1).\n",
            "--- Lot 2/3 termin√© en 28.05 secondes ---\n",
            "\n",
            "‚è∏Ô∏è Respect de la limite de taux : Attente de 61 secondes avant le prochain lot...\n",
            "\n",
            "--- Traitement du Lot 3/3 (Chunks 29 √† 39) ---\n",
            "üîÑ [Chunk 29/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 30/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 31/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 32/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 33/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 34/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 35/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 36/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 37/39] D√©but traduction (50 segments)...üîÑ [Chunk 38/39] D√©but traduction (50 segments)...\n",
            "üîÑ [Chunk 39/39] D√©but traduction (33 segments)...\n",
            "\n",
            "‚úÖ [Chunk 39/39] Traduction r√©ussie en 15.26 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 29/39] Traduction r√©ussie en 22.04 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 30/39] Traduction r√©ussie en 22.66 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 35/39] Traduction r√©ussie en 23.14 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 33/39] Traduction r√©ussie en 23.25 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 34/39] Traduction r√©ussie en 23.34 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 31/39] Traduction r√©ussie en 23.64 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 37/39] Traduction r√©ussie en 23.95 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 32/39] Traduction r√©ussie en 27.84 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 38/39] Traduction r√©ussie en 29.04 secondes (Tentative 1).\n",
            "‚úÖ [Chunk 36/39] Traduction r√©ussie en 29.22 secondes (Tentative 1).\n",
            "--- Lot 3/3 termin√© en 29.26 secondes ---\n",
            "\n",
            "Assemblage des r√©sultats...\n",
            "Assemblage termin√©.\n",
            "\n",
            "--- R√©sum√© de la Traduction ---\n",
            "Temps total de traduction : 208.05 secondes.\n",
            "Chunks trait√©s avec succ√®s : 39/39\n",
            "Nombre total de segments dans la sortie finale : 1933\n",
            "------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OIUGwSN_2bA9",
        "outputId": "a2272657-86b3-458d-bbdc-1015dba0a9cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Pr√©paration de la traduction pour 701 segments...\n",
            "D√©coupage en 15 chunks de 50 segments maximum.\n",
            "üîÑ [Chunk 1/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 1/15] Traduction r√©ussie en 27.64 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 2/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 2/15] Traduction r√©ussie en 28.44 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 3/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 3/15] Traduction r√©ussie en 28.99 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 4/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 4/15] Traduction r√©ussie en 28.63 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 5/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 5/15] Traduction r√©ussie en 28.15 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 6/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 6/15] Traduction r√©ussie en 28.24 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 7/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 7/15] Traduction r√©ussie en 27.59 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 8/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 8/15] Traduction r√©ussie en 28.72 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 9/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 9/15] Traduction r√©ussie en 27.16 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 10/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 10/15] Traduction r√©ussie en 28.05 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 11/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 11/15] Traduction r√©ussie en 28.05 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 12/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 12/15] Traduction r√©ussie en 27.56 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 13/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 13/15] Traduction r√©ussie en 27.98 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 14/15] Envoi √† l'API Gemini (50 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 14/15] Traduction r√©ussie en 26.84 secondes.\n",
            "--------------------\n",
            "üîÑ [Chunk 15/15] Envoi √† l'API Gemini (1 segments)...\n",
            "‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\n",
            "‚úÖ [Chunk 15/15] Traduction r√©ussie en 2.01 secondes.\n",
            "--------------------\n",
            "\n",
            "--- R√©sum√© de la Traduction ---\n",
            "Temps total de traduction : 394.06 secondes.\n",
            "Chunks trait√©s avec succ√®s : 15/15\n",
            "Nombre total de segments trait√©s (potentiellement traduits) : 701/701\n",
            "------------------------------\n",
            "\n",
            "--- Fin de la Transcription JSON ---\n"
          ]
        }
      ],
      "source": [
        "# @title # **Transcription & Translation (Optimized)**\n",
        "\n",
        "import whisper\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import concurrent.futures\n",
        "import math\n",
        "import requests\n",
        "import torch # For GPU check\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "\n",
        "# --- Configuration ---\n",
        "WHISPER_MODEL_SIZE = \"base\" # @param [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\", \"large-v3\"]\n",
        "# Nombre de segments √† envoyer √† Gemini en une seule requ√™te API\n",
        "TRANSLATION_CHUNK_SIZE = 50 # @param {type:\"integer\"}\n",
        "# Nombre de requ√™tes de traduction simultan√©es vers l'API Gemini\n",
        "MAX_TRANSLATION_WORKERS = 10 # @param {type:\"integer\"}\n",
        "# Cl√© API Gemini - !! NE PAS METTRE EN DUR DANS LE CODE PARTAG√â / PRODUCTION !!\n",
        "# Utiliser Colab Secrets: from google.colab import userdata; api_key = userdata.get('GEMINI_API_KEY')\n",
        "GEMINI_API_KEY = \"VOTRE_CLE_API_GEMINI_ICI\" #@param {type:\"string\"} # REMPLACEZ PAR VOTRE VRAIE CLE\n",
        "\n",
        "# --- Fonction download_youtube_audio_improved (Mostly Unchanged, added typing) ---\n",
        "def download_youtube_audio_improved(youtube_url: str, output_path: str) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Downloads YouTube audio and reliably extracts metadata using separate yt-dlp calls.\n",
        "\n",
        "    Args:\n",
        "        youtube_url: The URL of the YouTube video.\n",
        "        output_path: The desired path to save the MP3 audio file.\n",
        "\n",
        "    Returns:\n",
        "        A tuple (audio_file_path, video_info_dict).\n",
        "        Returns (None, video_info) if audio download fails but metadata is retrieved.\n",
        "        Returns (None, None) if metadata retrieval fails.\n",
        "    \"\"\"\n",
        "    video_info = None\n",
        "    audio_file_path = None\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    if not os.path.exists(output_dir):\n",
        "        try:\n",
        "            os.makedirs(output_dir)\n",
        "            print(f\"üìÅ Cr√©ation du r√©pertoire de sortie : {output_dir}\")\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Erreur lors de la cr√©ation du r√©pertoire {output_dir}: {e}\")\n",
        "            return None, None # Cannot proceed without output directory\n",
        "\n",
        "    # --- Step 1: Get Metadata ---\n",
        "    print(\"‚ÑπÔ∏è  R√©cup√©ration des m√©tadonn√©es de la vid√©o...\")\n",
        "    try:\n",
        "        metadata_command = [\n",
        "            \"yt-dlp\", \"--dump-json\", \"--encoding\", \"utf-8\", youtube_url,\n",
        "        ]\n",
        "        metadata_result = subprocess.run(metadata_command, check=True, capture_output=True, text=True, encoding='utf-8', errors='replace')\n",
        "        metadata = json.loads(metadata_result.stdout)\n",
        "\n",
        "        video_info = {\n",
        "            \"video_id\": metadata.get(\"id\", \"N/A\"),\n",
        "            \"channel_name\": metadata.get(\"uploader\", \"N/A\"),\n",
        "            \"channel_url\": metadata.get(\"uploader_url\", \"N/A\"),\n",
        "            \"title\": metadata.get(\"title\", \"N/A\"),\n",
        "            \"description\": metadata.get(\"description\", \"N/A\"),\n",
        "        }\n",
        "        # Fallback for video_id if yt-dlp fails to get it\n",
        "        if video_info[\"video_id\"] == \"N/A\":\n",
        "             video_id_match = re.search(r\"v=([a-zA-Z0-9_-]+)\", youtube_url)\n",
        "             video_info[\"video_id\"] = video_id_match.group(1) if video_id_match else \"UNKNOWN\"\n",
        "             print(f\"‚ö†Ô∏è ID vid√©o non trouv√© via yt-dlp, fallback regex: {video_info['video_id']}\")\n",
        "\n",
        "        print(\"‚úÖ M√©tadonn√©es r√©cup√©r√©es.\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Erreur yt-dlp (m√©tadonn√©es) (Code: {e.returncode}).\")\n",
        "        print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "        # Try to decode stderr even if it has errors\n",
        "        error_output = e.stderr.strip() if e.stderr else \"(no stderr)\"\n",
        "        print(f\"   Erreur: {error_output}\")\n",
        "        return None, None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Erreur analyse JSON m√©tadonn√©es yt-dlp : {e}\")\n",
        "        print(f\"--- Sortie brute yt-dlp ---\\n{metadata_result.stdout[:500]}...\\n---\")\n",
        "        return None, None\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå yt-dlp non trouv√©. Installez avec: pip install yt-dlp\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue (m√©tadonn√©es) : {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # --- Step 2: Download Audio ---\n",
        "    print(f\"üîÑ T√©l√©chargement/V√©rification audio vers {output_path}...\")\n",
        "    try:\n",
        "        # Check if file exists AND is non-empty before trying download\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "             print(f\"‚ÑπÔ∏è Fichier audio '{output_path}' existe d√©j√† et n'est pas vide. Utilisation du fichier existant.\")\n",
        "             audio_file_path = output_path\n",
        "        else:\n",
        "            if os.path.exists(output_path):\n",
        "                 print(f\"‚ÑπÔ∏è Fichier audio '{output_path}' existe mais est vide. Tentative de t√©l√©chargement.\")\n",
        "\n",
        "            download_command = [\n",
        "                \"yt-dlp\", \"-x\", \"--audio-format\", \"mp3\",\n",
        "                # '--audio-quality', '0', # Optional: Best audio quality\n",
        "                \"-o\", output_path,\n",
        "                \"--encoding\", \"utf-8\",\n",
        "                # \"-v\", # Uncomment for verbose debug output\n",
        "                youtube_url,\n",
        "            ]\n",
        "            # Capture both stdout and stderr\n",
        "            download_result = subprocess.run(download_command, check=True, capture_output=True, text=True, encoding='utf-8', errors='replace')\n",
        "\n",
        "            # Double-check file existence and size after download attempt\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f\"‚úÖ Fichier audio t√©l√©charg√©/v√©rifi√© : {output_path}\")\n",
        "                audio_file_path = output_path\n",
        "            else:\n",
        "                print(f\"‚ùå ERREUR: yt-dlp termin√© sans erreur mais fichier '{output_path}' introuvable ou vide apr√®s tentative.\")\n",
        "                print(f\"--- Sortie yt-dlp (stdout) ---\\n{download_result.stdout.strip()}\")\n",
        "                print(f\"--- Sortie yt-dlp (stderr) ---\\n{download_result.stderr.strip()}\")\n",
        "                return None, video_info # Return metadata even if download failed\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Log error even if the file exists (e.g., if download failed mid-way but left a file)\n",
        "        print(f\"‚ùå Erreur yt-dlp (t√©l√©chargement) (Code: {e.returncode}).\")\n",
        "        print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "        error_output = e.stderr.strip() if e.stderr else \"(no stderr)\"\n",
        "        print(f\"   Erreur: {error_output}\")\n",
        "        # Check if file exists despite error (maybe it completed partially?)\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "             print(f\"‚ö†Ô∏è Fichier '{output_path}' existe malgr√© l'erreur yt-dlp. Utilisation prudente.\")\n",
        "             audio_file_path = output_path # Use existing file cautiously\n",
        "        else:\n",
        "             return None, video_info # Failed, return metadata only\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå yt-dlp non trouv√©. Installez avec: pip install yt-dlp\")\n",
        "        return None, video_info\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue (t√©l√©chargement) : {e}\")\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "            print(f\"‚ö†Ô∏è Utilisation du fichier audio existant '{output_path}' malgr√© l'erreur inattendue.\")\n",
        "            audio_file_path = output_path\n",
        "        else:\n",
        "            return None, video_info\n",
        "\n",
        "    # Final check\n",
        "    if audio_file_path and (not os.path.exists(audio_file_path) or os.path.getsize(audio_file_path) == 0):\n",
        "        print(f\"‚ùå ERREUR FINALE: Chemin audio '{audio_file_path}' retourn√© mais fichier introuvable ou vide.\")\n",
        "        return None, video_info\n",
        "\n",
        "    return audio_file_path, video_info\n",
        "\n",
        "\n",
        "# --- Fonction transcribe_audio (Optimized for GPU, clearer logging) ---\n",
        "def transcribe_audio(file_path: str, model_size: str = \"base\", video_info: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Transcribes an audio file using Whisper, prioritizing GPU.\n",
        "\n",
        "    Args:\n",
        "      file_path: Path to the audio file.\n",
        "      model_size: Whisper model size.\n",
        "      video_info: Optional dictionary with video metadata.\n",
        "\n",
        "    Returns:\n",
        "      Dictionary containing 'segments' list and potentially video info.\n",
        "      Returns {'segments': []} on critical error.\n",
        "    \"\"\"\n",
        "    start_load_time = time.time()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"üîÑ Chargement du mod√®le Whisper '{model_size}' sur '{device}'...\")\n",
        "    try:\n",
        "        model = whisper.load_model(model_size, device=device)\n",
        "        load_time = time.time() - start_load_time\n",
        "        print(f\"‚úÖ Mod√®le charg√© en {load_time:.2f} secondes.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors du chargement du mod√®le : {e}\")\n",
        "        return {\"segments\": []}\n",
        "\n",
        "    print(f\"üéôÔ∏è D√©but de la transcription pour {os.path.basename(file_path)}...\")\n",
        "    start_transcribe_time = time.time()\n",
        "    try:\n",
        "        # Use fp16=False if on CPU or if encountering precision issues on some GPUs\n",
        "        use_fp16 = True if device == \"cuda\" else False\n",
        "        result = model.transcribe(file_path, verbose=False, fp16=use_fp16) # verbose=True for debug timestamps\n",
        "        transcription_time = time.time() - start_transcribe_time\n",
        "        print(f\"üïí Transcription termin√©e en {transcription_time:.2f} secondes.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Erreur: Fichier audio introuvable √† '{file_path}'\")\n",
        "        return {\"segments\": []}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors de la transcription : {e}\")\n",
        "        # Consider specific errors, e.g., out-of-memory on GPU\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"   Suggestion: Essayez un mod√®le plus petit ou r√©duisez la charge sur le GPU.\")\n",
        "        return {\"segments\": []}\n",
        "\n",
        "    source_segments = result.get(\"segments\", [])\n",
        "    if not source_segments:\n",
        "        print(\"‚ö†Ô∏è Aucun segment trouv√© dans la transcription.\")\n",
        "        output = {\"segments\": []}\n",
        "        if video_info:\n",
        "            output.update({k: v for k, v in video_info.items() if v is not None})\n",
        "        return output\n",
        "\n",
        "    # Determine total duration more robustly\n",
        "    total_audio_duration = source_segments[-1].get(\"end\") if source_segments else 0\n",
        "    if total_audio_duration <= 0:\n",
        "         print(\"‚ö†Ô∏è Impossible de d√©terminer la dur√©e totale √† partir des segments.\")\n",
        "         # Could try 'result.get(\"duration\")' if available, but segment end is often more reliable\n",
        "\n",
        "    transcript_segments = []\n",
        "    print(f\"Traitement et formatage de {len(source_segments)} segments...\")\n",
        "    for i, segment in enumerate(source_segments):\n",
        "        seg_start = segment.get(\"start\", 0.0)\n",
        "        seg_end = segment.get(\"end\", 0.0)\n",
        "        duration = max(0, seg_end - seg_start)\n",
        "        text = segment.get(\"text\", \"\").strip()\n",
        "\n",
        "        progress_percentage = (seg_end / total_audio_duration) * 100 if total_audio_duration > 0 else 0\n",
        "\n",
        "        transcript_segments.append({\n",
        "            \"id\": i, # Add a simple ID for potential reference\n",
        "            \"text\": text,\n",
        "            \"start\": round(seg_start, 3),\n",
        "            \"end\": round(seg_end, 3), # Adding end time can be useful\n",
        "            \"duration\": round(duration, 3),\n",
        "            \"progress_percentage\": round(progress_percentage, 2)\n",
        "        })\n",
        "        # Log progress less frequently\n",
        "        # if (i + 1) % 50 == 0 or (i + 1) == len(source_segments):\n",
        "        #      print(f\"  Segment {i+1}/{len(source_segments)} format√© ({progress_percentage:.1f}%).\")\n",
        "\n",
        "    output = {\"segments\": transcript_segments}\n",
        "    if video_info:\n",
        "        # Filter out None values from video_info before updating\n",
        "        output_video_info = {k: v for k, v in video_info.items() if v is not None}\n",
        "        output.update(output_video_info)\n",
        "\n",
        "    print(f\"‚úÖ Formatage des segments termin√©.\")\n",
        "    return output\n",
        "\n",
        "\n",
        "# --- Fonction Utilitaires Traduction (Unchanged) ---\n",
        "def chunk_list(lst: list, n: int) -> list:\n",
        "    \"\"\"Divise une liste en sous-listes de taille n.\"\"\"\n",
        "    if not isinstance(lst, list):\n",
        "        raise TypeError(\"L'entr√©e doit √™tre une liste.\")\n",
        "    if n <= 0:\n",
        "        raise ValueError(\"La taille du chunk doit √™tre positive.\")\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i+n]\n",
        "\n",
        "def extract_json_from_response(text_response: str) -> str:\n",
        "    \"\"\"\n",
        "    Tente d'extraire une cha√Æne JSON valide √† partir d'une r√©ponse textuelle,\n",
        "    en g√©rant les blocs de code markdown potentiels (```json ... ```).\n",
        "    \"\"\"\n",
        "    match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', text_response, re.DOTALL)\n",
        "    if match:\n",
        "        # print(\"‚ÑπÔ∏è Bloc JSON d√©tect√© via ```json ... ```.\")\n",
        "        return match.group(1).strip()\n",
        "\n",
        "    # Tentative plus simple : chercher le premier '[' et le dernier ']'\n",
        "    start = text_response.find('[')\n",
        "    end = text_response.rfind(']')\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "         # print(\"‚ÑπÔ∏è JSON d√©tect√© via d√©limiteurs [...] simples.\")\n",
        "         return text_response[start:end+1].strip()\n",
        "\n",
        "    # Fallback : essayer de trouver un objet JSON\n",
        "    start = text_response.find('{')\n",
        "    end = text_response.rfind('}')\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "        # print(\"‚ÑπÔ∏è JSON d√©tect√© via d√©limiteurs {...} simples.\")\n",
        "         return text_response[start:end+1].strip()\n",
        "\n",
        "    # print(\"‚ö†Ô∏è Impossible d'isoler un bloc JSON sp√©cifique. Tentative d'analyse du texte brut.\")\n",
        "    return text_response.strip()\n",
        "\n",
        "\n",
        "# --- Fonction de Traduction de Chunk (pour ex√©cution concurrente) ---\n",
        "def translate_chunk_with_gemini(\n",
        "    transcript_chunk: List[Dict[str, Any]],\n",
        "    api_key: str,\n",
        "    chunk_index: int,\n",
        "    total_chunks: int\n",
        ") -> Optional[List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Sends a transcription chunk to the Gemini API for translation. Designed for concurrency.\n",
        "\n",
        "    Args:\n",
        "        transcript_chunk: A piece (chunk) of the transcription (list of segments).\n",
        "        api_key: Gemini API Key.\n",
        "        chunk_index: Index of the current chunk (for logging).\n",
        "        total_chunks: Total number of chunks (for logging).\n",
        "\n",
        "    Returns:\n",
        "        The chunk modified with translations, or None on failure.\n",
        "    \"\"\"\n",
        "    if not transcript_chunk:\n",
        "        print(f\"üí® [Chunk {chunk_index+1}/{total_chunks}] Vide, ignor√©.\")\n",
        "        return [] # Return empty list for consistency\n",
        "\n",
        "    start_time = time.time()\n",
        "    log_prefix = f\"[Chunk {chunk_index+1:03d}/{total_chunks:03d}]\" # Padded index\n",
        "    print(f\"‚û°Ô∏è {log_prefix} Envoi vers Gemini API ({len(transcript_chunk)} segments)...\")\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}\"\n",
        "\n",
        "    try:\n",
        "        transcript_str = json.dumps(transcript_chunk, ensure_ascii=False, indent=None) # No indent for smaller payload\n",
        "    except TypeError as e:\n",
        "         print(f\"‚ùå {log_prefix} Erreur s√©rialisation JSON du chunk : {e}\")\n",
        "         return None\n",
        "\n",
        "    # Precise prompt\n",
        "    prompt = (\n",
        "        \"You are an expert multilingual translator specializing in transcription segments.\\n\"\n",
        "        \"Input: A JSON array of transcript segments, each with 'text', 'start', 'end', 'duration', 'progress_percentage'.\\n\"\n",
        "        \"Task: For EACH segment in the input array:\\n\"\n",
        "        \"1. Identify the original language of the 'text'.\\n\"\n",
        "        \"2. Translate 'text' into English. Add the result as 'text_english': \\\"<english_translation>\\\".\\n\"\n",
        "        \"3. Translate 'text' into French. Add the result as 'text_french': \\\"<french_translation>\\\".\\n\"\n",
        "        \"4. IMPORTANT: Preserve ALL original keys ('id', 'text', 'start', 'end', 'duration', 'progress_percentage') and their exact values.\\n\"\n",
        "        \"Output Format: Return ONLY the modified JSON array (a valid JSON list of objects). Do NOT include any introductory text, explanations, or markdown formatting (like ```json).\\n\\n\"\n",
        "        \"Input JSON array:\\n\"\n",
        "        f\"{transcript_str}\" # Directly embed the JSON string\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": 0.2, # Lower temp for more deterministic translation\n",
        "            \"maxOutputTokens\": 8192, # Generous limit\n",
        "            \"responseMimeType\": \"application/json\", # Request JSON directly if model supports it\n",
        "        },\n",
        "         \"safetySettings\": [ # Standard safety settings\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n",
        "        ]\n",
        "    }\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    raw_text_response = \"\" # Initialize in case of early error\n",
        "\n",
        "    try:\n",
        "        # Increased timeout for potentially longer API calls\n",
        "        response = requests.post(url, headers=headers, json=payload, timeout=240)\n",
        "\n",
        "        # Check for HTTP errors first\n",
        "        response.raise_for_status()\n",
        "\n",
        "        response_data = response.json()\n",
        "\n",
        "        # --- Gemini API Response Validation ---\n",
        "        if not response_data.get(\"candidates\"):\n",
        "            prompt_feedback = response_data.get(\"promptFeedback\", {})\n",
        "            block_reason = prompt_feedback.get(\"blockReason\")\n",
        "            safety_ratings = prompt_feedback.get(\"safetyRatings\")\n",
        "            error_message = \"Aucun candidat retourn√© par l'API.\"\n",
        "            if block_reason: error_message += f\" Raison blocage: {block_reason}.\"\n",
        "            if safety_ratings: error_message += f\" Safety Ratings: {safety_ratings}\"\n",
        "            print(f\"‚ùå {log_prefix} Erreur API Gemini: {error_message}\")\n",
        "            # print(f\"DEBUG: R√©ponse compl√®te re√ßue: {json.dumps(response_data, indent=2)}\") # Debug\n",
        "            return None\n",
        "\n",
        "        candidate = response_data[\"candidates\"][0]\n",
        "        finish_reason = candidate.get(\"finishReason\")\n",
        "        if finish_reason not in [\"STOP\", \"MAX_TOKENS\"]: # MAX_TOKENS might be ok if JSON is parseable\n",
        "             print(f\"‚ö†Ô∏è {log_prefix} Fin de g√©n√©ration anormale: {finish_reason}.\")\n",
        "             # If blocked by safety, log it\n",
        "             if finish_reason == \"SAFETY\":\n",
        "                 safety_ratings = candidate.get(\"safetyRatings\")\n",
        "                 print(f\"   Safety Ratings: {safety_ratings}\")\n",
        "\n",
        "\n",
        "        if \"content\" not in candidate or \"parts\" not in candidate[\"content\"]:\n",
        "             print(f\"‚ùå {log_prefix} Structure r√©ponse inattendue (manque content/parts).\")\n",
        "             # print(f\"DEBUG: Candidat re√ßu: {json.dumps(candidate, indent=2)}\") # Debug\n",
        "             return None\n",
        "\n",
        "        # Since we requested application/json, the content should ideally be parsed JSON already\n",
        "        # However, Gemini might still wrap it or return text if it fails.\n",
        "        raw_text_response = candidate[\"content\"][\"parts\"][0].get(\"text\", \"\")\n",
        "        if not raw_text_response:\n",
        "             print(f\"‚ùå {log_prefix} R√©ponse de l'API vide.\")\n",
        "             return None\n",
        "\n",
        "        # --- JSON Parsing ---\n",
        "        # Attempt to parse the raw text response directly first\n",
        "        result_json = None\n",
        "        try:\n",
        "            result_json = json.loads(raw_text_response)\n",
        "            # print(f\"‚ÑπÔ∏è {log_prefix} R√©ponse API directement pars√©e comme JSON.\")\n",
        "        except json.JSONDecodeError:\n",
        "            # If direct parsing fails, try extracting from potential markdown/text\n",
        "            # print(f\"‚ÑπÔ∏è {log_prefix} R√©ponse non-JSON direct, tentative d'extraction...\")\n",
        "            json_string = extract_json_from_response(raw_text_response)\n",
        "            try:\n",
        "                result_json = json.loads(json_string)\n",
        "            except json.JSONDecodeError as e_inner:\n",
        "                print(f\"‚ùå {log_prefix} √âchec final de l'analyse JSON : {e_inner}\")\n",
        "                print(f\"--- R√©ponse texte brute re√ßue (max 500 chars) ---\")\n",
        "                print(raw_text_response[:500] + (\"...\" if len(raw_text_response) > 500 else \"\"))\n",
        "                print(\"--- Fin R√©ponse texte brute ---\")\n",
        "                return None\n",
        "\n",
        "        # --- Validation of Parsed JSON ---\n",
        "        if not isinstance(result_json, list):\n",
        "             print(f\"‚ùå {log_prefix} R√©sultat d√©cod√© n'est pas une liste JSON.\")\n",
        "             # print(f\"DEBUG: JSON Pars√©: {result_json}\") # Debug\n",
        "             return None\n",
        "\n",
        "        if len(result_json) != len(transcript_chunk):\n",
        "             print(f\"‚ö†Ô∏è {log_prefix} Taille de retour ({len(result_json)}) != taille d'entr√©e ({len(transcript_chunk)}). Possible troncature ou erreur IA.\")\n",
        "             # Decide how to handle: return None (safer) or return partial result?\n",
        "             # For now, treat as failure to ensure data integrity.\n",
        "             return None\n",
        "\n",
        "        # Optional: Deeper validation (check for expected keys in first element)\n",
        "        if result_json and isinstance(result_json[0], dict):\n",
        "            if 'text_english' not in result_json[0] or 'text_french' not in result_json[0]:\n",
        "                 print(f\"‚ö†Ô∏è {log_prefix} Cl√©s 'text_english'/'text_french' manquantes dans le premier segment traduit.\")\n",
        "                 # Treat as failure? Or proceed with missing data? Let's treat as failure.\n",
        "                 return None\n",
        "        elif not result_json: # Empty list returned for empty input\n",
        "             pass # This is fine if the input chunk was empty\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"‚úÖ {log_prefix} Traduction r√©ussie ({elapsed_time:.2f}s).\")\n",
        "        return result_json\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"‚ùå {log_prefix} Timeout de la requ√™te API ({response.request.url}).\")\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur R√©seau/HTTP : {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e: # Error parsing the initial response from requests\n",
        "        print(f\"‚ùå {log_prefix} Erreur analyse JSON de la r√©ponse HTTP initiale : {e}\")\n",
        "        try:\n",
        "            print(f\"--- R√©ponse HTTP brute (max 500 chars) --- \\n{response.text[:500]}...\\n---\")\n",
        "        except NameError:\n",
        "             print(\" (Impossible d'afficher la r√©ponse HTTP)\") # response might not be defined\n",
        "        return None\n",
        "    except (KeyError, IndexError) as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur acc√®s aux cl√©s de la r√©ponse API : {e}\")\n",
        "        try:\n",
        "            print(f\"--- R√©ponse JSON brute re√ßue ---\")\n",
        "            print(json.dumps(response_data, indent=2, ensure_ascii=False))\n",
        "            print(\"--- Fin R√©ponse JSON brute ---\")\n",
        "        except NameError:\n",
        "             print(\"(Impossible d'afficher les donn√©es JSON de la r√©ponse)\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur inattendue : {e.__class__.__name__}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Fonction d'Orchestration de la Traduction Concurrent ---\n",
        "def translate_transcript_concurrently(\n",
        "    source_segments: List[Dict[str, Any]],\n",
        "    api_key: str,\n",
        "    chunk_size: int,\n",
        "    max_workers: int\n",
        ") -> Tuple[List[Dict[str, Any]], int, int]:\n",
        "    \"\"\"\n",
        "    Translates transcription segments concurrently using ThreadPoolExecutor.\n",
        "\n",
        "    Args:\n",
        "        source_segments: List of segments from Whisper.\n",
        "        api_key: Gemini API Key.\n",
        "        chunk_size: Number of segments per API call.\n",
        "        max_workers: Maximum number of concurrent API calls.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (list_of_translated_segments, successful_chunk_count, failed_chunk_count)\n",
        "    \"\"\"\n",
        "    if not source_segments:\n",
        "        return [], 0, 0\n",
        "\n",
        "    print(f\"\\nüöÄ Lancement de la traduction concurrente ({max_workers} workers max)...\")\n",
        "    all_translated_segments = []\n",
        "    failed_chunks_count = 0\n",
        "    successful_chunks_count = 0\n",
        "\n",
        "    # Create chunks with indices for proper ordering later\n",
        "    chunks = list(chunk_list(source_segments, chunk_size))\n",
        "    total_chunks = len(chunks)\n",
        "    # Store future results mapped by original chunk index\n",
        "    future_to_chunk_index = {}\n",
        "    results = [None] * total_chunks # Pre-allocate results list\n",
        "\n",
        "    start_total_time = time.time()\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit all jobs\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            future = executor.submit(\n",
        "                translate_chunk_with_gemini,\n",
        "                chunk,\n",
        "                api_key,\n",
        "                i, # Pass chunk index\n",
        "                total_chunks\n",
        "            )\n",
        "            future_to_chunk_index[future] = i\n",
        "\n",
        "        # Process completed jobs\n",
        "        for future in concurrent.futures.as_completed(future_to_chunk_index):\n",
        "            chunk_index = future_to_chunk_index[future]\n",
        "            try:\n",
        "                translated_chunk = future.result()\n",
        "                if translated_chunk is not None:\n",
        "                    results[chunk_index] = translated_chunk # Store result in correct position\n",
        "                    successful_chunks_count += 1\n",
        "                else:\n",
        "                    # Failure already logged in translate_chunk_with_gemini\n",
        "                    failed_chunks_count += 1\n",
        "                    # results[chunk_index] remains None\n",
        "            except Exception as exc:\n",
        "                # Catch exceptions raised *during* future.result() if not caught inside the worker\n",
        "                print(f\"‚ùå [Chunk {chunk_index+1:03d}/{total_chunks:03d}] Erreur lors de la r√©cup√©ration du r√©sultat: {exc}\")\n",
        "                failed_chunks_count += 1\n",
        "                # results[chunk_index] remains None\n",
        "\n",
        "    end_total_time = time.time()\n",
        "\n",
        "    # Combine results in the correct order, skipping None entries (failed chunks)\n",
        "    for chunk_result in results:\n",
        "        if chunk_result is not None:\n",
        "            all_translated_segments.extend(chunk_result)\n",
        "\n",
        "    print(\"\\n--- R√©sum√© Traduction Concurrente ---\")\n",
        "    print(f\"Temps total de traduction : {end_total_time - start_total_time:.2f} secondes.\")\n",
        "    print(f\"Chunks trait√©s : {successful_chunks_count}/{total_chunks} succ√®s, {failed_chunks_count}/{total_chunks} √©checs.\")\n",
        "    print(f\"Segments sources : {len(source_segments)}\")\n",
        "    print(f\"Segments traduits (r√©cup√©r√©s) : {len(all_translated_segments)}\")\n",
        "    print(\"-------------------------------------\\n\")\n",
        "\n",
        "    return all_translated_segments, successful_chunks_count, failed_chunks_count\n",
        "\n",
        "# --- Script Principal d'Ex√©cution ---\n",
        "def main():\n",
        "    start_pipeline_time = time.time()\n",
        "\n",
        "    # --- Configuration & Validation ---\n",
        "    youtube_url = \"https://www.youtube.com/watch?v=7q88I_hs3Uw\"#@param {\"type\":\"string\"}\n",
        "    output_dir = \"/content/audio_output\"\n",
        "    output_filename = \"youtube_audio.mp3\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    json_transcript_filename = os.path.splitext(output_path)[0] + \"_transcript.json\"\n",
        "    json_translated_filename = os.path.splitext(output_path)[0] + \"_transcript_translated.json\"\n",
        "\n",
        "    if not GEMINI_API_KEY or GEMINI_API_KEY == \"VOTRE_CLE_API_GEMINI_ICI\":\n",
        "         print(\"üõë ERREUR: Cl√© API Gemini manquante ou invalide.\")\n",
        "         print(\"   Veuillez la d√©finir dans la variable GEMINI_API_KEY en haut du script ou via Colab Secrets.\")\n",
        "         return # Stop execution\n",
        "\n",
        "    if TRANSLATION_CHUNK_SIZE <= 0:\n",
        "        print(f\"‚ö†Ô∏è Taille de chunk de traduction invalide ({TRANSLATION_CHUNK_SIZE}), utilisation de 50.\")\n",
        "        chunk_size = 50\n",
        "    else:\n",
        "        chunk_size = TRANSLATION_CHUNK_SIZE\n",
        "\n",
        "    if MAX_TRANSLATION_WORKERS <= 0:\n",
        "        print(f\"‚ö†Ô∏è Nombre de workers de traduction invalide ({MAX_TRANSLATION_WORKERS}), utilisation de 5.\")\n",
        "        max_workers = 5\n",
        "    else:\n",
        "        max_workers = MAX_TRANSLATION_WORKERS\n",
        "\n",
        "    # --- √âtape 1: T√©l√©chargement ---\n",
        "    print(\"-\" * 30)\n",
        "    print(\"√âTAPE 1: T√âL√âCHARGEMENT AUDIO & METADONN√âES\")\n",
        "    print(\"-\" * 30)\n",
        "    audio_file_path, video_info = download_youtube_audio_improved(youtube_url, output_path)\n",
        "\n",
        "    if not audio_file_path:\n",
        "        print(\"\\n‚ùå √âchec critique: Impossible de t√©l√©charger ou trouver le fichier audio.\")\n",
        "        if video_info:\n",
        "            print(\"   M√©tadonn√©es r√©cup√©r√©es:\", json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "        else:\n",
        "            print(\"   √âchec √©galement de la r√©cup√©ration des m√©tadonn√©es.\")\n",
        "        print(\"Pipeline arr√™t√©.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Informations Vid√©o R√©cup√©r√©es ---\")\n",
        "    print(json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "    print(\"-----------------------------------\\n\")\n",
        "\n",
        "    # --- √âtape 2: Transcription ---\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"√âTAPE 2: TRANSCRIPTION AUDIO (Mod√®le: {WHISPER_MODEL_SIZE})\")\n",
        "    print(\"-\" * 30)\n",
        "    transcript_data = transcribe_audio(audio_file_path, model_size=WHISPER_MODEL_SIZE, video_info=video_info)\n",
        "\n",
        "    if not transcript_data or not transcript_data.get(\"segments\"):\n",
        "        print(\"\\n‚ùå √âchec critique: La transcription n'a retourn√© aucun segment.\")\n",
        "        print(\"Pipeline arr√™t√©.\")\n",
        "        # Optional: Save metadata even if transcription failed?\n",
        "        # final_data = {\"segments\": []}\n",
        "        # if video_info: final_data.update(video_info)\n",
        "        # ... save final_data ...\n",
        "        return\n",
        "\n",
        "    # Sauvegarder la transcription originale (non traduite)\n",
        "    try:\n",
        "        with open(json_transcript_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(transcript_data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"üíæ Transcription originale sauvegard√©e dans : {json_transcript_filename}\")\n",
        "    except IOError as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur lors de la sauvegarde de la transcription originale : {e}\")\n",
        "    except Exception as e:\n",
        "         print(f\"‚ö†Ô∏è Erreur inattendue lors de la sauvegarde JSON (original) : {e}\")\n",
        "\n",
        "\n",
        "    source_segments = transcript_data.get(\"segments\", []) # Should exist based on check above\n",
        "\n",
        "    # --- √âtape 3: Traduction Concurrente ---\n",
        "    print(\"-\" * 30)\n",
        "    print(\"√âTAPE 3: TRADUCTION DES SEGMENTS\")\n",
        "    print(\"-\" * 30)\n",
        "    translated_segments, successful_chunks, failed_chunks = translate_transcript_concurrently(\n",
        "        source_segments,\n",
        "        GEMINI_API_KEY,\n",
        "        chunk_size=chunk_size,\n",
        "        max_workers=max_workers\n",
        "    )\n",
        "\n",
        "    # --- Assemblage Final et Sauvegarde ---\n",
        "    final_translated_data = transcript_data.copy() # Conserve les m√©tadonn√©es\n",
        "    final_translated_data[\"segments\"] = translated_segments # Remplace par les segments traduits\n",
        "\n",
        "    if failed_chunks > 0:\n",
        "        print(f\"‚ö†Ô∏è {failed_chunks} chunks de traduction ont √©chou√©. Le r√©sultat final peut √™tre incomplet.\")\n",
        "    elif not translated_segments and source_segments:\n",
        "         print(f\"‚ö†Ô∏è La traduction n'a retourn√© aucun segment alors que la source en contenait. V√©rifiez les logs API.\")\n",
        "    else:\n",
        "        print(\"‚úÖ Traduction termin√©e.\")\n",
        "\n",
        "\n",
        "    # Sauvegarder le r√©sultat final traduit\n",
        "    try:\n",
        "        with open(json_translated_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(final_translated_data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"üíæ Transcription traduite sauvegard√©e dans : {json_translated_filename}\")\n",
        "    except IOError as e:\n",
        "        print(f\"‚ùå Erreur lors de la sauvegarde du fichier JSON traduit : {e}\")\n",
        "    except TypeError as e:\n",
        "         print(f\"‚ùå Erreur de Type lors de la s√©rialisation finale en JSON : {e}\")\n",
        "         print(\"   Cela peut arriver si des donn√©es non s√©rialisables sont dans les r√©sultats.\")\n",
        "    except Exception as e:\n",
        "         print(f\"‚ùå Erreur inattendue lors de la sauvegarde JSON (traduit) : {e}\")\n",
        "\n",
        "\n",
        "    # --- Nettoyage Optionnel ---\n",
        "    # D√©commentez pour supprimer le fichier audio apr√®s traitement\n",
        "    # print(\"\\n--- Nettoyage ---\")\n",
        "    # if os.path.exists(audio_file_path):\n",
        "    #     try:\n",
        "    #         os.remove(audio_file_path)\n",
        "    #         print(f\"üóëÔ∏è Fichier audio supprim√© : {audio_file_path}\")\n",
        "    #     except OSError as e:\n",
        "    #         print(f\"‚ùå Erreur suppression fichier audio {audio_file_path}: {e}\")\n",
        "    # else:\n",
        "    #     print(f\"‚ÑπÔ∏è Fichier audio {audio_file_path} non trouv√© pour suppression.\")\n",
        "\n",
        "    end_pipeline_time = time.time()\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"üèÅ Pipeline Termin√© en {end_pipeline_time - start_pipeline_time:.2f} secondes.\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure prerequisite libraries are installed (useful in environments like Colab)\n",
        "    try:\n",
        "        import yt_dlp\n",
        "    except ImportError:\n",
        "        print(\"Tentative d'installation de yt-dlp...\")\n",
        "        subprocess.run(['pip', 'install', '-q', 'yt-dlp'])\n",
        "        print(\"yt-dlp install√©.\")\n",
        "\n",
        "    try:\n",
        "        import whisper\n",
        "    except ImportError:\n",
        "        print(\"Tentative d'installation de openai-whisper...\")\n",
        "        # Note: Whisper installation can be heavy. Consider git+https for latest version if needed.\n",
        "        subprocess.run(['pip', 'install', '-q', 'openai-whisper'])\n",
        "        print(\"openai-whisper install√©.\")\n",
        "        # May need ffmpeg too\n",
        "        try:\n",
        "             subprocess.run(['ffmpeg', '-version'], check=True, capture_output=True)\n",
        "        except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "             print(\"ffmpeg non trouv√© ou ne fonctionne pas. Installation n√©cessaire (apt-get install ffmpeg sous Debian/Ubuntu).\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        import requests\n",
        "    except ImportError:\n",
        "         print(\"Tentative d'installation de requests...\")\n",
        "         subprocess.run(['pip', 'install', '-q', 'requests'])\n",
        "         print(\"requests install√©.\")\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "    except ImportError:\n",
        "         print(\"Tentative d'installation de torch...\")\n",
        "         # Installation can vary depending on CUDA version. Provide a common one.\n",
        "         subprocess.run(['pip', 'install', '-q', 'torch'])\n",
        "         print(\"torch install√©.\")\n",
        "\n",
        "\n",
        "    # Run the main processing pipeline\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61on6U-xAlrX",
        "collapsed": true,
        "outputId": "1ea5f944-825f-42ec-cd6b-d42789caee44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request successful!\n",
            "Status code: 201\n",
            "Response body: {'id': 15, 'message': 'Transcript created successfully'}\n",
            "https://qingplay.pythonanywhere.com/vid/GRLdsdBDjE4\n"
          ]
        }
      ],
      "source": [
        "# @title # **Upload**\n",
        "\n",
        "url = \"https://qingplay.pythonanywhere.com/upload_transcripts\"  #@param {\"type\":\"string\"}\n",
        "\n",
        "# Prepare the data to be sent as JSON\n",
        "payload = {\n",
        "    \"video_id\": translated_transcript_data[\"video_id\"],\n",
        "    \"description\": translated_transcript_data[\"description\"],\n",
        "    \"channel_name\": translated_transcript_data[\"channel_name\"],\n",
        "    \"channel_url\": translated_transcript_data[\"channel_url\"],\n",
        "    \"title\": translated_transcript_data[\"title\"],\n",
        "    \"segments\": translated_transcript_data[\"segments\"]\n",
        "}\n",
        "\n",
        "# Send the POST request\n",
        "try:\n",
        "    response = requests.post(url, json=payload)  # Use json= to send JSON data\n",
        "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(\"Request successful!\")\n",
        "    print(\"Status code:\", response.status_code)\n",
        "    print(\"Response body:\", response.json())\n",
        "    print(\"https://qingplay.pythonanywhere.com/vid/\"+translated_transcript_data[\"video_id\"])\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Request failed: {e}\")\n",
        "    if response is not None:  # Print the error returned by the server.\n",
        "        print(f\"Response text: {response.text}\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Failed to decode JSON: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvSHsfCquSRV"
      },
      "outputs": [],
      "source": [
        "from flask import Flask\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "\n",
        "@app.route(\"/\")\n",
        "def hello_world():\n",
        "  return \"<p>Hello, World!</p>\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r audio_output"
      ],
      "metadata": {
        "id": "vxqAYnDpNE6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # **Transcription & Translation (Optimized with Streaming & Rate Limiting) - HARDCODED API KEY VERSION**\n",
        "\n",
        "# --- Core Python Libraries ---\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import asyncio # For async operations\n",
        "import concurrent.futures # Keep for potential future use, but not core here\n",
        "\n",
        "# --- Third-party Libraries ---\n",
        "import requests # For synchronous checks if needed later\n",
        "import torch # For GPU check\n",
        "import aiohttp # For async HTTP requests to Gemini\n",
        "from aiolimiter import AsyncLimiter # For precise rate limiting\n",
        "import nest_asyncio # For Colab/Jupyter\n",
        "\n",
        "# --- Typing ---\n",
        "from typing import List, Dict, Tuple, Optional, Any, AsyncGenerator, Set\n",
        "\n",
        "# --- Apply nest_asyncio ---\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Whisper Implementation ---\n",
        "# Separate the check/install logic from the actual import used later\n",
        "\n",
        "try:\n",
        "    # Just check if it *can* be imported for the initial message\n",
        "    import faster_whisper\n",
        "    print(\"‚úÖ faster-whisper est d√©j√† install√©.\")\n",
        "    # We will import specific names later, outside the try/except\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Importation de faster-whisper √©chou√©e. Tentative d'installation...\")\n",
        "    print(\"‚ÄºÔ∏è IMPORTANT: Si cela √©choue, ex√©cutez cette commande dans une cellule Colab s√©par√©e AVANT ce script:\")\n",
        "    print(\"   !pip install -q faster-whisper ctranslate2>=3.10.0,<4.0.0\")\n",
        "    try:\n",
        "        # Attempt installation\n",
        "        subprocess.run(['pip', 'install', '-q', 'faster-whisper', 'ctranslate2>=3.10.0,<4.0.0'], check=True)\n",
        "        import faster_whisper # Verify again after install\n",
        "        print(\"‚úÖ faster-whisper install√© avec succ√®s apr√®s tentative.\")\n",
        "    except (ImportError, subprocess.CalledProcessError) as install_err:\n",
        "        print(f\"‚ùå √âchec critique de l'installation/importation de faster-whisper : {install_err}\")\n",
        "        print(\"   Assurez-vous d'avoir ex√©cut√© la commande pip manuellement dans une autre cellule.\")\n",
        "        print(\"   V√©rifiez la compatibilit√© CTranslate2 (CPU/GPU, CUDA).\")\n",
        "        exit(1) # Cannot proceed\n",
        "\n",
        "# --- NOW, import the necessary components ---\n",
        "# This assumes the above block succeeded or exited.\n",
        "try:\n",
        "    from faster_whisper import WhisperModel, AutoModel\n",
        "    print(\"‚úÖ Noms requis (WhisperModel, AutoModel) import√©s depuis faster-whisper.\")\n",
        "except ImportError as e:\n",
        "     print(f\"‚ùå Erreur finale lors de l'importation de WhisperModel/AutoModel: {e}\")\n",
        "     print(\"   M√™me si l'installation a sembl√© r√©ussir, l'importation sp√©cifique a √©chou√©.\")\n",
        "     print(\"   Essayez de red√©marrer l'environnement d'ex√©cution et r√©installez.\")\n",
        "     exit(1)\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "WHISPER_MODEL_SIZE = \"medium\" # @param [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\", \"large-v3\"]\n",
        "WHISPER_COMPUTE_TYPE = \"float16\" # @param [\"default\", \"float16\", \"int8\", \"int8_float16\"]\n",
        "\n",
        "TRANSLATION_CHUNK_SIZE = 30 # @param {type:\"integer\"} # Segments per API call\n",
        "MAX_CONCURRENT_TRANSLATIONS = 5 # @param {type:\"integer\"} # Max simultaneous API calls\n",
        "GEMINI_RATE_LIMIT_PER_MINUTE = 15 # @param {type:\"integer\"} # Based on Gemini Free Tier (adjust if needed)\n",
        "\n",
        "# --- Gemini API Key (HARDCODED) ---\n",
        "GEMINI_API_KEY = \"AIzaSyBiZONd6VA8y9zAd8vueZRo_IrPnn7iHlw\" # <--- REMPLACEZ CECI\n",
        "\n",
        "if GEMINI_API_KEY == \"AIzaSyBiZONd6VA8y9zAd8vueZRo_IrPnn7iHlw\":\n",
        "    print(\"üõë ALERTE: Vous n'avez pas remplac√© 'VOTRE_CLE_API_GEMINI_ICI' par votre cl√© API r√©elle.\")\n",
        "    # exit(1) # Optional: Stop execution\n",
        "\n",
        "\n",
        "# --- Global Variables / State ---\n",
        "gemini_rate_limiter = AsyncLimiter(GEMINI_RATE_LIMIT_PER_MINUTE, 60)\n",
        "translation_semaphore = asyncio.Semaphore(MAX_CONCURRENT_TRANSLATIONS)\n",
        "\n",
        "\n",
        "# --- Fonction download_youtube_audio_improved ---\n",
        "# ... (keep function as is) ...\n",
        "def download_youtube_audio_improved(youtube_url: str, output_path: str) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Downloads YouTube audio and reliably extracts metadata using separate yt-dlp calls.\n",
        "\n",
        "    Args:\n",
        "        youtube_url: The URL of the YouTube video.\n",
        "        output_path: The desired path to save the MP3 audio file.\n",
        "\n",
        "    Returns:\n",
        "        A tuple (audio_file_path, video_info_dict).\n",
        "        Returns (None, video_info) if audio download fails but metadata is retrieved.\n",
        "        Returns (None, None) if metadata retrieval fails.\n",
        "    \"\"\"\n",
        "    video_info = None\n",
        "    audio_file_path = None\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    if not os.path.exists(output_dir):\n",
        "        try:\n",
        "            os.makedirs(output_dir)\n",
        "            print(f\"üìÅ Cr√©ation du r√©pertoire de sortie : {output_dir}\")\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Erreur lors de la cr√©ation du r√©pertoire {output_dir}: {e}\")\n",
        "            return None, None # Cannot proceed without output directory\n",
        "\n",
        "    # --- Step 1: Get Metadata ---\n",
        "    print(\"‚ÑπÔ∏è  R√©cup√©ration des m√©tadonn√©es de la vid√©o...\")\n",
        "    try:\n",
        "        metadata_command = [\n",
        "            \"yt-dlp\", \"--dump-json\", \"--encoding\", \"utf-8\", youtube_url,\n",
        "        ]\n",
        "        metadata_result = subprocess.run(metadata_command, check=True, capture_output=True, text=True, encoding='utf-8', errors='replace', timeout=120)\n",
        "        metadata = json.loads(metadata_result.stdout)\n",
        "\n",
        "        video_info = {\n",
        "            \"video_id\": metadata.get(\"id\", \"N/A\"),\n",
        "            \"channel_name\": metadata.get(\"uploader\", \"N/A\"),\n",
        "            \"channel_url\": metadata.get(\"uploader_url\", \"N/A\"),\n",
        "            \"title\": metadata.get(\"title\", \"N/A\"),\n",
        "            \"description\": metadata.get(\"description\", \"N/A\"),\n",
        "            \"duration\": metadata.get(\"duration\"), # Keep duration if available\n",
        "            \"upload_date\": metadata.get(\"upload_date\"), # Keep upload date\n",
        "            \"thumbnail\": metadata.get(\"thumbnail\"), # Keep thumbnail URL\n",
        "        }\n",
        "        if video_info[\"video_id\"] == \"N/A\":\n",
        "             video_id_match = re.search(r\"v=([a-zA-Z0-9_-]+)\", youtube_url)\n",
        "             video_info[\"video_id\"] = video_id_match.group(1) if video_id_match else \"UNKNOWN\"\n",
        "             print(f\"‚ö†Ô∏è ID vid√©o non trouv√© via yt-dlp, fallback regex: {video_info['video_id']}\")\n",
        "\n",
        "        print(\"‚úÖ M√©tadonn√©es r√©cup√©r√©es.\")\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(f\"‚ùå Timeout lors de la r√©cup√©ration des m√©tadonn√©es yt-dlp.\")\n",
        "        return None, None\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Erreur yt-dlp (m√©tadonn√©es) (Code: {e.returncode}).\")\n",
        "        print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "        error_output = e.stderr.strip() if e.stderr else \"(no stderr)\"\n",
        "        print(f\"   Erreur: {error_output}\")\n",
        "        return None, None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Erreur analyse JSON m√©tadonn√©es yt-dlp : {e}\")\n",
        "        print(f\"--- Sortie brute yt-dlp ---\\n{metadata_result.stdout[:500]}...\\n---\")\n",
        "        return None, None\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå yt-dlp non trouv√©. Installez avec: pip install yt-dlp\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue (m√©tadonn√©es) : {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # --- Step 2: Download Audio ---\n",
        "    print(f\"üîÑ T√©l√©chargement/V√©rification audio vers {output_path}...\")\n",
        "    try:\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "             print(f\"‚ÑπÔ∏è Fichier audio '{output_path}' existe d√©j√† et n'est pas vide. Utilisation du fichier existant.\")\n",
        "             audio_file_path = output_path\n",
        "        else:\n",
        "            if os.path.exists(output_path):\n",
        "                 print(f\"‚ÑπÔ∏è Fichier audio '{output_path}' existe mais est vide. Tentative de t√©l√©chargement.\")\n",
        "\n",
        "            download_command = [\n",
        "                \"yt-dlp\", \"-x\", \"--audio-format\", \"mp3\",\n",
        "                \"-o\", output_path,\n",
        "                \"--encoding\", \"utf-8\",\n",
        "                youtube_url,\n",
        "            ]\n",
        "            download_result = subprocess.run(download_command, check=True, capture_output=True, text=True, encoding='utf-8', errors='replace', timeout=600) # 10 min timeout\n",
        "\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f\"‚úÖ Fichier audio t√©l√©charg√©/v√©rifi√© : {output_path}\")\n",
        "                audio_file_path = output_path\n",
        "            else:\n",
        "                print(f\"‚ùå ERREUR: yt-dlp termin√© sans erreur mais fichier '{output_path}' introuvable ou vide apr√®s tentative.\")\n",
        "                print(f\"--- Sortie yt-dlp (stdout) ---\\n{download_result.stdout.strip()}\")\n",
        "                print(f\"--- Sortie yt-dlp (stderr) ---\\n{download_result.stderr.strip()}\")\n",
        "                return None, video_info\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "         print(f\"‚ùå Timeout lors du t√©l√©chargement audio yt-dlp.\")\n",
        "         if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "             print(f\"‚ö†Ô∏è Fichier partiel '{output_path}' existe malgr√© le timeout. Suppression...\")\n",
        "             try: os.remove(output_path)\n",
        "             except OSError: pass\n",
        "         return None, video_info\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Erreur yt-dlp (t√©l√©chargement) (Code: {e.returncode}).\")\n",
        "        print(f\"   Commande: {' '.join(e.cmd)}\")\n",
        "        error_output = e.stderr.strip() if e.stderr else \"(no stderr)\"\n",
        "        print(f\"   Erreur: {error_output}\")\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "             print(f\"‚ö†Ô∏è Fichier '{output_path}' existe malgr√© l'erreur yt-dlp. Utilisation prudente.\")\n",
        "             audio_file_path = output_path\n",
        "        else:\n",
        "             return None, video_info\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå yt-dlp non trouv√©. Installez avec: pip install yt-dlp\")\n",
        "        return None, video_info\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue (t√©l√©chargement) : {e}\")\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "            print(f\"‚ö†Ô∏è Utilisation du fichier audio existant '{output_path}' malgr√© l'erreur inattendue.\")\n",
        "            audio_file_path = output_path\n",
        "        else:\n",
        "            return None, video_info\n",
        "\n",
        "    if audio_file_path and (not os.path.exists(audio_file_path) or os.path.getsize(audio_file_path) == 0):\n",
        "        print(f\"‚ùå ERREUR FINALE: Chemin audio '{audio_file_path}' retourn√© mais fichier introuvable ou vide.\")\n",
        "        return None, video_info\n",
        "\n",
        "    return audio_file_path, video_info\n",
        "\n",
        "# --- Fonction Utilitaires ---\n",
        "# ... (keep function as is) ...\n",
        "def extract_json_from_response(text_response: str) -> str:\n",
        "    \"\"\"\n",
        "    Tente d'extraire une cha√Æne JSON valide √† partir d'une r√©ponse textuelle,\n",
        "    en g√©rant les blocs de code markdown potentiels (```json ... ```).\n",
        "    \"\"\"\n",
        "    match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', text_response, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    start = text_response.find('[')\n",
        "    end = text_response.rfind(']')\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "         return text_response[start:end+1].strip()\n",
        "    start = text_response.find('{')\n",
        "    end = text_response.rfind('}')\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "         return text_response[start:end+1].strip()\n",
        "    return text_response.strip()\n",
        "\n",
        "\n",
        "# --- Fonction de Traduction Async ---\n",
        "# ... (keep function as is) ...\n",
        "async def translate_chunk_gemini_async(\n",
        "    session: aiohttp.ClientSession,\n",
        "    transcript_chunk: List[Dict[str, Any]],\n",
        "    api_key: str, # La cl√© est pass√©e ici\n",
        "    chunk_index: int, # For logging\n",
        "    total_segments_in_chunk: int # For logging\n",
        ") -> Optional[List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Sends a transcription chunk to the Gemini API for translation asynchronously.\n",
        "    Designed to be called by concurrent workers. Returns translated chunk or None on failure.\n",
        "    \"\"\"\n",
        "    if not transcript_chunk:\n",
        "        # print(f\"üí® [Chunk {chunk_index+1:03d}] Vide, ignor√©.\") # Too verbose\n",
        "        return []\n",
        "\n",
        "    start_time = time.time()\n",
        "    log_prefix = f\"[Chunk {chunk_index+1:03d}]\" # Padded index\n",
        "\n",
        "    # V√©rification de la cl√© API avant de faire l'appel\n",
        "    if not api_key or api_key == \"VOTRE_CLE_API_GEMINI_ICI\":\n",
        "        print(f\"‚ùå {log_prefix} Cl√© API Gemini manquante ou invalide. Impossible de traduire.\")\n",
        "        return None # √âchec du chunk\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}\"\n",
        "\n",
        "    try:\n",
        "        transcript_str = json.dumps(transcript_chunk, ensure_ascii=False, indent=None)\n",
        "    except TypeError as e:\n",
        "         print(f\"‚ùå {log_prefix} Erreur s√©rialisation JSON du chunk : {e}\")\n",
        "         return None\n",
        "\n",
        "    prompt = (\n",
        "        \"You are an expert multilingual translator specializing in transcription segments.\\n\"\n",
        "        \"Input: A JSON array of transcript segments, each with 'id', 'text', 'start', 'end', 'duration', 'progress_percentage'.\\n\"\n",
        "        \"Task: For EACH segment in the input array:\\n\"\n",
        "        \"1. Identify the original language of the 'text'.\\n\"\n",
        "        \"2. Translate 'text' into English. Add the result as 'text_english': \\\"<english_translation>\\\".\\n\"\n",
        "        \"3. Translate 'text' into French. Add the result as 'text_french': \\\"<french_translation>\\\".\\n\"\n",
        "        \"4. IMPORTANT: Preserve ALL original keys ('id', 'text', 'start', 'end', 'duration', 'progress_percentage') and their exact values.\\n\"\n",
        "        \"Output Format: Return ONLY the modified JSON array (a valid JSON list of objects). Do NOT include any introductory text, explanations, or markdown formatting (like ```json).\\n\\n\"\n",
        "        \"Input JSON array:\\n\"\n",
        "        f\"{transcript_str}\"\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": 0.2,\n",
        "            \"maxOutputTokens\": 8192,\n",
        "            \"responseMimeType\": \"application/json\", # Explicitly request JSON\n",
        "        },\n",
        "         \"safetySettings\": [ # Standard safety settings\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n",
        "        ]\n",
        "    }\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    response_text = \"\"\n",
        "    response_json = None\n",
        "\n",
        "    try:\n",
        "        async with gemini_rate_limiter: # Wait if rate limit exceeded\n",
        "             async with translation_semaphore: # Wait if max concurrent requests reached\n",
        "                # print(f\"üîí {log_prefix} Rate limit/semaphore acquis. Envoi...\") # Verbose\n",
        "                req_start_time = time.time()\n",
        "                async with session.post(url, headers=headers, json=payload, timeout=aiohttp.ClientTimeout(total=240)) as response:\n",
        "                    req_duration = time.time() - req_start_time\n",
        "                    # print(f\"‚òÅÔ∏è {log_prefix} R√©ponse re√ßue ({response.status}) en {req_duration:.2f}s.\") # Verbose\n",
        "\n",
        "                    response_text = await response.text()\n",
        "                    response.raise_for_status() # Raise HTTP errors (4xx, 5xx)\n",
        "                    response_json = await response.json(content_type=None) # Allow flexible content type\n",
        "\n",
        "        # --- Process Response ---\n",
        "        if not response_json:\n",
        "             print(f\"‚ùå {log_prefix} R√©ponse JSON vide ou invalide re√ßue.\")\n",
        "             print(f\"   R√©ponse texte brute: {response_text[:200]}...\")\n",
        "             return None\n",
        "\n",
        "        if not response_json.get(\"candidates\"):\n",
        "            prompt_feedback = response_json.get(\"promptFeedback\", {})\n",
        "            block_reason = prompt_feedback.get(\"blockReason\")\n",
        "            safety_ratings = prompt_feedback.get(\"safetyRatings\")\n",
        "            error_message = \"Aucun candidat retourn√© par l'API.\"\n",
        "            if block_reason: error_message += f\" Raison blocage: {block_reason}.\"\n",
        "            if safety_ratings: error_message += f\" Safety Ratings: {safety_ratings}\"\n",
        "            print(f\"‚ùå {log_prefix} Erreur API Gemini: {error_message}\")\n",
        "            # print(f\"DEBUG: R√©ponse compl√®te: {json.dumps(response_json, indent=2)}\") # Debug only\n",
        "            return None\n",
        "\n",
        "        candidate = response_json[\"candidates\"][0]\n",
        "        finish_reason = candidate.get(\"finishReason\")\n",
        "        if finish_reason not in [\"STOP\", \"MAX_TOKENS\"]:\n",
        "             print(f\"‚ö†Ô∏è {log_prefix} Fin de g√©n√©ration anormale: {finish_reason}.\")\n",
        "             if finish_reason == \"SAFETY\":\n",
        "                 safety_ratings = candidate.get(\"safetyRatings\")\n",
        "                 print(f\"   Safety Ratings: {safety_ratings}\")\n",
        "\n",
        "        if \"content\" not in candidate or \"parts\" not in candidate[\"content\"]:\n",
        "             print(f\"‚ùå {log_prefix} Structure r√©ponse inattendue (manque content/parts).\")\n",
        "             # print(f\"DEBUG: Candidat re√ßu: {json.dumps(candidate, indent=2)}\") # Debug only\n",
        "             return None\n",
        "\n",
        "        raw_llm_output_text = candidate[\"content\"][\"parts\"][0].get(\"text\", \"\")\n",
        "        if not raw_llm_output_text:\n",
        "             print(f\"‚ùå {log_prefix} Contenu de la r√©ponse API vide.\")\n",
        "             return None\n",
        "\n",
        "        # --- JSON Parsing from LLM output ---\n",
        "        parsed_result_json = None\n",
        "        try:\n",
        "            # Try direct parsing first (due to responseMimeType)\n",
        "            parsed_result_json = json.loads(raw_llm_output_text)\n",
        "        except json.JSONDecodeError:\n",
        "            # print(f\"‚ÑπÔ∏è {log_prefix} R√©ponse non-JSON direct, tentative d'extraction...\") # Verbose\n",
        "            json_string = extract_json_from_response(raw_llm_output_text)\n",
        "            try:\n",
        "                parsed_result_json = json.loads(json_string)\n",
        "            except json.JSONDecodeError as e_inner:\n",
        "                print(f\"‚ùå {log_prefix} √âchec final de l'analyse JSON du contenu LLM : {e_inner}\")\n",
        "                print(f\"--- Contenu texte brut LLM re√ßu (max 500 chars) ---\\n{raw_llm_output_text[:500]}...\\n---\")\n",
        "                return None\n",
        "\n",
        "        # --- Validation of Parsed JSON ---\n",
        "        if not isinstance(parsed_result_json, list):\n",
        "             print(f\"‚ùå {log_prefix} R√©sultat d√©cod√© du LLM n'est pas une liste JSON.\")\n",
        "             # print(f\"DEBUG: JSON Pars√©: {parsed_result_json}\") # Debug only\n",
        "             return None\n",
        "\n",
        "        if len(parsed_result_json) != total_segments_in_chunk:\n",
        "             print(f\"‚ö†Ô∏è {log_prefix} Taille de retour LLM ({len(parsed_result_json)}) != taille d'entr√©e ({total_segments_in_chunk}). Possible troncature ou erreur IA. Chunk √©chou√©.\")\n",
        "             return None # Treat as failure\n",
        "\n",
        "        # Deeper validation (optional but good)\n",
        "        if parsed_result_json: # If list is not empty\n",
        "            first_segment = parsed_result_json[0]\n",
        "            if not isinstance(first_segment, dict) or \\\n",
        "               'text_english' not in first_segment or \\\n",
        "               'text_french' not in first_segment or \\\n",
        "               'id' not in first_segment: # Check essential keys\n",
        "                 print(f\"‚ö†Ô∏è {log_prefix} Cl√©s essentielles manquantes ('id', 'text_english', 'text_french') dans le premier segment traduit. Chunk √©chou√©.\")\n",
        "                 return None\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"‚úÖ {log_prefix} Traduction r√©ussie ({total_segments_in_chunk} segments, {elapsed_time:.2f}s total).\")\n",
        "        return parsed_result_json\n",
        "\n",
        "    except aiohttp.ClientResponseError as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur HTTP {e.status} de l'API Gemini : {e.message}\")\n",
        "        print(f\"   Headers: {e.headers}\")\n",
        "        print(f\"   R√©ponse Texte (max 500): {response_text[:500]}...\")\n",
        "        # Si l'erreur est 400 Bad Request, et contient \"API key not valid\"\n",
        "        if e.status == 400 and \"api key not valid\" in response_text.lower():\n",
        "             print(f\"   ‚ÄºÔ∏è ERREUR SP√âCIFIQUE: La cl√© API '{api_key[:4]}...{api_key[-4:]}' semble invalide. V√©rifiez la cl√© dans le code.\")\n",
        "        elif e.status == 429:\n",
        "             print(f\"   ‚ÄºÔ∏è ERREUR SP√âCIFIQUE: Rate limit (Quota) d√©pass√©. R√©duisez GEMINI_RATE_LIMIT_PER_MINUTE ou MAX_CONCURRENT_TRANSLATIONS.\")\n",
        "\n",
        "        return None\n",
        "    except asyncio.TimeoutError:\n",
        "        print(f\"‚ùå {log_prefix} Timeout de la requ√™te API Gemini ({url}).\")\n",
        "        return None\n",
        "    except aiohttp.ClientError as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur Client aiohttp : {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur analyse JSON de la r√©ponse principale: {e}\")\n",
        "        if response_text: print(f\"--- R√©ponse texte brute (max 500 chars) ---\\n{response_text[:500]}...\\n---\")\n",
        "        return None\n",
        "    except (KeyError, IndexError) as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur acc√®s aux cl√©s/index de la r√©ponse API : {e}\")\n",
        "        if response_json: print(f\"--- R√©ponse JSON brute re√ßue ---\\n{json.dumps(response_json, indent=2, ensure_ascii=False)}\\n---\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {log_prefix} Erreur inattendue dans translate_chunk : {e.__class__.__name__}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Fonction d'Orchestration Async ---\n",
        "# ... (keep function as is) ...\n",
        "async def process_audio_pipeline_async(\n",
        "    audio_file_path: str,\n",
        "    whisper_model_size: str,\n",
        "    whisper_compute_type: str,\n",
        "    api_key: str, # La cl√© est pass√©e ici\n",
        "    chunk_size: int,\n",
        "    num_workers_to_start: int, # Explicitly pass worker count\n",
        "    video_info: Optional[Dict[str, Any]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Asynchronous pipeline to transcribe and translate audio.\n",
        "    \"\"\"\n",
        "    start_pipeline_time = time.time()\n",
        "    print(\"\\nüöÄ D√©marrage du pipeline asynchrone...\")\n",
        "\n",
        "    # --- V√©rification cl√© API au d√©but du pipeline ---\n",
        "    if not api_key or api_key == \"VOTRE_CLE_API_GEMINI_ICI\":\n",
        "        print(\"‚ùå ERREUR PIPELINE: Cl√© API Gemini non valide ou non d√©finie dans le code.\")\n",
        "        return {\"segments\": [], **(video_info or {})} # Return empty data\n",
        "\n",
        "    # --- Load Whisper Model ---\n",
        "    start_load_time = time.time()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if whisper_compute_type == \"default\":\n",
        "        compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "    else:\n",
        "        compute_type = whisper_compute_type\n",
        "\n",
        "    print(f\"üîÑ Chargement du mod√®le faster-whisper '{whisper_model_size}' (compute_type={compute_type}) sur '{device}'...\")\n",
        "    try:\n",
        "        # >>> AutoModel SHOULD BE DEFINED HERE <<<\n",
        "        # Check if it exists just before using it for debugging\n",
        "        # if 'AutoModel' not in globals() and 'AutoModel' not in locals():\n",
        "        #      print(\"DEBUG: AutoModel is STILL not defined right before usage!\")\n",
        "        #      # You might want to raise an error here or handle it\n",
        "        # else:\n",
        "        #      print(\"DEBUG: AutoModel appears to be defined now.\")\n",
        "\n",
        "        # This is the line that caused the original error\n",
        "        model = AutoModel.from_pretrained(whisper_model_size, device=device, compute_type=compute_type)\n",
        "        load_time = time.time() - start_load_time\n",
        "        print(f\"‚úÖ Mod√®le charg√© en {load_time:.2f} secondes.\")\n",
        "    except NameError as ne:\n",
        "         # Catch the specific error if it still happens\n",
        "         print(f\"‚ùå ERREUR CRITIQUE (NameError): {ne}. 'AutoModel' n'est toujours pas d√©fini.\")\n",
        "         print(\"   Cela indique un probl√®me persistant avec l'importation de faster_whisper.\")\n",
        "         print(\"   V√©rifiez l'installation manuelle et l'environnement.\")\n",
        "         return {\"segments\": [], **(video_info or {})}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur critique lors du chargement du mod√®le faster-whisper: {e}\")\n",
        "        if \"ctranslate2\" in str(e).lower():\n",
        "            print(\"   Suggestion: Assurez-vous que CTranslate2 est install√© et compatible.\")\n",
        "        # Print more details for other errors\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {\"segments\": [], **(video_info or {})} # Return empty data\n",
        "\n",
        "    # --- Setup Async Components ---\n",
        "    translation_tasks: Set[asyncio.Task] = set()\n",
        "    all_translated_segments_list: List[Dict[str, Any]] = []\n",
        "    chunk_queue = asyncio.Queue(maxsize=num_workers_to_start * 2) # Bounded queue\n",
        "    processed_chunk_count = 0\n",
        "    failed_chunk_count = 0\n",
        "    total_segments_yielded = 0\n",
        "    current_chunk_index = 0\n",
        "\n",
        "    # --- Create aiohttp Session ---\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "\n",
        "        # --- Define the Translation Worker ---\n",
        "        async def translation_worker(worker_id: int):\n",
        "            nonlocal processed_chunk_count, failed_chunk_count\n",
        "            print(f\"üë∑ [Worker-{worker_id}] D√©marr√©.\")\n",
        "            while True:\n",
        "                try:\n",
        "                    chunk_data = await chunk_queue.get()\n",
        "                    if chunk_data is None: # Sentinel\n",
        "                        chunk_queue.task_done()\n",
        "                        break\n",
        "\n",
        "                    chunk_idx, segment_chunk = chunk_data\n",
        "                    log_prefix_worker = f\"[Worker-{worker_id} | Chunk {chunk_idx+1:03d}]\"\n",
        "\n",
        "                    translated_chunk = await translate_chunk_gemini_async(\n",
        "                        session, segment_chunk, api_key, chunk_idx, len(segment_chunk)\n",
        "                    )\n",
        "\n",
        "                    if translated_chunk is not None:\n",
        "                        all_translated_segments_list.extend(translated_chunk)\n",
        "                        processed_chunk_count += 1\n",
        "                    else:\n",
        "                        print(f\"üëé {log_prefix_worker} √âchec de la traduction.\")\n",
        "                        failed_chunk_count += 1\n",
        "\n",
        "                    chunk_queue.task_done()\n",
        "\n",
        "                except asyncio.CancelledError:\n",
        "                    print(f\"üö´ [Worker-{worker_id}] T√¢che annul√©e.\")\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå [Worker-{worker_id}] Erreur inattendue : {e}\")\n",
        "                    try: chunk_queue.task_done()\n",
        "                    except ValueError: pass\n",
        "                    await asyncio.sleep(1)\n",
        "\n",
        "            print(f\"üèÅ [Worker-{worker_id}] Termin√©.\")\n",
        "\n",
        "        # --- Start Translation Workers ---\n",
        "        for i in range(num_workers_to_start):\n",
        "            task = asyncio.create_task(translation_worker(i + 1))\n",
        "            translation_tasks.add(task)\n",
        "\n",
        "        # --- Transcribe and Produce Chunks ---\n",
        "        print(f\"\\nüéôÔ∏è D√©but de la transcription (yield) et mise en file d'attente des chunks...\")\n",
        "        start_transcribe_time = time.time()\n",
        "        current_chunk: List[Dict[str, Any]] = []\n",
        "        total_audio_duration_est = video_info.get(\"duration\") if video_info else None\n",
        "\n",
        "        try:\n",
        "            # Use the loaded 'model' variable here\n",
        "            segments_generator, info = model.transcribe(audio_file_path, beam_size=5, word_timestamps=False)\n",
        "\n",
        "            detected_language = info.language\n",
        "            lang_prob = info.language_probability\n",
        "            duration_detected = info.duration\n",
        "            print(f\"   Langue d√©tect√©e: {detected_language} (Probabilit√©: {lang_prob:.2f})\")\n",
        "            print(f\"   Dur√©e audio d√©tect√©e par Whisper: {duration_detected:.2f}s\")\n",
        "            if total_audio_duration_est is None:\n",
        "                 total_audio_duration_est = duration_detected\n",
        "                 print(f\"   Utilisation de la dur√©e d√©tect√©e ({total_audio_duration_est:.1f}s) pour le %.\")\n",
        "\n",
        "            if not total_audio_duration_est or total_audio_duration_est <= 0:\n",
        "                print(\"‚ö†Ô∏è Dur√©e audio nulle ou invalide, calcul du pourcentage d√©sactiv√©.\")\n",
        "                total_audio_duration_est = None\n",
        "\n",
        "            segment_id_counter = 0\n",
        "            last_log_time = time.time()\n",
        "\n",
        "            async for segment in segments_generator:\n",
        "                seg_start = segment.start\n",
        "                seg_end = segment.end\n",
        "                text = segment.text.strip()\n",
        "                if not text: continue\n",
        "\n",
        "                duration = max(0, seg_end - seg_start)\n",
        "                progress_percentage = 0.0\n",
        "                if total_audio_duration_est and total_audio_duration_est > 0:\n",
        "                    progress_percentage = min(100.0, max(0.0, (seg_end / total_audio_duration_est) * 100))\n",
        "\n",
        "                total_segments_yielded += 1\n",
        "\n",
        "                formatted_segment = {\n",
        "                    \"id\": segment_id_counter, \"text\": text,\n",
        "                    \"start\": round(seg_start, 3), \"end\": round(seg_end, 3),\n",
        "                    \"duration\": round(duration, 3),\n",
        "                    \"progress_percentage\": round(progress_percentage, 2)\n",
        "                }\n",
        "                segment_id_counter += 1\n",
        "                current_chunk.append(formatted_segment)\n",
        "\n",
        "                current_time = time.time()\n",
        "                if current_time - last_log_time > 10.0:\n",
        "                     q_size = chunk_queue.qsize()\n",
        "                     active_translators = num_workers_to_start - translation_semaphore._value if hasattr(translation_semaphore, '_value') else 'N/A'\n",
        "                     progress_str = f\"{progress_percentage:.1f}%\" if total_audio_duration_est else f\"{seg_end:.1f}s\"\n",
        "                     print(f\"   -> Transcription: {progress_str} | Segments: {total_segments_yielded} | Queue: {q_size} | Translators Active ~{active_translators}\")\n",
        "                     last_log_time = current_time\n",
        "\n",
        "                if len(current_chunk) >= chunk_size:\n",
        "                    await chunk_queue.put((current_chunk_index, current_chunk))\n",
        "                    current_chunk_index += 1\n",
        "                    current_chunk = []\n",
        "\n",
        "            if current_chunk:\n",
        "                 await chunk_queue.put((current_chunk_index, current_chunk))\n",
        "                 current_chunk_index += 1\n",
        "\n",
        "            transcription_time = time.time() - start_transcribe_time\n",
        "            print(f\"\\n‚úÖ Transcription termin√©e en {transcription_time:.2f}s. {total_segments_yielded} segments g√©n√©r√©s.\")\n",
        "            print(f\"   Total chunks envoy√©s √† la traduction : {current_chunk_index}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur critique pendant la transcription/production de chunks: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        finally:\n",
        "            print(\"\\nüö¶ Envoi des signaux d'arr√™t aux workers...\")\n",
        "            for _ in range(num_workers_to_start):\n",
        "                try: await chunk_queue.put(None)\n",
        "                except Exception as qe: print(f\"Erreur envoi sentinel: {qe}\")\n",
        "\n",
        "            print(\"‚è≥ Attente de la fin du traitement des chunks...\")\n",
        "            await chunk_queue.join()\n",
        "            print(\"‚úÖ File d'attente vide.\")\n",
        "\n",
        "            print(\"‚è≥ Attente de la terminaison des workers...\")\n",
        "            done, pending = await asyncio.wait(translation_tasks, timeout=60)\n",
        "\n",
        "            if pending:\n",
        "                print(f\"‚ö†Ô∏è {len(pending)} workers n'ont pas termin√©, annulation...\")\n",
        "                for task in pending: task.cancel()\n",
        "                await asyncio.gather(*pending, return_exceptions=True)\n",
        "\n",
        "            print(\"‚úÖ Tous les workers de traduction ont termin√©.\")\n",
        "\n",
        "    # --- Assemble Final Result ---\n",
        "    pipeline_duration = time.time() - start_pipeline_time\n",
        "    print(\"\\n--- R√©sum√© du Pipeline Async ---\")\n",
        "    print(f\"Temps total d'ex√©cution : {pipeline_duration:.2f} secondes.\")\n",
        "    # ... (rest of summary prints) ...\n",
        "    print(f\"Segments transcrits : {total_segments_yielded}\")\n",
        "    print(f\"Chunks vis√©s pour traduction : {current_chunk_index}\") # Chunks created\n",
        "    print(f\"Chunks trait√©s avec succ√®s : {processed_chunk_count}\")\n",
        "    print(f\"Chunks √©chou√©s : {failed_chunk_count}\")\n",
        "    print(f\"Segments traduits r√©cup√©r√©s : {len(all_translated_segments_list)}\")\n",
        "\n",
        "\n",
        "    final_data = video_info.copy() if video_info else {}\n",
        "    all_translated_segments_list.sort(key=lambda x: x.get('id', float('inf')))\n",
        "    final_data[\"segments\"] = all_translated_segments_list\n",
        "\n",
        "    if failed_chunk_count > 0:\n",
        "         missed_segments_approx = failed_chunk_count * chunk_size\n",
        "         print(f\"‚ö†Ô∏è ATTENTION: {failed_chunk_count} chunks ({missed_segments_approx} segments approx.) ont √©chou√©.\")\n",
        "         print(f\"   Segments sources: {total_segments_yielded}, Segments finaux: {len(all_translated_segments_list)}\")\n",
        "\n",
        "    return final_data\n",
        "\n",
        "\n",
        "# --- Script Principal d'Ex√©cution ---\n",
        "# ... (keep main function as is, it calls the async pipeline) ...\n",
        "def main():\n",
        "    start_overall_time = time.time()\n",
        "\n",
        "    # --- Configuration & Validation ---\n",
        "    youtube_url = \"https://www.youtube.com/watch?v=7q88I_hs3Uw\"#@param {\"type\":\"string\"}\n",
        "    output_dir = \"/content/audio_output\"\n",
        "    output_filename = \"youtube_audio.mp3\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    json_translated_filename = os.path.splitext(output_path)[0] + \"_transcript_translated.json\"\n",
        "\n",
        "    # --- API Key Check (Simplified for hardcoded key) ---\n",
        "    if not GEMINI_API_KEY or GEMINI_API_KEY == \"VOTRE_CLE_API_GEMINI_ICI\":\n",
        "         print(\"üõë ERREUR: La cl√© API Gemini est manquante ou est toujours le placeholder.\")\n",
        "         print(\"   Veuillez √©diter la variable GEMINI_API_KEY en haut du script avec votre cl√© r√©elle.\")\n",
        "         return # Stop execution\n",
        "    else:\n",
        "         print(\"üîë Cl√© API Gemini charg√©e depuis le code source (hardcod√©e).\")\n",
        "\n",
        "\n",
        "    # --- Parameter Validation & Global Updates ---\n",
        "    global TRANSLATION_CHUNK_SIZE, MAX_CONCURRENT_TRANSLATIONS, GEMINI_RATE_LIMIT_PER_MINUTE\n",
        "    global translation_semaphore, gemini_rate_limiter # Allow modification\n",
        "\n",
        "    chunk_size = TRANSLATION_CHUNK_SIZE\n",
        "    if not isinstance(chunk_size, int) or chunk_size <= 0:\n",
        "        print(f\"‚ö†Ô∏è Taille de chunk invalide ({chunk_size}), utilisation de 30.\")\n",
        "        chunk_size = 30\n",
        "\n",
        "    max_workers_local = MAX_CONCURRENT_TRANSLATIONS\n",
        "    if not isinstance(max_workers_local, int) or max_workers_local <= 0:\n",
        "        print(f\"‚ö†Ô∏è Nombre de workers invalide ({max_workers_local}), utilisation de 5.\")\n",
        "        max_workers_local = 5\n",
        "\n",
        "    rate_limit_local = GEMINI_RATE_LIMIT_PER_MINUTE\n",
        "    if not isinstance(rate_limit_local, int) or rate_limit_local <= 0:\n",
        "        print(f\"‚ö†Ô∏è Rate limit invalide ({rate_limit_local}), utilisation de 15.\")\n",
        "        rate_limit_local = 15\n",
        "\n",
        "    # Re-initialize globals based on potentially validated values\n",
        "    print(f\"üîß Configuration appliqu√©e : Chunk={chunk_size}, Workers={max_workers_local}, RateLimit={rate_limit_local}/min\")\n",
        "    translation_semaphore = asyncio.Semaphore(max_workers_local)\n",
        "    gemini_rate_limiter = AsyncLimiter(rate_limit_local, 60)\n",
        "\n",
        "\n",
        "    # --- √âtape 1: T√©l√©chargement (Synchrone) ---\n",
        "    print(\"\\n\" + \"-\" * 30)\n",
        "    print(\"√âTAPE 1: T√âL√âCHARGEMENT AUDIO & METADONN√âES\")\n",
        "    print(\"-\" * 30)\n",
        "    audio_file_path, video_info = download_youtube_audio_improved(youtube_url, output_path)\n",
        "\n",
        "    if not audio_file_path:\n",
        "        print(\"\\n‚ùå √âchec critique: Impossible de t√©l√©charger ou trouver le fichier audio.\")\n",
        "        if video_info: print(\"   M√©tadonn√©es partielles:\", json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "        print(\"Pipeline arr√™t√©.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Informations Vid√©o R√©cup√©r√©es ---\")\n",
        "    if video_info: print(json.dumps(video_info, indent=2, ensure_ascii=False))\n",
        "    else: print(\"(Aucune m√©tadonn√©e r√©cup√©r√©e)\")\n",
        "    print(\"-----------------------------------\\n\")\n",
        "\n",
        "    # --- √âtape 2 & 3: Transcription et Traduction Async ---\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"√âTAPE 2 & 3: TRANSCRIPTION & TRADUCTION ASYNCHRONE\")\n",
        "    print(f\"  Mod√®le Whisper: {WHISPER_MODEL_SIZE}, Compute: {WHISPER_COMPUTE_TYPE}, Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
        "    print(f\"  Chunk Size: {chunk_size}, Max Concurrent API: {max_workers_local}, Rate Limit: {rate_limit_local}/min\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Run the async pipeline, passing the hardcoded API key\n",
        "    final_translated_data = asyncio.run(process_audio_pipeline_async(\n",
        "        audio_file_path=audio_file_path,\n",
        "        whisper_model_size=WHISPER_MODEL_SIZE,\n",
        "        whisper_compute_type=WHISPER_COMPUTE_TYPE,\n",
        "        api_key=GEMINI_API_KEY, # Passer la cl√© API globale ici\n",
        "        chunk_size=chunk_size,\n",
        "        num_workers_to_start=max_workers_local, # Pass validated worker count\n",
        "        video_info=video_info\n",
        "    ))\n",
        "\n",
        "    # --- Sauvegarde Finale ---\n",
        "    print(\"\\n\" + \"-\" * 30)\n",
        "    print(\"√âTAPE 4: SAUVEGARDE DES R√âSULTATS\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    if final_translated_data and final_translated_data.get(\"segments\"):\n",
        "        try:\n",
        "            with open(json_translated_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(final_translated_data, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"üíæ Transcription traduite sauvegard√©e dans : {json_translated_filename}\")\n",
        "            print(f\"   Nombre total de segments dans le fichier final : {len(final_translated_data['segments'])}\")\n",
        "        except IOError as e:\n",
        "            print(f\"‚ùå Erreur lors de la sauvegarde du fichier JSON traduit : {e}\")\n",
        "        except TypeError as e:\n",
        "             print(f\"‚ùå Erreur de Type lors de la s√©rialisation finale en JSON : {e}\")\n",
        "        except Exception as e:\n",
        "             print(f\"‚ùå Erreur inattendue lors de la sauvegarde JSON (traduit) : {e}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Aucune donn√©e traduite √† sauvegarder (pipeline √©chou√© ou aucun segment g√©n√©r√©/traduit).\")\n",
        "\n",
        "\n",
        "    # --- Nettoyage Optionnel ---\n",
        "    # print(\"\\n--- Nettoyage ---\")\n",
        "    # try:\n",
        "    #     if audio_file_path and os.path.exists(audio_file_path):\n",
        "    #         os.remove(audio_file_path)\n",
        "    #         print(f\"üóëÔ∏è Fichier audio supprim√© : {audio_file_path}\")\n",
        "    # except OSError as e:\n",
        "    #     print(f\"‚ùå Erreur suppression fichier audio {audio_file_path}: {e}\")\n",
        "\n",
        "\n",
        "    end_overall_time = time.time()\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"üèÅ Pipeline Global Termin√© en {end_overall_time - start_overall_time:.2f} secondes.\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "\n",
        "# --- Bloc d'ex√©cution principal ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- V√©rification Initiale des D√©pendances ---\")\n",
        "    # --- Dependency Checks ---\n",
        "    required_libs = {'yt-dlp', 'requests', 'torch', 'faster_whisper', 'ctranslate2', 'aiohttp', 'aiolimiter', 'nest_asyncio'} # Added nest_asyncio\n",
        "    installed_libs = set()\n",
        "    try: import yt_dlp; installed_libs.add('yt-dlp')\n",
        "    except ImportError: pass\n",
        "    try: import requests; installed_libs.add('requests')\n",
        "    except ImportError: pass\n",
        "    try: import torch; installed_libs.add('torch')\n",
        "    except ImportError: pass\n",
        "    try: import faster_whisper; installed_libs.add('faster_whisper')\n",
        "    except ImportError: pass # Handled above\n",
        "    try: import ctranslate2; installed_libs.add('ctranslate2')\n",
        "    except ImportError: pass\n",
        "    try: import aiohttp; installed_libs.add('aiohttp')\n",
        "    except ImportError: pass\n",
        "    try: import aiolimiter; installed_libs.add('aiolimiter')\n",
        "    except ImportError: pass\n",
        "    try: import nest_asyncio; installed_libs.add('nest_asyncio')\n",
        "    except ImportError: pass\n",
        "\n",
        "\n",
        "    missing_libs = required_libs - installed_libs\n",
        "    # Don't try to re-install faster_whisper/ctranslate2 if the top-level import failed\n",
        "    missing_libs -= {'faster_whisper', 'ctranslate2'}\n",
        "\n",
        "\n",
        "    if missing_libs:\n",
        "        print(f\"Biblioth√®ques manquantes d√©tect√©es: {', '.join(missing_libs)}\")\n",
        "        print(\"Tentative d'installation via pip...\")\n",
        "        install_cmd = ['pip', 'install', '-q'] + list(missing_libs)\n",
        "        try:\n",
        "            subprocess.run(install_cmd, check=True)\n",
        "            print(\"Installation termin√©e. Re-v√©rification...\")\n",
        "            recheck_failed = False\n",
        "            # Minimal re-check (add more if needed)\n",
        "            if 'yt-dlp' in missing_libs:\n",
        "                 try: import yt_dlp\n",
        "                 except ImportError: recheck_failed = True; print(\"Echec import yt-dlp post-install\")\n",
        "            if 'aiohttp' in missing_libs:\n",
        "                 try: import aiohttp\n",
        "                 except ImportError: recheck_failed = True; print(\"Echec import aiohttp post-install\")\n",
        "            if 'nest_asyncio' in missing_libs:\n",
        "                 try: import nest_asyncio\n",
        "                 except ImportError: recheck_failed = True; print(\"Echec import nest_asyncio post-install\")\n",
        "\n",
        "            if recheck_failed:\n",
        "                 print(\"‚ÄºÔ∏è Certaines installations semblent avoir √©chou√©. Le script risque de ne pas fonctionner.\")\n",
        "            else:\n",
        "                 print(\"‚úÖ Biblioth√®ques suppl√©mentaires install√©es.\")\n",
        "\n",
        "        except subprocess.CalledProcessError as install_err:\n",
        "            print(f\"‚ùå √âchec de l'installation pip : {install_err}\")\n",
        "            print(\"   Veuillez essayer d'installer manuellement les biblioth√®ques manquantes.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur inattendue lors de l'installation pip : {e}\")\n",
        "\n",
        "    # Check ffmpeg\n",
        "    try:\n",
        "         result = subprocess.run(['ffmpeg', '-version'], check=True, capture_output=True, timeout=5, text=True, errors='ignore')\n",
        "         print(f\"‚úÖ ffmpeg trouv√©: {result.stdout.splitlines()[0]}\")\n",
        "    except Exception as ffmpeg_err:\n",
        "         print(f\"‚ö†Ô∏è ffmpeg non trouv√© ou ne fonctionne pas ({ffmpeg_err}). Requis par faster-whisper.\")\n",
        "         print(\"   Installation sugg√©r√©e: sudo apt update && sudo apt install ffmpeg\")\n",
        "\n",
        "\n",
        "    # Check for API Key placeholder *before* running main logic\n",
        "    if not GEMINI_API_KEY or GEMINI_API_KEY == \"VOTRE_CLE_API_GEMINI_ICI\":\n",
        "        print(\"\\n‚ö†Ô∏è ALERTE: Cl√© API Gemini non configur√©e ou placeholder non remplac√© dans le script.\")\n",
        "        print(\"   Veuillez √©diter le script et remplacer 'VOTRE_CLE_API_GEMINI_ICI'.\")\n",
        "    else:\n",
        "        print(\"\\nüîë Cl√© API Gemini trouv√©e (hardcod√©e) dans le script.\")\n",
        "    print(\"-------------------------------------------\")\n",
        "\n",
        "    # Run the main processing pipeline\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0tw7nuUHQmV",
        "outputId": "37d6b4f2-ee08-48df-8561-e170b7451c93",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ faster-whisper est d√©j√† install√©.\n",
            "‚ùå Erreur finale lors de l'importation de WhisperModel/AutoModel: cannot import name 'AutoModel' from 'faster_whisper' (/usr/local/lib/python3.11/dist-packages/faster_whisper/__init__.py)\n",
            "   M√™me si l'installation a sembl√© r√©ussir, l'importation sp√©cifique a √©chou√©.\n",
            "   Essayez de red√©marrer l'environnement d'ex√©cution et r√©installez.\n",
            "üõë ALERTE: Vous n'avez pas remplac√© 'VOTRE_CLE_API_GEMINI_ICI' par votre cl√© API r√©elle.\n",
            "--- V√©rification Initiale des D√©pendances ---\n",
            "‚úÖ ffmpeg trouv√©: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "\n",
            "üîë Cl√© API Gemini trouv√©e (hardcod√©e) dans le script.\n",
            "-------------------------------------------\n",
            "üîë Cl√© API Gemini charg√©e depuis le code source (hardcod√©e).\n",
            "üîß Configuration appliqu√©e : Chunk=50, Workers=5, RateLimit=15/min\n",
            "\n",
            "------------------------------\n",
            "√âTAPE 1: T√âL√âCHARGEMENT AUDIO & METADONN√âES\n",
            "------------------------------\n",
            "‚ÑπÔ∏è  R√©cup√©ration des m√©tadonn√©es de la vid√©o...\n",
            "‚úÖ M√©tadonn√©es r√©cup√©r√©es.\n",
            "üîÑ T√©l√©chargement/V√©rification audio vers /content/audio_output/youtube_audio.mp3...\n",
            "‚ÑπÔ∏è Fichier audio '/content/audio_output/youtube_audio.mp3' existe d√©j√† et n'est pas vide. Utilisation du fichier existant.\n",
            "\n",
            "--- Informations Vid√©o R√©cup√©r√©es ---\n",
            "{\n",
            "  \"video_id\": \"7q88I_hs3Uw\",\n",
            "  \"channel_name\": \"May Ho\",\n",
            "  \"channel_url\": \"https://www.youtube.com/@MayHo\",\n",
            "  \"title\": \"„Äê‰∏≠Âúã ÈáçÂ∫Ü Chongqing Vlog Part 2„ÄëÁ¨¨‰∏ÄÊ¨°Âá∫ÂúãË∑®Âπ¥ üéâ ÂêÉÈ≠öÂêÉÂà∞Áòã‰∫ÜÊàëÂÄë 4Ê¢ùÊì∫Êì∫Âïä üêü ÈæîÁÅòÂè§ÈéÆÊúâ1700Â§öÂπ¥ÁöÑÊ≠∑Âè≤‰∫Ü!! ‚û°Ô∏èÂåóÂÄâÊñáÂâµË°óÂçÄ,ËßÇÈü≥Ê°•Ë°óÈÅìÂ°îÂù™,Â±±ÂüéÂ∑∑,ÈáçÊÖ∂È≠ÅÊòüÊ®ì\",\n",
            "  \"description\": \"‚ú¶ ‰∏äÈõÜÂΩ±ÁâáÔºöhttps://youtu.be/mrLZdfZC724?si=KFthVhiNW5kzC476\\n‚ú¶ Ë®ÇÈñ±ÊàëÂêß üòù https://bit.ly/mayhosubscribe\\n‚ú¶ Â∑•‰ΩúÂêà‰ΩúÈÇÄÁ¥Ñ mayho0110@hotmail.com\\n\\n- - -\\n\\nÁ•ùÂ§ßÂÆ∂2025Âπ¥Âø´Ê®Ç‚ú® Êñ∞ÁöÑ‰∏ÄÂπ¥Â∏åÊúõÂ§ßÂÆ∂Ë∫´È´îÂÅ•Â∫∑ Âø´Âø´Ê®ÇÊ®Ç Èå¢Ë∂äË≥∫Ë∂äÂ§öüí∞‚ù§Ô∏è\\nÂú®60Ê®ì KÊàøË∑®Âπ¥ ÁàΩÂïäÔºÅ ‰∏ÄÈÇäÂêÉÁÅ´Èçã ‰∏ÄÈÇäÂî±K ‰∏ÄÈÇäÂÄíÊï∏ !!!!!!!\\n\\nËæ£Ê§íÂíñÂï°  Â§†ÁâπÂà•üå∂Ô∏è Â§ßÂÆ∂ÂèØ‰ª•Ë©¶‰∏Ä‰∏ã ü§£\\n\\n- - -‚úàÔ∏èüõ≥Ô∏èüöó- - -\\n\\nÂ™ΩÂ™ΩÊ°ëÁöÑÊúãÂèã Travel agent\\nÊâæÂåÖËªäÔºåÈÅäËº™ÔºåË©¢ÂïèÊóÖÈÅäË©≥ÊÉÖ ÂèØ‰ª•Êâæ‰ªñÂÄëÂî∑~\\nIG Ôºögrand.holidays\\nhttps://bit.ly/4axsgmC\\n\\n- - -\\n\\n‚ú¶ Day 5  00:00 \\n\\nüìçÈæîÁÅòÂè§ÈéÆ\\nüìçÊ¢µÈ´òÁï´ÂªäÂíñÂï°È§®\\n\\n‚ú¶ Day 6 19:44 \\n\\nüìçÂçÉÂêàÊô∫ÈÅ∏È´òÁ©∫ÈÖíÂ∫ó\\nüìçÂêõ‰∫≠Ë®≠Ë®àÈÖíÂ∫ó ÔºàÂî±KÔºâ\\n\\n‚ú¶  Day 7 36:02 \\n\\nüìçËÄÅË°óÁ≥ØÁ±≥Âõ¢(Ê¥™Â¥ñÊ¥ûÂ∫ó) \\nüìçÂåóÂÄâÊñáÂâµË°óÂçÄ\\nüìçËßÇÈü≥Ê°•Ë°óÈÅìÂ°îÂù™\\nüìçË¢ÅÂ™ΩÁ¥ÖË±ÜÊπØ\\nüìçÁÅ∂Êù±ÂÆ∂\\n\\n‚ú¶  Day 8 56:44 \\n\\nüìçÂ±±ÂüéÂ∑∑\\nüìçÂçÅÂÖ´Ê¢Ø ÔºàÊ≤íÂéªÂà∞ü•π Êúâ‰∫∫ÊãçÊà≤Ôºâ\\nüìçÈáçÊÖ∂È≠ÅÊòüÊ®ì\\nüìçÂåóÊ≠•ÂúíÁÅ´Èçã\\n\\n- - -\\n\\nhttps://youtu.be/7fUQ50-SllU?si=3quhCczRHFYwVgM0\\nhttps://youtu.be/lYE176cDqek?si=h90FsgxQAsm8HgDq\\nhttps://youtu.be/PHQNL99q58s?si=Q3br2KPEv0cf9nuN\\n\\n- - -\\n\\n‚úß Instagram  https://www.instagram.com/mayho10/\\n‚úß Facebook  http://www.facebook.com/msmayho/\\n‚úß TikTok https://www.tiktok.com/@mayho10\\n‚úß Â∞èÁ¥ÖÊõ∏ https://bit.ly/MayhoXhs\\n\\n‚ñ† ÊîùÂΩ±Âô®Êùê\\nSONY ZV-1 II & ECM-G1\\niPhone 16 Pro Max \\n\\n‚ñ† Music  \\nTrackTribe\\n\\n#MayHo‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã #ÁæéÂ•ΩÁöÑ‰∏ÄÂ§© #ÊóÖË°åvlog  #ÈáçÊÖ∂\",\n",
            "  \"duration\": 4253,\n",
            "  \"upload_date\": \"20250129\",\n",
            "  \"thumbnail\": \"https://i.ytimg.com/vi/7q88I_hs3Uw/maxresdefault.jpg\"\n",
            "}\n",
            "-----------------------------------\n",
            "\n",
            "------------------------------\n",
            "√âTAPE 2 & 3: TRANSCRIPTION & TRADUCTION ASYNCHRONE\n",
            "  Mod√®le Whisper: base, Compute: default, Device: cuda\n",
            "  Chunk Size: 50, Max Concurrent API: 5, Rate Limit: 15/min\n",
            "------------------------------\n",
            "\n",
            "üöÄ D√©marrage du pipeline asynchrone...\n",
            "üîÑ Chargement du mod√®le faster-whisper 'base' (compute_type=float16) sur 'cuda'...\n",
            "‚ùå ERREUR CRITIQUE (NameError): name 'AutoModel' is not defined. 'AutoModel' n'est toujours pas d√©fini.\n",
            "   Cela indique un probl√®me persistant avec l'importation de faster_whisper.\n",
            "   V√©rifiez l'installation manuelle et l'environnement.\n",
            "\n",
            "------------------------------\n",
            "√âTAPE 4: SAUVEGARDE DES R√âSULTATS\n",
            "------------------------------\n",
            "‚ö†Ô∏è Aucune donn√©e traduite √† sauvegarder (pipeline √©chou√© ou aucun segment g√©n√©r√©/traduit).\n",
            "\n",
            "========================================\n",
            "üèÅ Pipeline Global Termin√© en 5.09 secondes.\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp requests torch faster_whisper ctranslate2 aiohttp aiolimiter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXX_A2vdZftR",
        "outputId": "e2c9812e-a9fd-456f-cac9-5612ace434c7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.3.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting faster_whisper\n",
            "  Using cached faster_whisper-1.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting ctranslate2\n",
            "  Using cached ctranslate2-4.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.15)\n",
            "Collecting aiolimiter\n",
            "  Using cached aiolimiter-1.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster_whisper) (0.30.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster_whisper) (0.21.1)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster_whisper)\n",
            "  Using cached onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting av>=11 (from faster_whisper)\n",
            "  Using cached av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from faster_whisper) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ctranslate2) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ctranslate2) (2.0.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.11/dist-packages (from ctranslate2) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.19.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster_whisper) (24.2)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster_whisper)\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster_whisper) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster_whisper) (5.29.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster_whisper)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading faster_whisper-1.1.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiolimiter-1.2.1-py3-none-any.whl (6.7 kB)\n",
            "Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, ctranslate2, av, aiolimiter, coloredlogs, onnxruntime, faster_whisper\n",
            "Successfully installed aiolimiter-1.2.1 av-14.3.0 coloredlogs-15.0.1 ctranslate2-4.6.0 faster_whisper-1.1.1 humanfriendly-10.0 onnxruntime-1.21.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "collapsed": true,
        "id": "T-lK75Wtt26K",
        "outputId": "93d86721-7b17-4262-88b0-c2a0a99fe2fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h1 style=\"color: green;\">Done! ‚úÖ</h1></br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the URL to access the App: https://journal-congo-currently-pointer.trycloudflare.com                                 |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:09] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:10] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:13] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:13] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:13] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:13] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:44:13] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:50:27] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:50:28] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Apr/2025 12:50:57] \"GET / HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# @title # **Start App**\n",
        "# @markdown <-- Start the cell.\n",
        "\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  clear_output()\n",
        "  display(HTML(f'<strong style=\"color: blue;\">Trying to launch the App on the Web(if it gets stuck here cloudflared is having issues)</strong>'))\n",
        "  p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  for line in p.stderr:\n",
        "    l = line.decode()\n",
        "    if \"trycloudflare.com \" in l:\n",
        "      clear_output()\n",
        "      display(HTML('<h1 style=\"color: green;\">Done! ‚úÖ</h1></br>'))\n",
        "      print(\"This is the URL to access the App:\", l[l.find(\"http\"):], end='')\n",
        "\n",
        "clear_output()\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(5051,)).start()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=False, port=5051)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # **Transcription & Traduction (Upload Incr√©mental + Streamed Download/Segment)**\n",
        "\n",
        "# --- Initial Cleanup ---\n",
        "print(\"--- Initial Cleanup ---\")\n",
        "!rm -f /content/audio_output/*\n",
        "!rm -f /content/audio_output_optimized/*\n",
        "!rm -f /content/audio_output_optimized_v2/*\n",
        "!rm -rf /content/audio_chunks/*\n",
        "print(\"Cleanup done.\")\n",
        "\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "import math\n",
        "import shutil\n",
        "import requests\n",
        "import sys\n",
        "import random\n",
        "import concurrent.futures # <-- Ajout pour l'ex√©cution concurrente\n",
        "import glob # <-- Ajout pour lister les fichiers\n",
        "\n",
        "# --- Library Imports & Checks ---\n",
        "print(\"\\n--- Library Imports & Checks ---\")\n",
        "try:\n",
        "    import torch\n",
        "    print(\"‚úÖ PyTorch install√©.\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå PyTorch non install√©. Veuillez l'installer.\")\n",
        "    exit()\n",
        "try:\n",
        "    from faster_whisper import WhisperModel\n",
        "    print(\"‚úÖ faster-whisper install√©.\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå faster-whisper non install√©. Veuillez l'installer.\")\n",
        "    exit()\n",
        "try:\n",
        "    from pydub import AudioSegment # Still needed if not streaming\n",
        "    print(\"‚úÖ pydub install√©.\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå pydub non install√©. Veuillez l'installer.\")\n",
        "    # Don't exit if streaming is enabled, as pydub might not be needed\n",
        "    # exit() # Commented out\n",
        "\n",
        "# --- GPU Check ---\n",
        "print(\"\\n--- GPU Check ---\")\n",
        "IS_GPU_AVAILABLE = torch.cuda.is_available()\n",
        "print(f\"‚úÖ GPU d√©tect√©: {IS_GPU_AVAILABLE}\")\n",
        "if IS_GPU_AVAILABLE:\n",
        "    try:\n",
        "        print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "        major, minor = torch.cuda.get_device_capability(0)\n",
        "        print(f\"   Compute Capability: {major}.{minor}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Impossible de r√©cup√©rer les d√©tails du GPU: {e}\")\n",
        "\n",
        "\n",
        "# --- Directories ---\n",
        "print(\"\\n--- Directories ---\")\n",
        "CHUNK_DIR = \"/content/audio_chunks\"\n",
        "OUTPUT_DIR_V2 = \"/content/audio_output_optimized_v2\"\n",
        "os.makedirs(CHUNK_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ Dossier chunks pr√™t: {CHUNK_DIR}\")\n",
        "os.makedirs(OUTPUT_DIR_V2, exist_ok=True)\n",
        "print(f\"‚úÖ Dossier sortie pr√™t: {OUTPUT_DIR_V2}\")\n",
        "\n",
        "# ==============================================================\n",
        "# --- FONCTIONS UTILS (Existing and New) ---\n",
        "# ==============================================================\n",
        "print(\"\\n--- D√©finition Fonctions Utilitaires ---\")\n",
        "\n",
        "# --- download_youtube_metadata (Extract metadata part) ---\n",
        "def download_youtube_metadata(youtube_url):\n",
        "    \"\"\"Retrieves metadata from a YouTube URL using yt-dlp.\"\"\"\n",
        "    print(f\"\\n--- R√©cup√©ration M√©tadonn√©es YouTube ---\")\n",
        "    print(f\"URL: {youtube_url}\")\n",
        "\n",
        "    video_info = None\n",
        "    try:\n",
        "        print(\"‚ÑπÔ∏è R√©cup√©ration m√©tadonn√©es via yt-dlp...\")\n",
        "        # Use --print instead of --dump-json for potentially larger outputs\n",
        "        # Or stick to dump-json if it works reliably\n",
        "        cmd = [\"yt-dlp\", \"--dump-json\", \"--encoding\", \"utf-8\", \"--socket-timeout\", \"30\", youtube_url]\n",
        "        metadata_result = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', timeout=60)\n",
        "        m = json.loads(metadata_result.stdout)\n",
        "\n",
        "        video_id_match = re.search(r\"v=([a-zA-Z0-9_-]+)\", youtube_url)\n",
        "        v_id = m.get(\"id\") or (video_id_match.group(1) if video_id_match else None) or \"UNKNOWN_ID\"\n",
        "\n",
        "        video_info = {\n",
        "            \"video_id\": v_id,\n",
        "            \"channel_name\": m.get(\"uploader\", \"N/A\"),\n",
        "            \"channel_url\": m.get(\"uploader_url\", \"N/A\"),\n",
        "            \"title\": m.get(\"title\", \"N/A\"),\n",
        "            \"description\": m.get(\"description\", \"N/A\"),\n",
        "            \"original_url\": youtube_url,\n",
        "            \"duration\": m.get(\"duration\"),\n",
        "            \"upload_date\": m.get(\"upload_date\"),\n",
        "        }\n",
        "        print(\"‚úÖ M√©tadonn√©es r√©cup√©r√©es.\")\n",
        "        print(f\"   -> ID: {video_info.get('video_id', 'N/A')}\")\n",
        "        print(f\"   -> Titre: {video_info.get('title', 'N/A')}\")\n",
        "        print(f\"   -> Dur√©e: {video_info.get('duration', 'N/A')}s\")\n",
        "        return video_info\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "         print(f\"‚ùå Timeout r√©cup√©ration m√©tadonn√©es.\")\n",
        "         return None\n",
        "    except subprocess.CalledProcessError as e:\n",
        "         print(f\"‚ùå Erreur yt-dlp (metadata): {e}\\n{e.stderr}\")\n",
        "         return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Erreur d√©codage JSON m√©tadonn√©es: {e}\")\n",
        "        print(f\"   Raw output: {metadata_result.stdout[:500]}...\") # Log part of the output\n",
        "        return {\"original_url\": youtube_url, \"video_id\": \"UNKNOWN_ERROR_JSON\"}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue (m√©tadonn√©es): {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# --- download_youtube_audio_improved (Original Download - Keep for fallback) ---\n",
        "def download_youtube_audio_improved(youtube_url, output_path, video_info):\n",
        "    \"\"\"Downloads audio from a YouTube URL using yt-dlp (saves whole file).\"\"\"\n",
        "    print(f\"\\n--- T√©l√©chargement Audio Complet (M√©thode Originale) ---\")\n",
        "    print(f\"Destination: {output_path}\")\n",
        "\n",
        "    audio_file_path = None\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    os.makedirs(output_dir, exist_ok=True) # Ensure dir exists\n",
        "\n",
        "    print(f\"üîÑ V√©rif/T√©l√©chargement audio complet -> {os.path.basename(output_path)}...\")\n",
        "    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "        print(f\"‚úÖ Fichier audio existant trouv√© et non vide. Utilisation de '{os.path.basename(output_path)}'.\")\n",
        "        audio_file_path = output_path\n",
        "    else:\n",
        "        # ... (rest of the download logic from the original function) ...\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"‚ÑπÔ∏è Fichier existant '{os.path.basename(output_path)}' est vide. Re-t√©l√©chargement...\")\n",
        "        else:\n",
        "            print(f\"‚ÑπÔ∏è Fichier '{os.path.basename(output_path)}' absent. T√©l√©chargement...\")\n",
        "        try:\n",
        "            cmd = [\n",
        "                \"yt-dlp\", \"-x\",\n",
        "                \"--audio-format\", \"mp3\",\n",
        "                \"--audio-quality\", \"0\", # 0 is best quality\n",
        "                \"--force-overwrites\",\n",
        "                \"-o\", output_path,\n",
        "                \"--encoding\", \"utf-8\",\n",
        "                \"--socket-timeout\", \"30\",\n",
        "                 youtube_url\n",
        "            ]\n",
        "            dl_res = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', timeout=1800) # 30 min timeout\n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f\"‚úÖ Audio complet t√©l√©charg√© avec succ√®s.\")\n",
        "                audio_file_path = output_path\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è yt-dlp a termin√© sans erreur, mais le fichier est absent ou vide.\")\n",
        "                print(f\"   Sortie yt-dlp:\\n{dl_res.stdout}\\n{dl_res.stderr}\")\n",
        "                return None\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"‚ùå Timeout durant le t√©l√©chargement audio complet.\")\n",
        "            return None\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Erreur yt-dlp (t√©l√©chargement complet): {e}\\n{e.stderr}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur inattendue (t√©l√©chargement complet): {e}\")\n",
        "            return None\n",
        "\n",
        "    if audio_file_path and (not os.path.exists(audio_file_path) or os.path.getsize(audio_file_path) == 0):\n",
        "        print(f\"‚ùå ERREUR FINALE: Le chemin du fichier audio est d√©fini mais le fichier est absent ou vide.\")\n",
        "        return None\n",
        "\n",
        "    return audio_file_path\n",
        "\n",
        "\n",
        "# --- NOUVELLE FONCTION: Streamed Download and Segment ---\n",
        "def download_and_segment_audio_streamed(youtube_url, output_chunk_dir, chunk_duration_s):\n",
        "    \"\"\"\n",
        "    Downloads audio using yt-dlp and pipes it directly to ffmpeg for segmentation.\n",
        "    Returns a list of generated chunk file paths and their offsets.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- T√©l√©chargement Stream√© & Segmentation Audio ---\")\n",
        "    print(f\"URL: {youtube_url}\")\n",
        "    print(f\"Destination Chunks: {output_chunk_dir}\")\n",
        "    print(f\"Dur√©e Chunk: {chunk_duration_s}s\")\n",
        "\n",
        "    os.makedirs(output_chunk_dir, exist_ok=True)\n",
        "    # Clear previous chunks from this specific potential run\n",
        "    print(\"   Nettoyage anciens chunks (si pr√©sents)...\")\n",
        "    for f in glob.glob(os.path.join(output_chunk_dir, \"stream_chunk_*.mp3\")):\n",
        "        try:\n",
        "            os.remove(f)\n",
        "        except OSError as e:\n",
        "            print(f\"   ‚ö†Ô∏è Erreur suppression ancien chunk {f}: {e}\")\n",
        "\n",
        "    # Define the output pattern for ffmpeg\n",
        "    # Using a simple pattern - can be adjusted if needed\n",
        "    output_pattern = os.path.join(output_chunk_dir, \"stream_chunk_%04d.mp3\")\n",
        "\n",
        "    # --- yt-dlp Command ---\n",
        "    # Select best audio-only format\n",
        "    # Output raw stream to stdout (-)\n",
        "    yt_dlp_cmd = [\n",
        "        \"yt-dlp\",\n",
        "        \"-f\", \"bestaudio/best\", # Prioritize best audio, fallback to best overall if no dedicated audio\n",
        "        \"--socket-timeout\", \"60\", # Longer timeout for potential streaming stalls\n",
        "        \"-o\", \"-\",  # Output to stdout\n",
        "        youtube_url\n",
        "    ]\n",
        "\n",
        "    # --- ffmpeg Command ---\n",
        "    # Read from stdin (pipe:0)\n",
        "    # No video (-vn)\n",
        "    # Segment muxer (-f segment)\n",
        "    # Segment time (-segment_time)\n",
        "    # Copy codec if possible, otherwise re-encode to mp3 (-c:a mp3)\n",
        "    #   Using mp3 for better compatibility with Whisper, though slower than copy\n",
        "    # Set audio sampling rate (-ar 16000) - often good for ASR\n",
        "    # Set audio bitrate (-ab 64k or 128k) - control quality/size\n",
        "    ffmpeg_cmd = [\n",
        "        \"ffmpeg\",\n",
        "        \"-i\", \"pipe:0\",      # Read from stdin\n",
        "        \"-vn\",               # No video\n",
        "        \"-f\", \"segment\",\n",
        "        \"-segment_time\", str(chunk_duration_s),\n",
        "        \"-c:a\", \"mp3\",       # Encode to MP3\n",
        "        \"-ar\", \"16000\",      # Sample rate 16kHz\n",
        "        \"-ab\", \"128k\",       # Audio bitrate\n",
        "        \"-reset_timestamps\", \"1\", # Reset timestamps for each segment\n",
        "        \"-map\", \"0:a\",       # Ensure only audio stream is mapped\n",
        "        \"-sc_threshold\", \"0\", # Disable scene change detection if it interferes\n",
        "        output_pattern\n",
        "    ]\n",
        "\n",
        "    print(f\"   Commande yt-dlp: {' '.join(yt_dlp_cmd)}\")\n",
        "    print(f\"   Commande ffmpeg: {' '.join(ffmpeg_cmd)}\")\n",
        "    print(\"   Lancement du pipeline yt-dlp | ffmpeg...\")\n",
        "\n",
        "    yt_dlp_process = None\n",
        "    ffmpeg_process = None\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Start yt-dlp, piping its stdout\n",
        "        yt_dlp_process = subprocess.Popen(yt_dlp_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8')\n",
        "\n",
        "        # Start ffmpeg, reading from yt-dlp's stdout, piping its own stderr\n",
        "        ffmpeg_process = subprocess.Popen(ffmpeg_cmd, stdin=yt_dlp_process.stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8')\n",
        "\n",
        "        # Allow yt-dlp's stdout pipe to be closed when ffmpeg finishes reading\n",
        "        # (though it should close when yt-dlp finishes) - this might not be strictly necessary\n",
        "        # yt_dlp_process.stdout.close() # Be careful with closing pipes prematurely\n",
        "\n",
        "        # Wait for ffmpeg to finish. It implicitly waits for input from yt-dlp.\n",
        "        # Read stderr from ffmpeg in real-time (optional, good for debugging)\n",
        "        ffmpeg_stderr_output = \"\"\n",
        "        # Note: Reading stderr this way can block if there's a lot of output.\n",
        "        # A more robust solution might involve threading for stderr reading.\n",
        "        # For now, we'll capture it after completion.\n",
        "\n",
        "        ffmpeg_stdout, ffmpeg_stderr = ffmpeg_process.communicate(timeout=3600) # Wait up to 1 hour\n",
        "        ffmpeg_returncode = ffmpeg_process.returncode\n",
        "\n",
        "        # Check yt-dlp's status *after* ffmpeg is done (or has errored out)\n",
        "        # Capture any remaining stderr from yt-dlp\n",
        "        yt_dlp_stderr_output = \"\"\n",
        "        try:\n",
        "            # Poll yt-dlp to see if it exited, capture remaining stderr\n",
        "            yt_dlp_process.poll()\n",
        "            yt_dlp_stdout_res, yt_dlp_stderr_res = yt_dlp_process.communicate(timeout=10) # Short timeout to grab remaining output\n",
        "            yt_dlp_stderr_output = yt_dlp_stderr_res\n",
        "            yt_dlp_returncode = yt_dlp_process.returncode\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"   ‚ö†Ô∏è yt-dlp process did not terminate quickly after ffmpeg. Forcing termination.\")\n",
        "            yt_dlp_process.terminate()\n",
        "            try:\n",
        "                yt_dlp_process.wait(timeout=5)\n",
        "            except subprocess.TimeoutExpired:\n",
        "                yt_dlp_process.kill()\n",
        "            yt_dlp_returncode = yt_dlp_process.returncode # Might be None or signal code\n",
        "            yt_dlp_stderr_output = \"yt-dlp timed out after ffmpeg finished.\"\n",
        "\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"   Pipeline termin√© en {elapsed_time:.2f}s.\")\n",
        "\n",
        "        # --- Error Checking ---\n",
        "        if ffmpeg_returncode != 0:\n",
        "            print(f\"‚ùå Erreur FFMPEG (Code: {ffmpeg_returncode}). Impossible de segmenter.\")\n",
        "            print(\"   --- FFMPEG Stderr ---\")\n",
        "            print(ffmpeg_stderr)\n",
        "            print(\"   --------------------\")\n",
        "            # Also print yt-dlp stderr in case it contributed\n",
        "            if yt_dlp_stderr_output:\n",
        "                 print(\"   --- yt-dlp Stderr ---\")\n",
        "                 print(yt_dlp_stderr_output)\n",
        "                 print(\"   --------------------\")\n",
        "            return [], []\n",
        "\n",
        "        if yt_dlp_returncode != 0 and yt_dlp_returncode is not None:\n",
        "             print(f\"‚ö†Ô∏è Avertissement: yt-dlp a termin√© avec un code d'erreur ({yt_dlp_returncode}) mais ffmpeg a r√©ussi.\")\n",
        "             print(\"   --- yt-dlp Stderr ---\")\n",
        "             print(yt_dlp_stderr_output)\n",
        "             print(\"   --------------------\")\n",
        "             # Continue, as ffmpeg might have processed partial data\n",
        "\n",
        "        print(\"‚úÖ Pipeline yt-dlp | ffmpeg termin√© avec succ√®s (ffmpeg code 0).\")\n",
        "        if ffmpeg_stderr: # Log ffmpeg stderr even on success, might contain useful info/warnings\n",
        "             # Filter common progress lines if too verbose\n",
        "             filtered_stderr = \"\\n\".join(line for line in ffmpeg_stderr.splitlines() if not line.strip().startswith('size=') and not line.strip().startswith('frame='))\n",
        "             if filtered_stderr.strip():\n",
        "                 print(\"   --- FFMPEG Stderr (Info/Warnings) ---\")\n",
        "                 print(filtered_stderr)\n",
        "                 print(\"   -----------------------------------\")\n",
        "\n",
        "\n",
        "        # --- Collect generated chunks ---\n",
        "        generated_files = sorted(glob.glob(os.path.join(output_chunk_dir, \"stream_chunk_*.mp3\")))\n",
        "\n",
        "        if not generated_files:\n",
        "            print(\"‚ùå Pipeline termin√© mais aucun chunk audio trouv√© !\")\n",
        "            return [], []\n",
        "\n",
        "        print(f\"‚úÖ {len(generated_files)} chunks audio cr√©√©s dans {output_chunk_dir}\")\n",
        "\n",
        "        # Calculate offsets (simple fixed duration)\n",
        "        offsets = [i * chunk_duration_s for i in range(len(generated_files))]\n",
        "\n",
        "        return generated_files, offsets\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"‚ùå Timeout durant l'ex√©cution du pipeline yt-dlp | ffmpeg.\")\n",
        "        if ffmpeg_process: ffmpeg_process.kill()\n",
        "        if yt_dlp_process: yt_dlp_process.kill()\n",
        "        return [], []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue durant le pipeline: {e}\")\n",
        "        traceback.print_exc()\n",
        "        if ffmpeg_process: ffmpeg_process.kill()\n",
        "        if yt_dlp_process: yt_dlp_process.kill()\n",
        "        return [], []\n",
        "\n",
        "\n",
        "# --- split_audio_by_fixed_duration (Original Split - Keep for fallback) ---\n",
        "def split_audio_by_fixed_duration(input_audio_path, output_chunk_dir, chunk_duration_s):\n",
        "    \"\"\"Splits an audio file into fixed duration chunks using pydub.\"\"\"\n",
        "    # ... (code inchang√© - utiliser celui de la question originale) ...\n",
        "    print(f\"\\nüîä D√©coupage audio (pydub) '{os.path.basename(input_audio_path)}' en chunks de {chunk_duration_s}s...\")\n",
        "    # ... (rest of the function) ...\n",
        "    if not os.path.exists(input_audio_path) or os.path.getsize(input_audio_path) == 0:\n",
        "        print(f\"‚ùå Fichier audio d'entr√©e absent ou vide: {input_audio_path}\")\n",
        "        return [], []\n",
        "    # Check if pydub is available\n",
        "    try:\n",
        "        from pydub import AudioSegment\n",
        "    except ImportError:\n",
        "        print(\"‚ùå pydub non install√©. Impossible d'utiliser cette m√©thode de d√©coupage.\")\n",
        "        return [], []\n",
        "\n",
        "    if chunk_duration_s <= 0:\n",
        "        print(f\"‚ùå Dur√©e de chunk invalide ({chunk_duration_s}s). Doit √™tre positive.\")\n",
        "        return [], []\n",
        "\n",
        "    paths, offsets = [], []\n",
        "    chunk_dur_ms = int(chunk_duration_s * 1000)\n",
        "\n",
        "    try:\n",
        "        print(f\"   Chargement de l'audio avec pydub...\")\n",
        "        audio = AudioSegment.from_file(input_audio_path)\n",
        "        total_dur_ms = len(audio)\n",
        "        total_dur_s = total_dur_ms / 1000.0\n",
        "        print(f\"   Audio charg√© ({total_dur_s:.2f}s). D√©coupage en cours...\")\n",
        "\n",
        "        if total_dur_ms <= 0:\n",
        "            print(f\"‚ùå La dur√©e de l'audio est nulle ou n√©gative.\")\n",
        "            return [], []\n",
        "\n",
        "        start_ms = 0\n",
        "        idx = 0\n",
        "        while start_ms < total_dur_ms:\n",
        "            end_ms = start_ms + chunk_dur_ms\n",
        "            chunk = audio[start_ms:end_ms]\n",
        "            cur_dur_ms = len(chunk)\n",
        "            cur_dur_s = cur_dur_ms / 1000.0\n",
        "            cur_start_s = start_ms / 1000.0\n",
        "\n",
        "            if cur_dur_s <= 0.1: # Skip very short segments\n",
        "                start_ms = end_ms\n",
        "                continue\n",
        "\n",
        "            # Use a simpler filename matching the streamed approach for consistency? Or keep detailed?\n",
        "            # Let's keep detailed for pydub for now.\n",
        "            start_fmt = f\"{cur_start_s:.3f}\".replace('.', '_')\n",
        "            dur_fmt = f\"{cur_dur_s:.3f}\".replace('.', '_')\n",
        "            fname = f\"pydub_chunk_{idx:04d}_start{start_fmt}s_dur{dur_fmt}s.mp3\"\n",
        "            fpath = os.path.join(output_chunk_dir, fname)\n",
        "\n",
        "            try:\n",
        "                # Ensure target directory exists before export\n",
        "                os.makedirs(os.path.dirname(fpath), exist_ok=True)\n",
        "                # Export with parameters similar to ffmpeg if possible\n",
        "                chunk.export(fpath, format=\"mp3\", parameters=[\"-ar\", \"16000\", \"-ab\", \"128k\"])\n",
        "                if os.path.exists(fpath) and os.path.getsize(fpath) > 0:\n",
        "                    paths.append(fpath)\n",
        "                    offsets.append(cur_start_s)\n",
        "                else:\n",
        "                    print(f\"     ‚ö†Ô∏è √âchec export ou fichier vide cr√©√©: {fpath}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Erreur lors de l'export du chunk {fpath}: {e}\")\n",
        "\n",
        "            start_ms = end_ms\n",
        "            idx += 1\n",
        "\n",
        "        if not paths:\n",
        "            print(\"‚ùå Aucun chunk n'a √©t√© cr√©√© ou sauvegard√© avec pydub.\")\n",
        "        else:\n",
        "            print(f\"‚úÖ D√©coupage pydub termin√©: {len(paths)} chunks cr√©√©s dans {output_chunk_dir}\")\n",
        "\n",
        "        return paths, offsets\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Erreur pydub: Fichier d'entr√©e non trouv√©: {input_audio_path}\")\n",
        "        return [], []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur inattendue durant le d√©coupage pydub: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return [], []\n",
        "\n",
        "\n",
        "# --- transcribe_audio_faster (INCHANG√âE) ---\n",
        "def transcribe_audio_faster(file_path, model, chunk_offset_s, beam_size, vad_filter, vad_min_silence_ms, chunk_index, total_chunks):\n",
        "    \"\"\"Transcribes a single audio file chunk using the preloaded faster-whisper model.\"\"\"\n",
        "    start_time_transcribe = time.time()\n",
        "    # Make log slightly different for parallel execution clarity\n",
        "    log_prefix = f\"üéôÔ∏è [Transcribe {chunk_index + 1}/{total_chunks}]\"\n",
        "    print(f\"{log_prefix} D√©marrage: {os.path.basename(file_path)} (Offset Global: {chunk_offset_s:.3f}s)\")\n",
        "\n",
        "    segments_data = []\n",
        "    total_duration = 0\n",
        "    last_prog = -1\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"{log_prefix} ‚ùå Mod√®le Whisper non charg√©!\")\n",
        "        return {\"segments\": [], \"success\": False} # Indicate failure\n",
        "\n",
        "    try:\n",
        "        vad_params = {\"min_silence_duration_ms\": vad_min_silence_ms} if vad_filter else None\n",
        "\n",
        "        segments_generator, info = model.transcribe(\n",
        "            file_path,\n",
        "            beam_size=beam_size,\n",
        "            vad_filter=vad_filter,\n",
        "            vad_parameters=vad_params\n",
        "        )\n",
        "\n",
        "        lang, prob, total_duration = info.language, info.language_probability, info.duration\n",
        "        # print(f\"{log_prefix}   Infos chunk: Lang='{lang}' (Conf: {prob:.2f}), Dur√©e: {total_duration:.2f}s\") # Peut √™tre trop verbeux en parall√®le\n",
        "\n",
        "        if total_duration <= 0.1: # Use a small threshold\n",
        "            print(f\"{log_prefix}   ‚ö†Ô∏è Dur√©e du chunk audio <= 0.1s selon Whisper. Ignor√©.\")\n",
        "            transcription_time = time.time() - start_time_transcribe\n",
        "            # print(f\"{log_prefix}   üïí Termin√© (vide) en {transcription_time:.2f}s.\")\n",
        "            return {\"segments\": [], \"success\": True} # Success, but no segments\n",
        "\n",
        "        # print(f\"{log_prefix}   Traitement des segments...\") # Peut √™tre trop verbeux\n",
        "        seg_count = 0\n",
        "        for segment in segments_generator:\n",
        "            seg_count += 1\n",
        "            start_local, end_local = segment.start, segment.end\n",
        "            duration_local = max(0, end_local - start_local)\n",
        "            text = segment.text.strip() if segment.text else \"\"\n",
        "            # Ensure start/end are valid before adding offset\n",
        "            if start_local is None or end_local is None or start_local < 0 or end_local < 0:\n",
        "                 print(f\"{log_prefix}   ‚ö†Ô∏è Segment invalide ignor√©: start={start_local}, end={end_local}\")\n",
        "                 continue\n",
        "\n",
        "            start_global = round(start_local + chunk_offset_s, 3)\n",
        "            duration_rounded = round(duration_local, 3)\n",
        "\n",
        "            # Progression log less frequent in parallel\n",
        "            # prog = min(100.0, (end_local / total_duration) * 100) if total_duration > 0 else 0\n",
        "            # rounded_prog = math.floor(prog)\n",
        "            # if rounded_prog > last_prog and (rounded_prog % 25 == 0 or rounded_prog >= 99): # Log every 25%\n",
        "            #     sys.stdout.write(f\"\\r{log_prefix}   Progression: {prog:.0f}% \")\n",
        "            #     sys.stdout.flush()\n",
        "            #     last_prog = rounded_prog\n",
        "\n",
        "            segments_data.append({\n",
        "                \"text\": text,\n",
        "                \"start\": start_global,\n",
        "                \"duration\": duration_rounded\n",
        "            })\n",
        "        # sys.stdout.write(\"\\n\")\n",
        "        # sys.stdout.flush()\n",
        "\n",
        "        transcription_time = time.time() - start_time_transcribe\n",
        "        print(f\"{log_prefix} ‚úÖ Termin√© en {transcription_time:.2f}s. {seg_count} segments trouv√©s.\")\n",
        "        if seg_count == 0:\n",
        "            print(f\"{log_prefix}   ‚ö†Ô∏è Aucun segment de parole trouv√©.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{log_prefix} ‚ùå Erreur durant la transcription: {e}\")\n",
        "        traceback.print_exc()\n",
        "        # sys.stdout.write(\"\\n\")\n",
        "        # sys.stdout.flush()\n",
        "        return {\"segments\": [], \"success\": False} # Indicate failure\n",
        "\n",
        "    return {\"segments\": segments_data, \"success\": True}\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# --- FONCTIONS TRADUCTION (Inchang√©es mais appel√©es par // workers) ---\n",
        "# ==============================================================\n",
        "print(\"\\n--- D√©finition Fonctions Traduction ---\")\n",
        "# --- chunk_list (INCHANG√âE) ---\n",
        "def chunk_list(lst, n):\n",
        "    # ... (code inchang√©) ...\n",
        "    if not isinstance(lst, list): raise TypeError(\"Input must be a list.\")\n",
        "    if n <= 0: raise ValueError(\"Chunk size must be positive.\")\n",
        "    for i in range(0, len(lst), n): yield lst[i:i + n]\n",
        "\n",
        "# --- extract_json_from_response (INCHANG√âE) ---\n",
        "# (Fonction inchang√©e)\n",
        "\n",
        "# --- translate_audio_chunk_segments (INCHANG√âE EN ELLE-M√äME) ---\n",
        "def translate_audio_chunk_segments(transcript_segments, api_key, audio_chunk_index, total_audio_chunks, segment_chunk_index=None, total_segment_chunks=None):\n",
        "    # ... (code inchang√© - utiliser celui de la question originale) ...\n",
        "    # Note: Les logs √† l'int√©rieur incluent d√©j√† les index, ce qui est bien pour le parall√®le.\n",
        "    if not transcript_segments:\n",
        "        log_prefix = f\"   >> [Translate AC:{audio_chunk_index + 1}/{total_audio_chunks}]\" # AC = Audio Chunk\n",
        "        if segment_chunk_index is not None: log_prefix += f\"[SC:{segment_chunk_index+1}/{total_segment_chunks}]\" # SC = Segment Chunk\n",
        "        # print(f\"{log_prefix} Aucun segment √† traduire.\", flush=True) # Peut √™tre trop verbeux\n",
        "        return []\n",
        "\n",
        "    start_time = time.time()\n",
        "    num_segments = len(transcript_segments)\n",
        "\n",
        "    log_prefix = f\"   >> [Translate AC:{audio_chunk_index + 1}/{total_audio_chunks}\"\n",
        "    if segment_chunk_index is not None and total_segment_chunks is not None:\n",
        "        log_prefix += f\"|SC:{segment_chunk_index + 1}/{total_segment_chunks}\"\n",
        "    log_prefix += \"]\"\n",
        "\n",
        "    # Rendre le log initial plus discret\n",
        "    # print(f\"{log_prefix} üîÑ Traduction de {num_segments} segments...\", flush=True)\n",
        "\n",
        "    if not api_key or not api_key.startswith(\"AIzaSy\"):\n",
        "         print(f\"{log_prefix} ‚ùå Cl√© API Gemini invalide ou manquante.\", flush=True)\n",
        "         return None # Error\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}\"\n",
        "\n",
        "    try:\n",
        "        # Reduce precision slightly for smaller payload? Might not be needed.\n",
        "        # segments_for_json = [{**s, 'start': round(s['start'], 2), 'duration': round(s['duration'], 2)} for s in transcript_segments]\n",
        "        # transcript_str = json.dumps(segments_for_json, ensure_ascii=False, separators=(',', ':'))\n",
        "        transcript_str = json.dumps(transcript_segments, ensure_ascii=False, separators=(',', ':'))\n",
        "\n",
        "    except TypeError as e:\n",
        "        print(f\"{log_prefix} ‚ùå Erreur pr√©paration JSON pour API: {e}\", flush=True)\n",
        "        return None # Error\n",
        "\n",
        "    # Prompt inchang√©\n",
        "    prompt = (\n",
        "        \"You are an expert multilingual transcriber and translator.\\n\"\n",
        "        \"INPUT: A JSON array. Each object in the array represents a transcript segment and has keys 'text', 'start', and 'duration'.\\n\"\n",
        "        \"TASK: Process EACH segment object in the input JSON array:\\n\"\n",
        "        \"1. Identify the original language of the 'text' field.\\n\"\n",
        "        \"2. Add a new key 'text_english' containing the English translation of the original 'text'.\\n\"\n",
        "        \"3. Add a new key 'text_french' containing the French translation of the original 'text'.\\n\"\n",
        "        \"4. IMPORTANT: Preserve ALL original keys ('text', 'start', 'duration') and their values.\\n\"\n",
        "        \"OUTPUT: Return ONLY the modified JSON array containing all processed segments. Ensure the output is a single, valid JSON array. Do NOT include any extra text, explanations, or markdown formatting (like ```json ... ```) outside the JSON array itself.\\n\\n\"\n",
        "        \"INPUT JSON:\\n\"\n",
        "        f\"{transcript_str}\"\n",
        "    )\n",
        "    # Payload inchang√©\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": 0.2,\n",
        "            \"maxOutputTokens\": 8192, # Consider Flash's context window if needed, but output limit is key here\n",
        "            \"response_mime_type\": \"application/json\" # Request JSON directly\n",
        "        },\n",
        "        \"safetySettings\": [ # Keep safety settings\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            # ... other categories ...\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    max_retries = 3\n",
        "    base_delay = 4 # Slightly increased base delay\n",
        "    raw_text_response = \"\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        if attempt > 0:\n",
        "            delay = base_delay * (2 ** attempt) + random.uniform(0, 2) # More jitter\n",
        "            # print(f\"{log_prefix} ‚è≥ Tentative Traduction {attempt + 1}/{max_retries} apr√®s {delay:.1f}s...\", flush=True) # Verbeux\n",
        "            time.sleep(delay)\n",
        "\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=payload, timeout=240) # Keep timeout\n",
        "\n",
        "            if response.status_code == 429: # Rate limit\n",
        "                retry_after = 25 + random.uniform(0, 10) # Longer base wait for rate limit\n",
        "                print(f\"{log_prefix} ‚ö†Ô∏è Erreur 429 (Rate Limit API). Tentative {attempt + 1}/{max_retries}. Re-essai dans {retry_after:.1f}s...\", flush=True)\n",
        "                time.sleep(retry_after)\n",
        "                if attempt == max_retries - 1:\n",
        "                     print(f\"{log_prefix} ‚ùå Rate Limit persiste.\", flush=True)\n",
        "                     return None # Final failure\n",
        "                continue # Next attempt\n",
        "\n",
        "            response.raise_for_status() # Other HTTP errors\n",
        "            response_data = response.json()\n",
        "\n",
        "            # --- Validation R√©ponse Gemini (inchang√©e) ---\n",
        "            if not response_data.get(\"candidates\"):\n",
        "                # ... (error handling inchang√©) ...\n",
        "                prompt_feedback = response_data.get(\"promptFeedback\", {})\n",
        "                block_reason = prompt_feedback.get(\"blockReason\")\n",
        "                safety_ratings = prompt_feedback.get(\"safetyRatings\", [])\n",
        "                error_message = f\"Aucun 'candidates' dans la r√©ponse API.\"\n",
        "                if block_reason: error_message += f\" Raison blocage: {block_reason}.\"\n",
        "                if safety_ratings: error_message += f\" Safety Ratings: {safety_ratings}\"\n",
        "                print(f\"{log_prefix} ‚ùå Erreur API: {error_message}\", flush=True)\n",
        "                if block_reason == \"SAFETY\": return None # Don't retry SAFETY block\n",
        "                if attempt == max_retries - 1: return None # Final failure\n",
        "                continue # Retry other errors\n",
        "\n",
        "            candidate = response_data[\"candidates\"][0]\n",
        "            finish_reason = candidate.get(\"finishReason\")\n",
        "\n",
        "            if finish_reason not in [\"STOP\", \"MAX_TOKENS\"]:\n",
        "                # ... (error handling inchang√©) ...\n",
        "                safety_ratings = candidate.get(\"safetyRatings\", [])\n",
        "                print(f\"{log_prefix} ‚ùå Fin anormale API: {finish_reason}.\", flush=True)\n",
        "                if safety_ratings: print(f\"      -> Safety Ratings: {safety_ratings}\", flush=True)\n",
        "                if finish_reason == \"SAFETY\": return None # Don't retry SAFETY finish reason\n",
        "                if attempt == max_retries - 1: return None # Final failure\n",
        "                continue # Retry other abnormal finishes\n",
        "\n",
        "            if not (\"content\" in candidate and \"parts\" in candidate[\"content\"] and\n",
        "                    candidate[\"content\"][\"parts\"] and \"text\" in candidate[\"content\"][\"parts\"][0]):\n",
        "                # ... (error handling inchang√©) ...\n",
        "                 print(f\"{log_prefix} ‚ùå Structure de contenu invalide.\", flush=True)\n",
        "                 if attempt == max_retries - 1: return None\n",
        "                 continue # Retry\n",
        "\n",
        "            # --- Extraction et Parsing JSON (inchang√©) ---\n",
        "            raw_text_response = candidate[\"content\"][\"parts\"][0][\"text\"]\n",
        "            # Since we requested application/json, it should already be parsed if valid\n",
        "            # If it's still text, Gemini might have ignored the mime type, try parsing\n",
        "            try:\n",
        "                # If response_mime_type worked, raw_text_response *is* the JSON object/array\n",
        "                # If not, it's a string that might contain JSON\n",
        "                 if isinstance(raw_text_response, (list, dict)):\n",
        "                     result_json = raw_text_response # Already parsed by requests\n",
        "                 else:\n",
        "                      # Try parsing the text content\n",
        "                      extracted_json_string = extract_json_from_response(raw_text_response) # Use extractor just in case\n",
        "                      result_json = json.loads(extracted_json_string)\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                 # ... (error handling inchang√©) ...\n",
        "                 print(f\"{log_prefix} ‚ùå Erreur d√©codage JSON: {e}\", flush=True)\n",
        "                 print(f\"      R√©ponse brute:\\n{str(raw_text_response)[:500]}...\")\n",
        "                 if attempt == max_retries - 1: return None\n",
        "                 continue # Retry\n",
        "\n",
        "            # --- Validation R√©sultat Pars√© (inchang√©e) ---\n",
        "            if not isinstance(result_json, list):\n",
        "                # ... (error handling inchang√©) ...\n",
        "                print(f\"{log_prefix} ‚ùå JSON d√©cod√© n'est pas une liste.\", flush=True)\n",
        "                if attempt == max_retries - 1: return None\n",
        "                continue\n",
        "\n",
        "            if len(result_json) != num_segments:\n",
        "                 # Minor warning, proceed but log\n",
        "                 print(f\"{log_prefix} ‚ö†Ô∏è Nombre segments retourn√©s ({len(result_json)}) != entr√©e ({num_segments}).\", flush=True)\n",
        "\n",
        "            if result_json and isinstance(result_json[0], dict):\n",
        "                 if 'text_english' not in result_json[0] or 'text_french' not in result_json[0]:\n",
        "                     print(f\"{log_prefix} ‚ö†Ô∏è Cl√©s traduites manquantes premier segment.\", flush=True)\n",
        "\n",
        "            # --- Succ√®s ---\n",
        "            elapsed_time = time.time() - start_time\n",
        "            # Log de succ√®s plus discret\n",
        "            # print(f\"{log_prefix} ‚úÖ Traduction r√©ussie en {elapsed_time:.2f}s.\", flush=True)\n",
        "            if finish_reason == \"MAX_TOKENS\":\n",
        "                 print(f\"{log_prefix} ‚ö†Ô∏è Attention: MAX_TOKENS atteint.\", flush=True)\n",
        "            return result_json # Success\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "             print(f\"{log_prefix} ‚ö†Ô∏è Timeout API (Tentative {attempt + 1}/{max_retries}).\", flush=True)\n",
        "             if attempt == max_retries - 1: return None # Final failure\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "             print(f\"{log_prefix} ‚ùå Erreur R√©seau/HTTP API (Tentative {attempt + 1}/{max_retries}): {e}\", flush=True)\n",
        "             if attempt == max_retries - 1: return None # Final failure\n",
        "             time.sleep(base_delay * (2 ** attempt) + random.uniform(0, 1)) # Wait before retry\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"{log_prefix} ‚ùå Erreur inattendue traduction (Tentative {attempt + 1}/{max_retries}): {e}\", flush=True)\n",
        "             traceback.print_exc()\n",
        "             if attempt == max_retries - 1: return None # Final failure\n",
        "             time.sleep(base_delay * (2 ** attempt) + random.uniform(0, 1)) # Wait\n",
        "\n",
        "\n",
        "    # Si la boucle se termine sans succ√®s\n",
        "    print(f\"{log_prefix} ‚ùå Traduction √âCHOU√âE apr√®s {max_retries} tentatives.\", flush=True)\n",
        "    return None # Final failure\n",
        "\n",
        "\n",
        "# =======================================================================\n",
        "# --- Configuration et Ex√©cution Principale (AVEC NOUVELLES OPTIONS) ---\n",
        "# =======================================================================\n",
        "print(\"\\n\\n\" + \"=\"*40 + \"\\n--- Configuration Principale ---\\n\" + \"=\"*40)\n",
        "\n",
        "# --- Param√®tres G√©n√©raux ---\n",
        "youtube_url = \"https://www.youtube.com/watch?v=v7RRTGdTquc\" #@param {\"type\":\"string\"}\n",
        "model_size = \"large-v3\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v3\"]\n",
        "gemini_api_key = \"AIzaSyAI2CLDtikFeKi5P6UxgXi9D9bMwYA6l8w\" #@param {\"type\":\"string\"}\n",
        "\n",
        "#@markdown _____\n",
        "#@markdown ### **üöÄ Options de Vitesse (Exp√©rimental)**\n",
        "#@markdown Activer pour t√©l√©charger et d√©couper l'audio en flux continu (n√©cessite `ffmpeg` et `yt-dlp`). Potentiellement plus rapide, moins d'espace disque temporaire.\n",
        "enable_streaming_download = True #@param {type:\"boolean\"}\n",
        "#@markdown Nombre de transcriptions Whisper √† ex√©cuter en parall√®le (0 ou 1 = s√©quentiel). Augmenter peut acc√©l√©rer sur CPU multi-coeur ou si le GPU n'est pas satur√© par une seule t√¢che. Typiquement 2-4. Attention: augmente l'utilisation de la RAM/VRAM.\n",
        "parallel_transcription_workers = 1 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown _____\n",
        "#@markdown ### Param√®tres de D√©coupage\n",
        "#@markdown *Utilis√© par le t√©l√©chargement stream√© OU le d√©coupage pydub classique si le streaming est d√©sactiv√©.*\n",
        "split_duration_minutes = 10 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "#@markdown _____\n",
        "#@markdown ### Param√®tres de Transcription (Whisper)\n",
        "#@markdown *Si activ√©, Whisper essaie de filtrer les silences. Utile si `split_duration_minutes` est grand.*\n",
        "use_vad_during_transcription = False #@param {type:\"boolean\"}\n",
        "vad_silence_duration_ms = 500 #@param {type:\"slider\", min:100, max:2000, step:50}\n",
        "beam_search_size = 5\n",
        "\n",
        "#@markdown _____\n",
        "#@markdown ### Param√®tres de Traduction (Gemini)\n",
        "#@markdown *D√©couper la liste des segments transcrits en plus petits groupes pour l'API Gemini (√©vite les erreurs de taille de prompt/r√©ponse).*\n",
        "enable_segment_chunking = True #@param {type:\"boolean\"}\n",
        "max_segments_per_translation_chunk = 30 #@param {type:\"integer\"}\n",
        "#@markdown *Nombre d'appels API Gemini simultan√©s. Augmenter acc√©l√®re la traduction mais peut atteindre les limites de taux de l'API.*\n",
        "max_concurrent_translation_tasks = 14 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown _____\n",
        "#@markdown ### Param√®tres d'Upload Incr√©mental\n",
        "enable_incremental_upload = True #@param {type:\"boolean\"}\n",
        "upload_chunk_url = \"default\"  #@param {\"type\":\"string\"}\n",
        "if not upload_chunk_url or upload_chunk_url.strip().lower() == \"default\":\n",
        "    upload_chunk_url = \"https://qingplay.pythonanywhere.com/update_transcript_chunk\"\n",
        "\n",
        "\n",
        "# --- Variables Globales ---\n",
        "original_audio_file_path = None # Path if classical download is used\n",
        "video_info = None\n",
        "final_output_data_local = None\n",
        "json_output_filename_final = None\n",
        "loaded_whisper_model = None\n",
        "audio_files_to_process = []\n",
        "chunk_offsets = []\n",
        "\n",
        "# --- Validation Configuration ---\n",
        "print(\"\\n--- Validation Configuration ---\")\n",
        "valid_config = True\n",
        "if not youtube_url or not youtube_url.startswith(\"http\"):\n",
        "    print(\"‚ùå URL YouTube invalide.\")\n",
        "    valid_config = False\n",
        "\n",
        "# Check for ffmpeg/yt-dlp if streaming enabled\n",
        "if enable_streaming_download:\n",
        "    if not shutil.which(\"ffmpeg\"):\n",
        "        print(\"‚ùå FFMPEG non trouv√© dans le PATH. T√©l√©chargement stream√© d√©sactiv√©.\")\n",
        "        # valid_config = False # Or just disable streaming? Let's disable.\n",
        "        enable_streaming_download = False\n",
        "        print(\"   -> Basculement vers le t√©l√©chargement classique.\")\n",
        "    if not shutil.which(\"yt-dlp\"):\n",
        "        print(\"‚ùå yt-dlp non trouv√© dans le PATH. T√©l√©chargement stream√© d√©sactiv√©.\")\n",
        "        # valid_config = False\n",
        "        enable_streaming_download = False\n",
        "        print(\"   -> Basculement vers le t√©l√©chargement classique.\")\n",
        "\n",
        "if not enable_streaming_download:\n",
        "     # Check pydub requirement only if streaming is OFF\n",
        "     try: from pydub import AudioSegment\n",
        "     except ImportError:\n",
        "         print(\"‚ùå pydub non install√© et t√©l√©chargement stream√© d√©sactiv√©. Impossible de d√©couper l'audio.\")\n",
        "         valid_config = False\n",
        "\n",
        "if not gemini_api_key or gemini_api_key == \"YOUR_GEMINI_API_KEY\":\n",
        "     if enable_incremental_upload or enable_segment_chunking: # Translation needed for these\n",
        "         print(\"‚ùå Cl√© API Gemini manquante ou non remplac√©e. La traduction et/ou l'upload √©choueront.\")\n",
        "         valid_config = False\n",
        "     else:\n",
        "         print(\"‚ö†Ô∏è Cl√© API Gemini manquante ou non remplac√©e. Traduction et upload seront ignor√©s.\")\n",
        "     gemini_api_key = None # Set to None to disable dependent features\n",
        "\n",
        "if enable_incremental_upload and (not upload_chunk_url or not upload_chunk_url.startswith(\"http\")):\n",
        "    print(\"‚ùå URL d'upload invalide.\")\n",
        "    valid_config = False\n",
        "\n",
        "if split_duration_minutes <= 0:\n",
        "    print(\"‚ùå split_duration_minutes doit √™tre positif.\")\n",
        "    valid_config = False\n",
        "\n",
        "if parallel_transcription_workers < 0:\n",
        "     print(\"‚ö†Ô∏è parallel_transcription_workers doit √™tre >= 0. Utilisation de 1 (s√©quentiel).\")\n",
        "     parallel_transcription_workers = 1\n",
        "elif parallel_transcription_workers > 0 and not IS_GPU_AVAILABLE:\n",
        "     print(f\"‚ö†Ô∏è Ex√©cution parall√®le ({parallel_transcription_workers}) de Whisper sur CPU. Peut √™tre lent et gourmand en RAM.\")\n",
        "elif parallel_transcription_workers > 4 and IS_GPU_AVAILABLE:\n",
        "     print(f\"‚ö†Ô∏è Nombre √©lev√© de workers ({parallel_transcription_workers}) pour Whisper sur GPU. Peut saturer la VRAM ou ne pas am√©liorer la vitesse.\")\n",
        "\n",
        "\n",
        "if enable_segment_chunking and max_segments_per_translation_chunk <= 0:\n",
        "    print(\"‚ùå max_segments_per_translation_chunk doit √™tre positif.\")\n",
        "    valid_config = False\n",
        "if enable_segment_chunking and max_concurrent_translation_tasks <= 0:\n",
        "    print(\"‚ùå max_concurrent_translation_tasks doit √™tre positif.\")\n",
        "    valid_config = False\n",
        "\n",
        "if not valid_config:\n",
        "    print(\"\\nüö´ Erreurs de configuration d√©tect√©es. Arr√™t du script.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"‚úÖ Configuration valid√©e.\")\n",
        "    print(f\"   Mode T√©l√©chargement/D√©coupage: {'Stream√© (yt-dlp | ffmpeg)' if enable_streaming_download else 'Classique (T√©l√©charger puis d√©couper avec pydub)'}\")\n",
        "    print(f\"   Dur√©e Chunk Audio: {split_duration_minutes} min\")\n",
        "    print(f\"   Mod√®le Whisper: {model_size}, VAD: {'Activ√©' if use_vad_during_transcription else 'D√©sactiv√©'}\")\n",
        "    print(f\"   Workers Transcription Parall√®le: {parallel_transcription_workers if parallel_transcription_workers > 0 else '1 (S√©quentiel)'}\")\n",
        "    print(f\"   Traduction Gemini: {'Activ√©e' if gemini_api_key else 'D√©sactiv√©e'}\")\n",
        "    if gemini_api_key:\n",
        "        print(f\"     Chunking Segments Trad: {'Activ√©' if enable_segment_chunking else 'D√©sactiv√©'} (Max Seg/Chunk: {max_segments_per_translation_chunk if enable_segment_chunking else 'N/A'})\")\n",
        "        print(f\"     Workers Traduction Parall√®le: {max_concurrent_translation_tasks}\")\n",
        "    print(f\"   Upload Incr√©mental: {'Activ√©' if enable_incremental_upload else 'D√©sactiv√©'}\")\n",
        "\n",
        "\n",
        "# --- Ex√©cution ---\n",
        "try:\n",
        "    # ========================================\n",
        "    # √âTAPE 1: Pr√©paration & Chargement Mod√®le\n",
        "    # ========================================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 1: PR√âPARATION & CHARGEMENT MOD√àLE\\n\" + \"=\"*40)\n",
        "    start_step1 = time.time()\n",
        "\n",
        "    # --- 1.1 Get Metadata ---\n",
        "    video_info = download_youtube_metadata(youtube_url)\n",
        "    if not video_info or not video_info.get(\"video_id\") or video_info.get(\"video_id\") == \"UNKNOWN_ID\":\n",
        "        raise ValueError(\"√âchec de la r√©cup√©ration des m√©tadonn√©es YouTube ou ID vid√©o manquant.\")\n",
        "\n",
        "    safe_video_id = video_info.get(\"video_id\").replace(\"-\", \"_\")\n",
        "    output_filename_base = f\"{safe_video_id}_audio.mp3\" # Base name for classical download\n",
        "    output_path_original_download = os.path.join(OUTPUT_DIR_V2, output_filename_base) # Full path if downloading whole file\n",
        "\n",
        "    # --- 1.2 Download & Segment Audio ---\n",
        "    split_secs = split_duration_minutes * 60\n",
        "\n",
        "    if enable_streaming_download:\n",
        "        audio_files_to_process, chunk_offsets = download_and_segment_audio_streamed(\n",
        "            youtube_url, CHUNK_DIR, split_secs\n",
        "        )\n",
        "        if not audio_files_to_process:\n",
        "            print(\"‚ùå √âchec du t√©l√©chargement stream√©/segmentation. Arr√™t.\")\n",
        "            # TODO: Optionally fallback to classical download here?\n",
        "            raise RuntimeError(\"√âchec du pipeline de streaming audio.\")\n",
        "        print(f\"‚úÖ Stream/Segmentation r√©ussi. {len(audio_files_to_process)} chunks pr√™ts.\")\n",
        "    else:\n",
        "        # Fallback to classical download and split\n",
        "        print(\"‚ÑπÔ∏è Utilisation du mode t√©l√©chargement classique...\")\n",
        "        original_audio_file_path = download_youtube_audio_improved(\n",
        "            youtube_url, output_path_original_download, video_info\n",
        "        )\n",
        "        if not original_audio_file_path:\n",
        "             raise ValueError(\"√âchec du t√©l√©chargement audio classique.\")\n",
        "\n",
        "        # Now split the downloaded file using pydub\n",
        "        try:\n",
        "             audio_files_to_process, chunk_offsets = split_audio_by_fixed_duration(\n",
        "                 original_audio_file_path, CHUNK_DIR, split_secs\n",
        "             )\n",
        "             if not audio_files_to_process:\n",
        "                 print(\"‚ö†Ô∏è Le d√©coupage pydub a √©chou√© ou n'a produit aucun chunk. Traitement du fichier entier (si possible).\")\n",
        "                 # Attempt to process the whole file as one chunk\n",
        "                 if os.path.exists(original_audio_file_path):\n",
        "                     audio_files_to_process = [original_audio_file_path]\n",
        "                     chunk_offsets = [0.0]\n",
        "                     print(f\"   -> Traitement du fichier complet: {original_audio_file_path}\")\n",
        "                 else:\n",
        "                     raise RuntimeError(\"√âchec du d√©coupage pydub et fichier original introuvable.\")\n",
        "             else:\n",
        "                 print(f\"‚úÖ D√©coupage pydub r√©ussi. {len(audio_files_to_process)} chunks pr√™ts.\")\n",
        "        except ImportError:\n",
        "             print(\"‚ùå pydub n'est pas install√©. Impossible de d√©couper le fichier t√©l√©charg√©.\")\n",
        "             raise RuntimeError(\"pydub requis pour le d√©coupage classique mais non trouv√©.\")\n",
        "\n",
        "\n",
        "    total_audio_chunks = len(audio_files_to_process)\n",
        "    if total_audio_chunks == 0:\n",
        "        raise ValueError(\"Aucun chunk audio n'a √©t√© g√©n√©r√© pour le traitement.\")\n",
        "    print(f\"Nombre total de chunks audio √† traiter: {total_audio_chunks}\")\n",
        "\n",
        "\n",
        "    # --- 1.3 Chargement Mod√®le Whisper ---\n",
        "    print(\"\\n--- Chargement Mod√®le Whisper ---\")\n",
        "    print(f\"Mod√®le demand√©: {model_size}\")\n",
        "    start_load = time.time()\n",
        "    device = \"cuda\" if IS_GPU_AVAILABLE else \"cpu\"\n",
        "    compute_type = \"default\" # Let faster-whisper decide based on device/capability\n",
        "\n",
        "    # Determine best compute_type\n",
        "    if device == \"cuda\":\n",
        "        major, _ = torch.cuda.get_device_capability(0)\n",
        "        if major >= 8:\n",
        "             # Ampere and later: bfloat16 potentially faster if supported well by model/ops\n",
        "             # float16 generally safe and good performance\n",
        "             # Let's default to float16 for broader compatibility unless user specifically wants bfloat16\n",
        "             compute_type = \"float16\"\n",
        "             print(\"   Utilisation compute_type: float16 (GPU)\")\n",
        "             # Add option for bfloat16 later if needed/tested\n",
        "             # compute_type = \"bfloat16\"\n",
        "             # print(\"   Utilisation compute_type: bfloat16 (GPU)\")\n",
        "        else:\n",
        "             compute_type = \"float16\"\n",
        "             print(\"   Utilisation compute_type: float16 (GPU)\")\n",
        "    else: # CPU\n",
        "        # int8 is generally good for CPU inference speed vs accuracy trade-off\n",
        "        compute_type = \"int8\"\n",
        "        print(\"   Utilisation compute_type: int8 (CPU)\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Ensure model files are downloaded if necessary\n",
        "        # This might take time on first run\n",
        "        print(f\"   Chargement/V√©rification mod√®le '{model_size}' sur {device} avec compute_type '{compute_type}'...\")\n",
        "        loaded_whisper_model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "        load_time = time.time() - start_load\n",
        "        print(f\"‚úÖ Mod√®le '{model_size}' charg√© en {load_time:.2f}s.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå √âchec chargement mod√®le Whisper: {e}\")\n",
        "        traceback.print_exc()\n",
        "        # Try falling back to a simpler compute_type?\n",
        "        if device == \"cuda\" and compute_type != \"float16\":\n",
        "             print(\"   Tentative de fallback vers compute_type 'float16'...\")\n",
        "             try:\n",
        "                 compute_type = \"float16\"\n",
        "                 loaded_whisper_model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "                 load_time = time.time() - start_load\n",
        "                 print(f\"‚úÖ Mod√®le '{model_size}' charg√© (fallback float16) en {load_time:.2f}s.\")\n",
        "             except Exception as e2:\n",
        "                 print(f\"‚ùå √âchec chargement mod√®le Whisper (fallback float16): {e2}\")\n",
        "                 raise ValueError(f\"Impossible de charger le mod√®le Whisper '{model_size}' sur {device}.\")\n",
        "        else:\n",
        "            raise ValueError(f\"Impossible de charger le mod√®le Whisper '{model_size}' sur {device}.\")\n",
        "\n",
        "\n",
        "    step1_time = time.time() - start_step1\n",
        "    print(f\"‚è±Ô∏è Temps √âtape 1 (Pr√©paration & Chargement Mod√®le): {step1_time:.2f}s\")\n",
        "\n",
        "    # ================================================\n",
        "    # √âTAPE 2: Boucle de Traitement & Upload Intercal√© (MODIFI√âE POUR TRANSCRIPTION //)\n",
        "    # ================================================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 2: TRAITEMENT & UPLOAD INTERCAL√âS\\n\" + \"=\"*40)\n",
        "    all_final_segments_local = []\n",
        "    total_transcribed_segments = 0\n",
        "    total_translated_segments_ok = 0\n",
        "    successful_uploads = 0\n",
        "    failed_uploads = 0\n",
        "    failed_translation_audio_chunks = [] # Store index of audio chunks where translation failed\n",
        "    failed_transcription_audio_chunks = [] # Store index of audio chunks where transcription failed\n",
        "    global_start_proc = time.time()\n",
        "\n",
        "    # --- 2.1 Transcription (S√©quentielle ou Parall√®le) ---\n",
        "    print(f\"--- D√©marrage Transcription ({'Parall√®le' if parallel_transcription_workers > 1 else 'S√©quentielle'}) ---\")\n",
        "    transcription_results_ordered = [None] * total_audio_chunks # Pre-allocate list for ordered results\n",
        "    transcription_start_time = time.time()\n",
        "\n",
        "    if parallel_transcription_workers > 1:\n",
        "        # --- Parallel Transcription ---\n",
        "        print(f\"   Utilisation de {parallel_transcription_workers} workers...\")\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=parallel_transcription_workers, thread_name_prefix='WhisperWorker') as executor:\n",
        "            futures_transcribe = {}\n",
        "            for i, file_path in enumerate(audio_files_to_process):\n",
        "                future = executor.submit(\n",
        "                    transcribe_audio_faster,\n",
        "                    file_path,\n",
        "                    loaded_whisper_model,\n",
        "                    chunk_offsets[i],\n",
        "                    beam_search_size,\n",
        "                    use_vad_during_transcription,\n",
        "                    vad_silence_duration_ms,\n",
        "                    i, # chunk index\n",
        "                    total_audio_chunks # total chunks\n",
        "                )\n",
        "                futures_transcribe[future] = i # Map future to original index\n",
        "\n",
        "            processed_count = 0\n",
        "            for future in concurrent.futures.as_completed(futures_transcribe):\n",
        "                original_index = futures_transcribe[future]\n",
        "                try:\n",
        "                    result = future.result() # Gets the dict {'segments': [...], 'success': bool}\n",
        "                    transcription_results_ordered[original_index] = result\n",
        "                    if not result[\"success\"]:\n",
        "                         failed_transcription_audio_chunks.append(original_index + 1)\n",
        "                         print(f\"   ‚ö†Ô∏è √âchec transcription pour chunk audio {original_index + 1}.\")\n",
        "                    processed_count += 1\n",
        "                    # Simple progress indicator\n",
        "                    sys.stdout.write(f\"\\r   Progression transcription: {processed_count}/{total_audio_chunks} chunks termin√©s.\")\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "                except Exception as exc:\n",
        "                     print(f'\\n   ‚ùå Erreur majeure lors de la transcription du chunk audio {original_index + 1}: {exc}')\n",
        "                     traceback.print_exc()\n",
        "                     transcription_results_ordered[original_index] = {\"segments\": [], \"success\": False} # Mark as failed\n",
        "                     failed_transcription_audio_chunks.append(original_index + 1)\n",
        "                     processed_count += 1\n",
        "                     sys.stdout.write(f\"\\r   Progression transcription: {processed_count}/{total_audio_chunks} chunks termin√©s.\")\n",
        "                     sys.stdout.flush()\n",
        "\n",
        "            sys.stdout.write(\"\\n\") # New line after progress\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    else:\n",
        "        # --- Sequential Transcription ---\n",
        "        print(\"   Ex√©cution s√©quentielle...\")\n",
        "        for i, file_path in enumerate(audio_files_to_process):\n",
        "            result = transcribe_audio_faster(\n",
        "                file_path,\n",
        "                loaded_whisper_model,\n",
        "                chunk_offsets[i],\n",
        "                beam_search_size,\n",
        "                use_vad_during_transcription,\n",
        "                vad_silence_duration_ms,\n",
        "                i, total_audio_chunks\n",
        "            )\n",
        "            transcription_results_ordered[i] = result\n",
        "            if not result[\"success\"]:\n",
        "                 failed_transcription_audio_chunks.append(i + 1)\n",
        "                 # Error already logged inside transcribe_audio_faster\n",
        "\n",
        "    transcription_end_time = time.time()\n",
        "    print(f\"--- Transcription termin√©e en {transcription_end_time - transcription_start_time:.2f}s ---\")\n",
        "    if failed_transcription_audio_chunks:\n",
        "         print(f\"   ‚ö†Ô∏è √âchec(s) de transcription pour les chunks audio: {sorted(list(set(failed_transcription_audio_chunks)))}\")\n",
        "\n",
        "    # --- 2.2 Boucle de Traduction & Upload (sur les r√©sultats ordonn√©s de la transcription) ---\n",
        "    print(\"\\n--- D√©marrage Traduction & Upload ---\")\n",
        "    total_processing_loop_start = time.time()\n",
        "\n",
        "    for i, transcription_result in enumerate(transcription_results_ordered):\n",
        "        chunk_proc_start_time = time.time()\n",
        "        current_chunk_segments_transcribed = []\n",
        "        log_prefix_loop = f\"[Chunk {i+1}/{total_audio_chunks}]\"\n",
        "\n",
        "        if transcription_result is None:\n",
        "            print(f\"{log_prefix_loop} ‚ö†Ô∏è R√©sultat de transcription manquant (erreur interne?). Chunk ignor√©.\")\n",
        "            continue\n",
        "        elif not transcription_result[\"success\"]:\n",
        "             print(f\"{log_prefix_loop} ‚ö†Ô∏è Transcription √©chou√©e pour ce chunk. Ignor√© pour traduction/upload.\")\n",
        "             # Store original (empty) segments if needed for local save consistency?\n",
        "             # all_final_segments_local.extend([]) # Add empty list\n",
        "             continue\n",
        "        else:\n",
        "            current_chunk_segments_transcribed = transcription_result[\"segments\"]\n",
        "\n",
        "        num_transcribed = len(current_chunk_segments_transcribed)\n",
        "        total_transcribed_segments += num_transcribed\n",
        "        print(f\"\\n{log_prefix_loop} Traitement - Segments Transcrits: {num_transcribed}\")\n",
        "\n",
        "        if num_transcribed == 0:\n",
        "            print(f\"{log_prefix_loop}   -> Aucun segment transcrit. Passage au chunk suivant.\")\n",
        "            chunk_proc_end_time = time.time()\n",
        "            # print(f\"{log_prefix_loop} ‚è±Ô∏è Temps traitement (vide): {chunk_proc_end_time - chunk_proc_start_time:.2f}s\")\n",
        "            continue # Skip translation and upload for empty chunks\n",
        "\n",
        "        # --- 2.2 Traduction (avec gestion concurrence si activ√©e) ---\n",
        "        aggregated_translated_segments_for_audio_chunk = []\n",
        "        translation_failed_for_this_audio_chunk = False\n",
        "        translation_api_calls = 0\n",
        "        translation_start_time = time.time()\n",
        "\n",
        "        if not gemini_api_key:\n",
        "            print(f\"{log_prefix_loop}   >> ‚ö†Ô∏è Traduction ignor√©e (pas de cl√© API).\")\n",
        "            # Use transcribed segments directly for local save\n",
        "            all_final_segments_local.extend(current_chunk_segments_transcribed)\n",
        "        else:\n",
        "            # --- Segment Chunking & Concurrent Translation ---\n",
        "            if enable_segment_chunking and num_transcribed > 0:\n",
        "                segment_sub_chunks = list(chunk_list(current_chunk_segments_transcribed, max_segments_per_translation_chunk))\n",
        "                total_segment_sub_chunks = len(segment_sub_chunks)\n",
        "                print(f\"{log_prefix_loop}   >> Pr√©paration traduction ({num_transcribed} segments en {total_segment_sub_chunks} sous-chunks) avec max {max_concurrent_translation_tasks} workers...\")\n",
        "\n",
        "                futures_results_translate = {}\n",
        "                futures_map_translate = {}\n",
        "                active_api_calls = 0\n",
        "\n",
        "                with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_translation_tasks, thread_name_prefix='GeminiWorker') as executor:\n",
        "                    for j, segment_sub_chunk in enumerate(segment_sub_chunks):\n",
        "                         if segment_sub_chunk: # Ensure sub-chunk is not empty\n",
        "                             future = executor.submit(\n",
        "                                 translate_audio_chunk_segments,\n",
        "                                 segment_sub_chunk, gemini_api_key,\n",
        "                                 i, total_audio_chunks, # Audio chunk index\n",
        "                                 j, total_segment_sub_chunks # Segment chunk index\n",
        "                             )\n",
        "                             futures_map_translate[future] = j\n",
        "                             active_api_calls += 1\n",
        "                         else:\n",
        "                             futures_results_translate[j] = [] # Empty input -> empty output\n",
        "\n",
        "                    # print(f\"{log_prefix_loop}      >> {active_api_calls} t√¢ches de traduction soumises...\")\n",
        "                    translation_api_calls = active_api_calls # Total calls for this audio chunk\n",
        "                    processed_translate_count = 0\n",
        "                    for future in concurrent.futures.as_completed(futures_map_translate):\n",
        "                         j_index = futures_map_translate[future]\n",
        "                         try:\n",
        "                             result = future.result() # List of segments or None\n",
        "                             futures_results_translate[j_index] = result\n",
        "                             if result is None:\n",
        "                                  translation_failed_for_this_audio_chunk = True\n",
        "                                  # Error logged within the function\n",
        "                         except Exception as exc:\n",
        "                             print(f'\\n{log_prefix_loop}   >> ‚ùå Erreur traduction sous-chunk {j_index + 1}: {exc}')\n",
        "                             traceback.print_exc()\n",
        "                             futures_results_translate[j_index] = None\n",
        "                             translation_failed_for_this_audio_chunk = True\n",
        "                         finally:\n",
        "                             processed_translate_count +=1\n",
        "                             # Less verbose progress for translation\n",
        "                             # sys.stdout.write(f\"\\r{log_prefix_loop}      >> Progression traduction: {processed_translate_count}/{translation_api_calls} termin√©es.\")\n",
        "                             # sys.stdout.flush()\n",
        "\n",
        "                # sys.stdout.write(\"\\n\")\n",
        "                # sys.stdout.flush()\n",
        "\n",
        "                # Aggregate results in order\n",
        "                for j in range(total_segment_sub_chunks):\n",
        "                     result_sub_chunk = futures_results_translate.get(j)\n",
        "                     if result_sub_chunk is not None:\n",
        "                          aggregated_translated_segments_for_audio_chunk.extend(result_sub_chunk)\n",
        "                     # If None, failure already marked\n",
        "\n",
        "\n",
        "            # --- No Segment Chunking (Sequential Translation) ---\n",
        "            elif num_transcribed > 0: # Sequential translation only if segments exist\n",
        "                 print(f\"{log_prefix_loop}   >> Traduction s√©quentielle de {num_transcribed} segments...\")\n",
        "                 translation_api_calls = 1\n",
        "                 translated_segments = translate_audio_chunk_segments(\n",
        "                     current_chunk_segments_transcribed,\n",
        "                     gemini_api_key,\n",
        "                     i, total_audio_chunks\n",
        "                 )\n",
        "                 if translated_segments is not None:\n",
        "                     aggregated_translated_segments_for_audio_chunk = translated_segments\n",
        "                 else:\n",
        "                     translation_failed_for_this_audio_chunk = True\n",
        "                     # Error logged within the function\n",
        "\n",
        "            # --- Post-Translation ---\n",
        "            translation_end_time = time.time()\n",
        "            translation_duration = translation_end_time - translation_start_time\n",
        "            print(f\"{log_prefix_loop}   >> Traduction termin√©e en {translation_duration:.2f}s ({translation_api_calls} appels API).\")\n",
        "\n",
        "            if translation_failed_for_this_audio_chunk:\n",
        "                failed_translation_audio_chunks.append(i + 1)\n",
        "                print(f\"{log_prefix_loop}   >> ‚ÑπÔ∏è Sauvegarde locale utilisera fallback (transcrit original).\")\n",
        "                all_final_segments_local.extend(current_chunk_segments_transcribed) # Fallback\n",
        "            else:\n",
        "                 num_translated = len(aggregated_translated_segments_for_audio_chunk)\n",
        "                 total_translated_segments_ok += num_translated\n",
        "                 all_final_segments_local.extend(aggregated_translated_segments_for_audio_chunk) # Use translated\n",
        "                 print(f\"{log_prefix_loop}   >> {num_translated} segments traduits agr√©g√©s.\")\n",
        "\n",
        "\n",
        "        # --- 2.3 Upload Incr√©mental (op√®re sur le r√©sultat agr√©g√© de la traduction SI r√©ussie) ---\n",
        "        segments_to_upload = []\n",
        "        can_upload = (enable_incremental_upload and\n",
        "                      gemini_api_key and\n",
        "                      not translation_failed_for_this_audio_chunk and\n",
        "                      aggregated_translated_segments_for_audio_chunk)\n",
        "\n",
        "        if can_upload:\n",
        "            segments_to_upload = aggregated_translated_segments_for_audio_chunk\n",
        "            print(f\"{log_prefix_loop}      >> ‚¨ÜÔ∏è Tentative d'upload ({len(segments_to_upload)} segments)...\", flush=True)\n",
        "            upload_payload = {\n",
        "                \"video_id\": video_info[\"video_id\"],\n",
        "                \"description\": video_info.get(\"description\", \"\"), # Ensure string even if None\n",
        "                \"channel_name\": video_info.get(\"channel_name\", \"N/A\"),\n",
        "                \"channel_url\": video_info.get(\"channel_url\", \"N/A\"),\n",
        "                \"segments\": segments_to_upload,\n",
        "                \"chunk_index\": i,\n",
        "                \"total_chunks\": total_audio_chunks,\n",
        "                # Only send title/duration on first chunk to avoid overwrites? Check server logic.\n",
        "                \"title\": video_info.get(\"title\", \"N/A\") if i == 0 else None,\n",
        "                \"duration\": video_info.get(\"duration\") if i == 0 else None,\n",
        "            }\n",
        "            upload_success = False\n",
        "            try:\n",
        "                response = requests.post(upload_chunk_url, json=upload_payload, timeout=60)\n",
        "                response.raise_for_status()\n",
        "                try: server_message = response.json().get(\"message\", response.text[:100])\n",
        "                except json.JSONDecodeError: server_message = response.text[:100]\n",
        "                print(f\"{log_prefix_loop}      >> ‚úÖ Upload r√©ussi (Status: {response.status_code}). Serveur: {server_message}\", flush=True)\n",
        "                successful_uploads += 1\n",
        "                upload_success = True\n",
        "            except requests.exceptions.Timeout: print(f\"{log_prefix_loop}      >> ‚ùå Upload √©chou√© (Timeout).\", flush=True)\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"{log_prefix_loop}      >> ‚ùå Upload √©chou√© (Erreur HTTP/R√©seau): {e}\", flush=True)\n",
        "                if e.response is not None: print(f\"         R√©ponse serveur ({e.response.status_code}): {e.response.text[:200]}...\")\n",
        "            except Exception as e: print(f\"{log_prefix_loop}      >> ‚ùå Erreur inattendue upload: {e}\", flush=True); traceback.print_exc()\n",
        "\n",
        "            if not upload_success: failed_uploads += 1\n",
        "        else:\n",
        "             # Log why upload is skipped\n",
        "             if not enable_incremental_upload: print(f\"{log_prefix_loop}      >> ‚ÑπÔ∏è Upload incr√©mental d√©sactiv√©.\")\n",
        "             elif not gemini_api_key: print(f\"{log_prefix_loop}      >> ‚ÑπÔ∏è Upload ignor√© (pas de cl√© API).\")\n",
        "             elif translation_failed_for_this_audio_chunk: print(f\"{log_prefix_loop}      >> ‚ÑπÔ∏è Upload ignor√© (√©chec traduction).\")\n",
        "             elif not aggregated_translated_segments_for_audio_chunk: print(f\"{log_prefix_loop}      >> ‚ÑπÔ∏è Aucun segment traduit √† uploader.\")\n",
        "             # No need to log if transcription failed, already logged earlier.\n",
        "\n",
        "\n",
        "        chunk_proc_end_time = time.time()\n",
        "        print(f\"{log_prefix_loop} ‚è±Ô∏è Temps traitement total chunk: {chunk_proc_end_time - chunk_proc_start_time:.2f}s\")\n",
        "\n",
        "\n",
        "    total_processing_loop_end = time.time()\n",
        "\n",
        "    # --- R√©sum√© Traitement ---\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 2: R√âSUM√â DU TRAITEMENT\\n\" + \"=\"*40)\n",
        "    total_proc_time = total_processing_loop_end - global_start_proc # Includes transcription + loop time\n",
        "    print(f\"‚è±Ô∏è Temps total Traitement & Upload (√âtape 2): {total_proc_time:.2f}s.\")\n",
        "    print(f\"üìä Total Chunks Audio Trait√©s: {total_audio_chunks}\")\n",
        "    failed_transcription_list = sorted(list(set(failed_transcription_audio_chunks)))\n",
        "    if failed_transcription_list: print(f\"‚ùå Transcription √©chou√©e pour {len(failed_transcription_list)} chunk(s) audio: {', '.join(map(str, failed_transcription_list))}\")\n",
        "    else: print(f\"‚úÖ Transcription r√©ussie pour tous les chunks.\")\n",
        "    print(f\"üìä Total Segments Transcrits (depuis chunks r√©ussis): {total_transcribed_segments}\")\n",
        "\n",
        "    if gemini_api_key:\n",
        "        print(f\"üìä Total Segments agr√©g√©s apr√®s traduction r√©ussie: {total_translated_segments_ok}\")\n",
        "        failed_translation_list = sorted(list(set(failed_translation_audio_chunks)))\n",
        "        if failed_translation_list:\n",
        "             print(f\"‚ùå Traduction √©chou√©e (partiellement ou totalement) pour {len(failed_translation_list)} chunk(s) audio: {', '.join(map(str, failed_translation_list))}\")\n",
        "             print(f\"   (Sauvegarde locale utilise fallback transcrit pour eux)\")\n",
        "        elif total_transcribed_segments > 0 : print(f\"‚úÖ Traduction r√©ussie pour tous les chunks audio ayant des segments transcrits.\")\n",
        "        elif total_audio_chunks > 0: print(f\"‚ÑπÔ∏è Aucune traduction effectu√©e (pas de segments transcrits).\")\n",
        "        else: print(f\"‚ÑπÔ∏è Aucune traduction effectu√©e (pas de chunks).\") # Should not happen if checks above work\n",
        "    else: print(\"‚ÑπÔ∏è Traduction ignor√©e (pas de cl√© API).\")\n",
        "\n",
        "    print(f\"üíæ Total Segments agr√©g√©s pour sauvegarde locale: {len(all_final_segments_local)}\")\n",
        "    if enable_incremental_upload:\n",
        "        print(f\"‚òÅÔ∏è Uploads Incr√©mentaux R√©ussis (bas√©s sur chunks traduits avec succ√®s): {successful_uploads}\")\n",
        "        print(f\"‚òÅÔ∏è Uploads Incr√©mentaux √âchou√©s (tentatives): {failed_uploads}\")\n",
        "    else: print(\"‚òÅÔ∏è Upload Incr√©mental d√©sactiv√©.\")\n",
        "\n",
        "\n",
        "    # ========================================\n",
        "    # √âTAPE 3: SAUVEGARDE LOCALE FINALE (Inchang√©e)\n",
        "    # ========================================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 3: SAUVEGARDE LOCALE FINALE\\n\" + \"=\"*40)\n",
        "    start_step3 = time.time()\n",
        "    if all_final_segments_local:\n",
        "        # Sort segments by start time just in case parallel processing messed up order slightly\n",
        "        # (Shouldn't happen with how results are collected, but good practice)\n",
        "        all_final_segments_local.sort(key=lambda x: x.get('start', 0))\n",
        "\n",
        "        final_output_data_local = {\"segments\": all_final_segments_local}\n",
        "        if video_info:\n",
        "             metadata_to_include = [\"video_id\", \"title\", \"channel_name\", \"channel_url\", \"duration\", \"original_url\", \"upload_date\"]\n",
        "             final_output_data_local.update({k: v for k, v in video_info.items() if k in metadata_to_include and v is not None})\n",
        "\n",
        "        # --- Filename Generation ---\n",
        "        mode_suffix = \"_stream\" if enable_streaming_download else \"_classic\"\n",
        "        split_suffix = f\"_split{split_duration_minutes}min\" # Always splitting now\n",
        "        vad_suffix = \"_VAD\" if use_vad_during_transcription else \"_noVAD\"\n",
        "        trans_workers_suffix = f\"_T{parallel_transcription_workers}\" if parallel_transcription_workers > 1 else \"\"\n",
        "\n",
        "        segchunk_suffix = \"\"\n",
        "        if gemini_api_key:\n",
        "             if enable_segment_chunking:\n",
        "                 segchunk_suffix = f\"_segChunk{max_segments_per_translation_chunk}_conc{max_concurrent_translation_tasks}\"\n",
        "             else:\n",
        "                 segchunk_suffix = \"_segFull\" # Translation active but not chunked\n",
        "\n",
        "        # Determine overall status based on failures\n",
        "        status_suffix = \"\"\n",
        "        if not total_transcribed_segments and not failed_transcription_list : status_suffix = \"_no_segments_found\"\n",
        "        elif failed_transcription_list and not total_transcribed_segments : status_suffix = \"_transcription_failed_all\"\n",
        "        elif failed_transcription_list: status_suffix = \"_transcription_partial_fail\" # Some transcription failed\n",
        "        # Suffixes below assume at least *some* transcription succeeded\n",
        "        elif not gemini_api_key: status_suffix = \"_transcribed_only\"\n",
        "        elif not failed_translation_list and total_transcribed_segments > 0: status_suffix = \"_fully_translated\"\n",
        "        elif failed_translation_list and total_translated_segments_ok > 0: status_suffix = \"_partially_translated\"\n",
        "        elif failed_translation_list: status_suffix = \"_translation_failed_all\" # Transcription OK, but all translation attempts failed\n",
        "        else: status_suffix = \"_fully_transcribed_untranslated\" # Should be caught by _transcribed_only\n",
        "\n",
        "\n",
        "        json_output_filename_final = os.path.join(\n",
        "            OUTPUT_DIR_V2,\n",
        "            f\"{safe_video_id}{mode_suffix}{split_suffix}{vad_suffix}{trans_workers_suffix}{segchunk_suffix}{status_suffix}.json\"\n",
        "        )\n",
        "\n",
        "        print(f\"Tentative de sauvegarde du r√©sultat local agr√©g√© vers:\")\n",
        "        print(f\"   {json_output_filename_final}\")\n",
        "        try:\n",
        "            with open(json_output_filename_final, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(final_output_data_local, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"‚úÖ R√©sultat local complet sauvegard√© avec succ√®s.\")\n",
        "            if failed_uploads > 0: print(f\"   -> ‚ö†Ô∏è Attention: {failed_uploads} upload(s) incr√©mentaux ont √©chou√©.\")\n",
        "            if failed_transcription_list: print(f\"   -> ‚ö†Ô∏è Ce fichier ne contient PAS les segments des chunks o√π la transcription a √©chou√©.\")\n",
        "            if failed_translation_list: print(f\"   -> ‚ÑπÔ∏è Ce fichier contient les segments transcrits originaux pour les chunks audio o√π la traduction a √©chou√©.\")\n",
        "        except IOError as e: print(f\"‚ùå Erreur d'√©criture lors de la sauvegarde JSON locale: {e}\")\n",
        "        except Exception as e: print(f\"‚ùå Erreur inattendue lors de la sauvegarde JSON locale: {e}\"); traceback.print_exc()\n",
        "    else:\n",
        "        print(\"‚ùå Aucune donn√©e de segment n'a √©t√© collect√©e/g√©n√©r√©e (v√©rifier erreurs transcription/traitement). Pas de fichier JSON local sauvegard√©.\")\n",
        "    step3_time = time.time() - start_step3\n",
        "    print(f\"‚è±Ô∏è Temps √âtape 3 (Sauvegarde): {step3_time:.2f}s\")\n",
        "\n",
        "except ValueError as ve: print(f\"\\n‚ùå ERREUR DE CONFIGURATION OU DE PROCESSUS: {ve}\"); traceback.print_exc()\n",
        "except RuntimeError as rte: print(f\"\\n‚ùå ERREUR D'EX√âCUTION: {rte}\"); traceback.print_exc()\n",
        "except Exception as e: print(f\"\\n‚ùå ERREUR GLOBALE INATTENDUE: {e}\"); traceback.print_exc()\n",
        "finally:\n",
        "    # ===========================\n",
        "    # √âTAPE 4: NETTOYAGE (Optionnel)\n",
        "    # ===========================\n",
        "    print(\"\\n\\n\" + \"=\"*40 + \"\\n√âTAPE 4: NETTOYAGE (OPTIONNEL)\\n\" + \"=\"*40)\n",
        "    cleanup_intermediate = False #@param {type:\"boolean\"} # Control cleanup\n",
        "    if cleanup_intermediate:\n",
        "        print(\"--- Nettoyage des fichiers temporaires ---\")\n",
        "        # Delete downloaded full file if it exists (classic mode)\n",
        "        if original_audio_file_path and os.path.exists(original_audio_file_path):\n",
        "            try: os.remove(original_audio_file_path); print(f\"üóëÔ∏è Fichier audio complet t√©l√©charg√© supprim√©.\")\n",
        "            except Exception as e: print(f\"‚ùå Erreur suppression fichier audio complet: {e}\")\n",
        "        # Delete audio chunks\n",
        "        if os.path.exists(CHUNK_DIR) and os.listdir(CHUNK_DIR):\n",
        "             try:\n",
        "                 # Be cautious with rmtree\n",
        "                 print(f\"   Suppression des chunks dans {CHUNK_DIR}...\")\n",
        "                 # Remove individual files instead of rmtree for safety\n",
        "                 for filename in os.listdir(CHUNK_DIR):\n",
        "                     file_path_to_del = os.path.join(CHUNK_DIR, filename)\n",
        "                     try:\n",
        "                         if os.path.isfile(file_path_to_del) or os.path.islink(file_path_to_del):\n",
        "                             os.unlink(file_path_to_del)\n",
        "                         # elif os.path.isdir(file_path_to_del): # Should not have subdirs\n",
        "                         #     shutil.rmtree(file_path_to_del)\n",
        "                     except Exception as e_del:\n",
        "                         print(f'   √âchec suppression {file_path_to_del}. Raison: {e_del}')\n",
        "                 # Try removing dir if empty now\n",
        "                 # os.rmdir(CHUNK_DIR) # Might fail if hidden files etc.\n",
        "                 print(f\"üóëÔ∏è Chunks audio supprim√©s.\")\n",
        "             except Exception as e:\n",
        "                 print(f\"‚ùå Erreur suppression dossier/fichiers chunks: {e}\")\n",
        "        else: print(\"   -> Dossier chunks vide ou inexistant.\")\n",
        "        print(\"--- Nettoyage termin√© ---\")\n",
        "    else:\n",
        "        print(\"--- Nettoyage d√©sactiv√© ---\")\n",
        "        if original_audio_file_path and os.path.exists(original_audio_file_path): print(f\"‚ÑπÔ∏è Fichier audio complet conserv√©: {original_audio_file_path}\")\n",
        "        if os.path.exists(CHUNK_DIR) and os.listdir(CHUNK_DIR): print(f\"‚ÑπÔ∏è Chunks audio conserv√©s dans: {CHUNK_DIR}\")\n",
        "        else: print(f\"‚ÑπÔ∏è Aucun chunk audio √† conserver dans {CHUNK_DIR}\")\n",
        "        if json_output_filename_final and os.path.exists(json_output_filename_final): print(f\"‚ÑπÔ∏è Fichier JSON final conserv√©: {json_output_filename_final}\")\n",
        "\n",
        "    print(\"\\nüèÅ Script complet termin√©.\")\n",
        "    total_runtime = 0\n",
        "    # Ensure global_start_proc exists before calculating total time\n",
        "    if 'global_start_proc' in locals() and global_start_proc > 0:\n",
        "       if 'start_step1' in locals() and start_step1 > 0:\n",
        "           full_runtime = time.time() - start_step1\n",
        "           print(f\"‚è±Ô∏è Dur√©e totale d'ex√©cution du script (toutes √©tapes): {full_runtime:.2f}s\")\n",
        "           total_runtime = full_runtime # Use the full runtime if available\n",
        "       else:\n",
        "           total_runtime_proc = time.time() - global_start_proc\n",
        "           print(f\"‚è±Ô∏è Dur√©e d'ex√©cution (√âtape 2 et suivantes): {total_runtime_proc:.2f}s\")\n",
        "           total_runtime = total_runtime_proc # Use processing runtime otherwise\n",
        "\n",
        "    # Final status summary\n",
        "    if json_output_filename_final and os.path.exists(json_output_filename_final):\n",
        "         print(f\"\\n‚úÖ Fichier r√©sultat principal: {json_output_filename_final}\")\n",
        "    else: print(\"\\n‚ö†Ô∏è Aucun fichier JSON final n'a √©t√© sauvegard√©.\")\n",
        "\n",
        "    if enable_incremental_upload and video_info and video_info.get(\"video_id\") != \"UNKNOWN_ID\":\n",
        "        upload_status_summary = \"\"\n",
        "        if successful_uploads == total_audio_chunks and failed_uploads == 0 and total_audio_chunks > 0:\n",
        "            upload_status_summary = \" (Tous les chunks upload√©s avec succ√®s)\"\n",
        "        elif successful_uploads > 0 :\n",
        "            upload_status_summary = f\" ({successful_uploads}/{total_audio_chunks} chunks upload√©s, {failed_uploads} √©checs)\"\n",
        "        elif failed_uploads > 0:\n",
        "             upload_status_summary = f\" (Aucun upload r√©ussi, {failed_uploads} √©checs)\"\n",
        "        elif total_audio_chunks > 0: # No uploads, no fails -> translation likely skipped/failed\n",
        "             upload_status_summary = \" (Aucun upload effectu√© - v√©rifier traduction)\"\n",
        "        else:\n",
        "             upload_status_summary = \" (Aucun chunk √† traiter/uploader)\"\n",
        "\n",
        "        print(f\"\\nüîó Lien potentiel vers r√©sultat serveur{upload_status_summary}:\")\n",
        "        print(f\"   https://qingplay.pythonanywhere.com/vid/{video_info['video_id']}\")\n",
        "\n",
        "    elif enable_incremental_upload: print(f\"\\n‚ÑπÔ∏è Upload incr√©mental activ√© mais √©chec r√©cup√©ration ID vid√©o ou autres probl√®mes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "cellView": "form",
        "id": "uIZnw1qHD5vY",
        "outputId": "37098e0c-ac4d-4874-d146-418d6dbaed3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Cleanup ---\n",
            "Cleanup done.\n",
            "\n",
            "--- Library Imports & Checks ---\n",
            "‚úÖ PyTorch install√©.\n",
            "‚úÖ faster-whisper install√©.\n",
            "‚úÖ pydub install√©.\n",
            "\n",
            "--- GPU Check ---\n",
            "‚úÖ GPU d√©tect√©: True\n",
            "   GPU Name: Tesla T4\n",
            "   Compute Capability: 7.5\n",
            "\n",
            "--- Directories ---\n",
            "‚úÖ Dossier chunks pr√™t: /content/audio_chunks\n",
            "‚úÖ Dossier sortie pr√™t: /content/audio_output_optimized_v2\n",
            "\n",
            "--- D√©finition Fonctions Utilitaires ---\n",
            "\n",
            "--- D√©finition Fonctions Traduction ---\n",
            "\n",
            "\n",
            "========================================\n",
            "--- Configuration Principale ---\n",
            "========================================\n",
            "\n",
            "--- Validation Configuration ---\n",
            "‚úÖ Configuration valid√©e.\n",
            "   Mode T√©l√©chargement/D√©coupage: Stream√© (yt-dlp | ffmpeg)\n",
            "   Dur√©e Chunk Audio: 10 min\n",
            "   Mod√®le Whisper: large-v3, VAD: D√©sactiv√©\n",
            "   Workers Transcription Parall√®le: 1\n",
            "   Traduction Gemini: Activ√©e\n",
            "     Chunking Segments Trad: Activ√© (Max Seg/Chunk: 30)\n",
            "     Workers Traduction Parall√®le: 14\n",
            "   Upload Incr√©mental: Activ√©\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 1: PR√âPARATION & CHARGEMENT MOD√àLE\n",
            "========================================\n",
            "\n",
            "--- R√©cup√©ration M√©tadonn√©es YouTube ---\n",
            "URL: https://www.youtube.com/watch?v=v7RRTGdTquc\n",
            "‚ÑπÔ∏è R√©cup√©ration m√©tadonn√©es via yt-dlp...\n",
            "‚úÖ M√©tadonn√©es r√©cup√©r√©es.\n",
            "   -> ID: v7RRTGdTquc\n",
            "   -> Titre: Our FIRST 24 hours in China‚Äôs futuristic megacity‚Ä¶ üá®üá≥ | Suzhou, China\n",
            "   -> Dur√©e: 1786s\n",
            "\n",
            "--- T√©l√©chargement Stream√© & Segmentation Audio ---\n",
            "URL: https://www.youtube.com/watch?v=v7RRTGdTquc\n",
            "Destination Chunks: /content/audio_chunks\n",
            "Dur√©e Chunk: 600s\n",
            "   Nettoyage anciens chunks (si pr√©sents)...\n",
            "   Commande yt-dlp: yt-dlp -f bestaudio/best --socket-timeout 60 -o - https://www.youtube.com/watch?v=v7RRTGdTquc\n",
            "   Commande ffmpeg: ffmpeg -i pipe:0 -vn -f segment -segment_time 600 -c:a mp3 -ar 16000 -ab 128k -reset_timestamps 1 -map 0:a -sc_threshold 0 /content/audio_chunks/stream_chunk_%04d.mp3\n",
            "   Lancement du pipeline yt-dlp | ffmpeg...\n",
            "   Pipeline termin√© en 41.68s.\n",
            "‚úÖ Pipeline yt-dlp | ffmpeg termin√© avec succ√®s (ffmpeg code 0).\n",
            "   --- FFMPEG Stderr (Info/Warnings) ---\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, matroska,webm, from 'pipe:0':\n",
            "  Metadata:\n",
            "    encoder         : google/video-file\n",
            "  Duration: 00:29:45.60, start: -0.007000, bitrate: N/A\n",
            "  Stream #0:0(eng): Audio: opus, 48000 Hz, stereo, fltp (default)\n",
            "Codec AVOption sc_threshold (Scene change threshold) specified for output file #0 (/content/audio_chunks/stream_chunk_%04d.mp3) has not been used for any stream. The most likely reason is either wrong type (e.g. a video option with no video streams) or that it is a private option of some encoder which was not actually used for any stream.\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (opus (native) -> mp3 (libmp3lame))\n",
            "[segment @ 0x5cdbbc34c340] Opening '/content/audio_chunks/stream_chunk_0000.mp3' for writing\n",
            "Output #0, segment, to '/content/audio_chunks/stream_chunk_%04d.mp3':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(eng): Audio: mp3, 16000 Hz, stereo, fltp, 128 kb/s (default)\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.134.100 libmp3lame\n",
            "[segment @ 0x5cdbbc34c340] Opening '/content/audio_chunks/stream_chunk_0001.mp3' for writing\n",
            "[segment @ 0x5cdbbc34c340] Opening '/content/audio_chunks/stream_chunk_0002.mp3' for writing\n",
            "video:0kB audio:27901kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n",
            "   -----------------------------------\n",
            "‚úÖ 3 chunks audio cr√©√©s dans /content/audio_chunks\n",
            "‚úÖ Stream/Segmentation r√©ussi. 3 chunks pr√™ts.\n",
            "Nombre total de chunks audio √† traiter: 3\n",
            "\n",
            "--- Chargement Mod√®le Whisper ---\n",
            "Mod√®le demand√©: large-v3\n",
            "   Utilisation compute_type: float16 (GPU)\n",
            "   Chargement/V√©rification mod√®le 'large-v3' sur cuda avec compute_type 'float16'...\n",
            "‚úÖ Mod√®le 'large-v3' charg√© en 4.92s.\n",
            "‚è±Ô∏è Temps √âtape 1 (Pr√©paration & Chargement Mod√®le): 54.39s\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 2: TRAITEMENT & UPLOAD INTERCAL√âS\n",
            "========================================\n",
            "--- D√©marrage Transcription (S√©quentielle) ---\n",
            "   Ex√©cution s√©quentielle...\n",
            "üéôÔ∏è [Transcribe 1/3] D√©marrage: stream_chunk_0000.mp3 (Offset Global: 0.000s)\n",
            "üéôÔ∏è [Transcribe 1/3] ‚úÖ Termin√© en 423.04s. 108 segments trouv√©s.\n",
            "üéôÔ∏è [Transcribe 1/3] ‚úÖ Termin√© en 235.79s. 120 segments trouv√©s.\n",
            "üéôÔ∏è [Transcribe 2/3] D√©marrage: stream_chunk_0001.mp3 (Offset Global: 600.000s)\n",
            "\n",
            "\n",
            "========================================\n",
            "√âTAPE 4: NETTOYAGE (OPTIONNEL)\n",
            "========================================\n",
            "--- Nettoyage d√©sactiv√© ---\n",
            "‚ÑπÔ∏è Chunks audio conserv√©s dans: /content/audio_chunks\n",
            "\n",
            "üèÅ Script complet termin√©.\n",
            "‚è±Ô∏è Dur√©e totale d'ex√©cution du script (toutes √©tapes): 322.82s\n",
            "\n",
            "‚ö†Ô∏è Aucun fichier JSON final n'a √©t√© sauvegard√©.\n",
            "\n",
            "üîó Lien potentiel vers r√©sultat serveur (Aucun upload effectu√© - v√©rifier traduction):\n",
            "   https://qingplay.pythonanywhere.com/vid/v7RRTGdTquc\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7a55b6a09b11>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   Ex√©cution s√©quentielle...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_files_to_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             result = transcribe_audio_faster(\n\u001b[0m\u001b[1;32m   1034\u001b[0m                 \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0mloaded_whisper_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7a55b6a09b11>\u001b[0m in \u001b[0;36mtranscribe_audio_faster\u001b[0;34m(file_path, model, chunk_offset_s, beam_size, vad_filter, vad_min_silence_ms, chunk_index, total_chunks)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# print(f\"{log_prefix}   Traitement des segments...\") # Peut √™tre trop verbeux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mseg_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msegment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msegments_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mseg_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mstart_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/faster_whisper/transcribe.py\u001b[0m in \u001b[0;36mgenerate_segments\u001b[0;34m(self, features, tokenizer, options, log_progress, encoder_output)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                 \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                 \u001b[0mcompression_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m             ) = self.generate_with_fallback(encoder_output, prompt, tokenizer, options)\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_speech_threshold\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/faster_whisper/transcribe.py\u001b[0m in \u001b[0;36mgenerate_with_fallback\u001b[0;34m(self, encoder_output, prompt, tokenizer, options)\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 }\n\u001b[1;32m   1403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m             result = self.model.generate(\n\u001b[0m\u001b[1;32m   1405\u001b[0m                 \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}